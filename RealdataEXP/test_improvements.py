#!/usr/bin/env python3
"""
æµ‹è¯•è®­ç»ƒæ”¹è¿›æ•ˆæœ
éªŒè¯æ¨¡å‹æ€§èƒ½å’ŒGPUä½¿ç”¨æƒ…å†µ
"""

import torch
import pandas as pd
import numpy as np
import logging
from pathlib import Path
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from libs.modes.global_mode_optimized import GlobalModeOptimized
from libs.data import KuaiRandDataLoader, FeatureProcessor
from libs.models import MultiLabelModel

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

def test_gpu_usage():
    """æµ‹è¯•GPUä½¿ç”¨æƒ…å†µ"""
    print("=== GPUä½¿ç”¨æƒ…å†µæµ‹è¯• ===")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"GPUåç§°: {torch.cuda.get_device_name()}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # æµ‹è¯•å¼ é‡æ“ä½œ
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        x = torch.randn(1000, 100).to(device)
        y = torch.randn(1000, 1).to(device)
        
        # ç®€å•è®¡ç®—
        z = torch.mm(x.T, x)
        print(f"çŸ©é˜µä¹˜æ³•å®Œæˆï¼Œç»“æœå½¢çŠ¶: {z.shape}")
        
        # æ¸…ç†å†…å­˜
        del x, y, z
        torch.cuda.empty_cache()
        
    return torch.cuda.is_available()

def test_model_improvements():
    """æµ‹è¯•æ¨¡å‹æ”¹è¿›"""
    print("\n=== æ¨¡å‹æ”¹è¿›æµ‹è¯• ===")
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
    input_dim = 157  # æ ¹æ®æ—¥å¿—ä¸­çš„ç‰¹å¾ç»´åº¦
    batch_size = 64
    
    # æµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, input_dim)
    y_play_time = torch.abs(torch.randn(batch_size, 1)) * 1000  # æ’­æ”¾æ—¶é•¿
    y_click = torch.randint(0, 2, (batch_size, 1)).float()  # ç‚¹å‡»
    
    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: x={x.shape}, play_time={y_play_time.shape}, click={y_click.shape}")
    
    # æµ‹è¯•é…ç½®
    config = {
        'labels': [
            {
                'name': 'play_time',
                'target': 'play_time_ms',
                'type': 'numerical',
                'loss_function': 'logMAE',
                'model_params': {
                    'hidden_layers': [512, 256, 128, 64],
                    'dropout': 0.2,
                    'batch_norm': True,
                    'residual': True,
                    'embedding_dim': 64
                },
                'learning_rate': 0.0005,
                'weight_decay': 0.00001
            },
            {
                'name': 'click',
                'target': 'is_click',
                'type': 'binary',
                'loss_function': 'BCE',
                'model_params': {
                    'hidden_layers': [128, 64, 32],
                    'dropout': 0.2,
                    'batch_norm': True,
                    'residual': False,
                    'embedding_dim': 16
                },
                'learning_rate': 0.0001,
                'weight_decay': 0.0001
            }
        ]
    }
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = MultiLabelModel(config, input_dim, device)
    
    print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    try:
        predictions = model.predict_all(x.to(device))
        print(f"é¢„æµ‹æˆåŠŸ: {list(predictions.keys())}")
        
        # æµ‹è¯•è®­ç»ƒæ­¥éª¤
        targets = {
            'play_time': y_play_time.to(device),
            'click': y_click.to(device)
        }
        
        losses = model.train_step(x.to(device), targets)
        print(f"è®­ç»ƒæ­¥éª¤æˆåŠŸï¼ŒæŸå¤±: {losses}")
        
        # æµ‹è¯•è¯„ä¼°
        metrics = model.evaluate_with_metrics(x.to(device), targets)
        print(f"è¯„ä¼°æŒ‡æ ‡: {metrics}")
        
        return True
        
    except Exception as e:
        print(f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_configuration():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\n=== é…ç½®æ–‡ä»¶æµ‹è¯• ===")
    
    try:
        import yaml
        
        # è¯»å–ä¼˜åŒ–é…ç½®
        with open('configs/experiment_optimized.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥å…³é”®é…ç½®
        play_time_config = config['labels'][0]
        print(f"Play_timeæ¨¡å‹é…ç½®:")
        print(f"  - éšè—å±‚: {play_time_config['model_params']['hidden_layers']}")
        print(f"  - æ‰¹å½’ä¸€åŒ–: {play_time_config['model_params']['batch_norm']}")
        print(f"  - æ®‹å·®è¿æ¥: {play_time_config['model_params']['residual']}")
        print(f"  - å­¦ä¹ ç‡: {play_time_config['learning_rate']}")
        print(f"  - æŸå¤±å‡½æ•°: {play_time_config['loss_function']}")
        
        return True
        
    except Exception as e:
        print(f"é…ç½®æ–‡ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    
    print("å¼€å§‹æµ‹è¯•è®­ç»ƒæ”¹è¿›...")
    
    results = {}
    
    # æµ‹è¯•GPUä½¿ç”¨
    results['gpu'] = test_gpu_usage()
    
    # æµ‹è¯•æ¨¡å‹æ”¹è¿›
    results['model'] = test_model_improvements()
    
    # æµ‹è¯•é…ç½®æ–‡ä»¶
    results['config'] = test_configuration()
    
    # æ€»ç»“
    print(f"\n=== æµ‹è¯•ç»“æœæ€»ç»“ ===")
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    if all(results.values()):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ”¹è¿›å·²ç”Ÿæ•ˆã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")

if __name__ == "__main__":
    main()