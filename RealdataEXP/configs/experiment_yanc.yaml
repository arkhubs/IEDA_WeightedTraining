# base_dir: "/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP"
base_dir: "/home/export/base/sc100352/sc100352/online1/IEDA_WeightedTraining/RealdataEXP"
mode: 'global_optimized'  # 使用优化模式


# --- Device Configuration ---
# Specifies the hardware backend.
# Options:
#   'auto': (Recommended) Automatically detects best available hardware in order: cuda -> ipex -> xpu -> dml -> cpu
#   'cuda': NVIDIA GPU with full AMP support.
#   'ipex': Intel GPU with full IPEX optimizations and AMP support.
#   'xpu':  Intel GPU with basic device placement only (No IPEX optimizations, no AMP).
#   'dml':  DirectML-compatible GPU (AMD, Intel, etc.) (No AMP).
#   'cpu':  CPU execution.
device: 'auto'
# --- New: Optimization Settings ---
optimization:
  # --- New: Choose the optimization strategy for Intel GPUs (IPEX) ---
  # 'optimize': (Default, Recommended) Uses the stable ipex.optimize() function.
  # 'compile': (Experimental) Uses the newer torch.compile(backend='ipex'). May be unstable.
  ipex_strategy: 'optimize'
  torch_compile:
    enabled: false  # Set to true to enable torch.compile where supported
    backend: "inductor" # Options: default, inductor, ipex

# 数据集配置
dataset:
  name: "KuaiRand-1K"
  path: "data/KuaiRand/1K"  # 1K数据在KuaiRand/1K目录下
  cache_path: "data/KuaiRand/cache"
  # --- 新增: DataLoader优化参数 ---
  num_workers: 32  # 使用64个CPU核心进行并行数据加载
  pin_memory: true # 锁定内存，加速CPU到GPU的数据传输
  # --- 新增: 数据分块配置 ---
  chunking:
    enabled: true  # 设置为 true 来启用分块功能
    num_chunks: 20 # 将用户数据分割成20个块
    # 用于拟合特征处理器的采样比例（例如，使用20%的数据来学习标准化参数）
    # 这有助于在处理非常大的数据集时避免将所有数据加载到内存中进行拟合
    fit_sample_ratio: 0.2

# dataset:
#   name: "KuaiRand-Pure"
#   path: "data/KuaiRand/Pure"  # 1K数据在KuaiRand/1K目录下
#   cache_path: "data/KuaiRand/cache"
#   # --- 新增: DataLoader优化参数 ---
#   num_workers: 96  # 使用32个CPU核心进行并行数据加载
#   pin_memory: true # 锁定内存，加速CPU到GPU的数据传输

# 启用混合精度训练
use_amp: true

# 特征配置（27K数据集特征）
feature:
  numerical:
    - "video_duration"
    - "server_width"
    - "server_height"
    - "show_cnt"
    - "play_cnt"
    - "play_user_num"
    - "complete_play_cnt"
    - "like_cnt"
    - "comment_cnt"
    - "share_cnt"
    - "collect_cnt"
    - "is_live_streamer"
    - "is_video_author"
    - "follow_user_num"
    - "fans_user_num"
    - "friend_user_num"
    - "register_days"
  categorical:
    - "user_active_degree"
    - "video_type"
    - "tag"

# 标签配置（调整了模型参数以提升效果）
labels:
  - name: "play_time"
    target: "play_time_ms"
    type: "numerical"
    loss_function: "logMAE"
    model: "MLP"
    model_params:
      hidden_layers: [256, 128, 64, 32]  # 增加模型容量
      dropout: 0.3  # 增加dropout防止过拟合
      # dropout: 0.0 # dml会减速
      embedding_dim: 32  # 增加嵌入维度
    # --- New: Configurable Optimizer ---
    optimizer:
      name: "Lookahead" # Options: Adam, Lookahead
      learning_rate: 0.00001
      weight_decay: 0.000005
      params: # Optimizer-specific parameters
        k: 5
        alpha: 0.5
    # --- New: Configurable Scheduler ---
    scheduler:
      name: "CosineAnnealingLR" # Options: ReduceLROnPlateau, CosineAnnealingLR
      params:
        T_max: 10000000 # Set to total expected iterations
        eta_min: 0.000001
    # removed legacy top-level learning_rate/weight_decay (use optimizer block above)
    alpha_T: 1.0
    alpha_C: 0.5
    
  - name: "click"
    target: "is_click"
    type: "binary"
    loss_function: "BCE"
    model: "MLP"
    model_params:
      hidden_layers: [128, 64, 32, 16]  # 增加模型容量
      dropout: 0.2  # 适度dropout
      # dropout: 0.0
      embedding_dim: 16  # 增加嵌入维度
    # --- New: Configurable Optimizer ---
    optimizer:
      name: "Adam" # Options: Adam, Lookahead
      learning_rate: 0.0001
      weight_decay: 0.00005
      params: {}
    # --- New: Configurable Scheduler ---
    scheduler:
      name: "ReduceLROnPlateau" # Options: ReduceLROnPlateau, CosineAnnealingLR
      params:
        mode: 'min'
        factor: 0.5
        patience: 5
    # removed legacy top-level learning_rate/weight_decay (use optimizer block above)
    alpha_T: 1.0
    alpha_C: 0.8

# 预训练配置（优化）
pretrain:
  enabled: true
  batch_size: 1024  # 增加batch size以更好利用GPU
  epochs: 150  
  # learning_rate and weight_decay removed here in favor of per-label optimizer blocks
  # (Use label.optimizer.learning_rate and label.optimizer.weight_decay)
  early_stopping: 10
  # --- 新增配置 ---
  # 指定要加载的预训练权重文件路径，如果为null则不加载
  load_checkpoint_path: null  # "results/YYYYMMDD_HHMM/checkpoints/pretrain_latest.pt"
  # 预训练数据的验证集划分比例
  val_split_ratio: 0.5
  # 是否在每个epoch后绘制并保存损失曲线图
  plot_loss_curves: true
  # --- 新增：基于迭代次数的验证频率 ---
  validate_every_iters: 500

# 全局仿真配置（优化）
global:
  user_p_val: 0.2
  batch_size: 128  # 增加batch size
  n_candidate: 10
  n_steps: 5  # 减少步数以便快速测试优化效果
  validate_every: 1  # 更频繁的验证
  save_every: 25
  # learning_rate and weight_decay removed from global config; use per-label optimizer settings

# 日志配置
logging:
  level: "INFO"
  log_dir: "results"

# --- 新增：验证配置 ---
validation:
  validation_batches: 100  # 限制验证为100个批次以提高速度
  # --- New: Weighted score for determining the best overall model ---
  # The final score is the sum of (metric * weight). A higher score is better.
  # This allows you to balance different aspects of model performance.
  #
  # Available metrics to use (based on your current code):
  #   - val_play_time_loss
  #   - val_click_loss
  #   - val_click_accuracy
  #   - val_click_auc
  #
  best_overall_score:
    # For LOSS metrics, use a NEGATIVE weight because a lower loss is better.
    # A weight of -0.4 means that a decrease of 0.1 in loss will increase the score by 0.04.
    val_play_time_loss: -0.1
    val_click_loss: -0.4

    # For ACCURACY and AUC metrics, use a POSITIVE weight because higher is better.
    # A weight of +0.2 means that an increase of 0.1 in accuracy will increase the score by 0.02.
    val_click_accuracy: 0.2
    val_click_auc: 0.3
    val_play_time_mape: -0.2 # Added the new MAPE metric

