#!/usr/bin/env python3
"""
GPUè®¾ç½®æµ‹è¯•è„šæœ¬
å¿«é€ŸéªŒè¯GPUé…ç½®æ˜¯å¦æ­£ç¡®
"""

import os
import sys
import yaml
import torch
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(__file__)
sys.path.insert(0, project_root)

def test_basic_setup():
    """æµ‹è¯•åŸºæœ¬è®¾ç½®"""
    print("=" * 60)
    print("åŸºæœ¬ç¯å¢ƒæµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥PyTorchå’ŒCUDA
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.get_device_name()}")
        
        # GPUå†…å­˜æµ‹è¯•
        device = torch.device('cuda')
        x = torch.randn(100, 100, device=device)
        y = torch.randn(100, 100, device=device)
        z = torch.mm(x, y)
        print(f"âœ“ GPUè®¡ç®—æµ‹è¯•é€šè¿‡")
        
        # æ¸…ç†GPUå†…å­˜
        del x, y, z
        torch.cuda.empty_cache()
    else:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")

def test_config_loading():
    """æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½"""
    print("\n" + "=" * 60)
    print("é…ç½®æ–‡ä»¶æµ‹è¯•")
    print("=" * 60)
    
    config_path = "configs/experiment.yaml"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        print(f"å®éªŒæ¨¡å¼: {config['mode']}")
        print(f"è®¾å¤‡é…ç½®: {config.get('device', 'auto')}")
        print(f"æ•°æ®é›†: {config['dataset']['name']}")
        
        return config
    except Exception as e:
        print(f"âœ— é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return None

def test_device_selection(config):
    """æµ‹è¯•è®¾å¤‡é€‰æ‹©é€»è¾‘"""
    print("\n" + "=" * 60)
    print("è®¾å¤‡é€‰æ‹©æµ‹è¯•")
    print("=" * 60)
    
    if config is None:
        print("âœ— è·³è¿‡è®¾å¤‡é€‰æ‹©æµ‹è¯•ï¼ˆé…ç½®åŠ è½½å¤±è´¥ï¼‰")
        return
    
    device_config = config.get('device', 'auto')
    print(f"é…ç½®ä¸­çš„è®¾å¤‡è®¾ç½®: {device_config}")
    
    # æ¨¡æ‹ŸGlobalModeçš„è®¾å¤‡é€‰æ‹©é€»è¾‘
    if device_config == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    elif device_config == 'cuda':
        if torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œé€€å›åˆ°CPU")
            device = torch.device('cpu')
    else:
        device = torch.device(device_config)
    
    print(f"é€‰æ‹©çš„è®¾å¤‡: {device}")
    
    # æµ‹è¯•æ¨¡å‹åˆ›å»º
    try:
        import torch.nn as nn
        model = nn.Linear(10, 1).to(device)
        print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè®¾å¤‡: {next(model.parameters()).device}")
        
        # æµ‹è¯•æ•°æ®ä¼ è¾“
        x = torch.randn(5, 10).to(device)
        y = model(x)
        print(f"âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {y.shape}")
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")

def test_data_paths():
    """æµ‹è¯•æ•°æ®è·¯å¾„"""
    print("\n" + "=" * 60)
    print("æ•°æ®è·¯å¾„æµ‹è¯•")
    print("=" * 60)
    
    data_dir = "data/KuaiRand/Pure"
    cache_dir = "data/KuaiRand/cache"
    
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
    
    if os.path.exists(data_dir):
        print("âœ“ æ•°æ®ç›®å½•å­˜åœ¨")
        files = os.listdir(data_dir)
        print(f"æ•°æ®æ–‡ä»¶æ•°: {len(files)}")
    else:
        print("âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨")
    
    if os.path.exists(cache_dir):
        print("âœ“ ç¼“å­˜ç›®å½•å­˜åœ¨")
    else:
        print("âš ï¸ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("RealdataEXP GPUè®¾ç½®æµ‹è¯•")
    print("æ—¶é—´:", torch.datetime.now() if hasattr(torch, 'datetime') else "æœªçŸ¥")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_basic_setup()
    config = test_config_loading()
    test_device_selection(config)
    test_data_paths()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    
    if torch.cuda.is_available():
        print("ğŸ‰ GPUç¯å¢ƒé…ç½®æ­£ç¡®ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œå®éªŒ:")
        print("   sbatch run_gpu.sh")
        print("   æˆ–")
        print("   bash run_interactive_gpu.sh")
    else:
        print("â„¹ï¸ å½“å‰åœ¨CPUç¯å¢ƒï¼Œè¦ä½¿ç”¨GPUè¯·åœ¨GPUèŠ‚ç‚¹ä¸Šè¿è¡Œæ­¤æµ‹è¯•")

if __name__ == "__main__":
    main()