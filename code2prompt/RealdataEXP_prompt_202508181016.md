# Table of Contents
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\.gitignore
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\debug_ipex.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\Exp logs.md
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\experiment_yanc.yaml
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\GPU_OPTIMIZATION_GUIDE.md
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\main.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\performance_analysis.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\README.md
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\requirements.txt
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\run_gpu.sh
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\run_gpu_optimized.sh
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\run_gpu_yanc.sh
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\run_windows.bat
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\configs\experiment.yaml
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\configs\experiment_optimized copy.yaml
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\configs\experiment_optimized.yaml
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\configs\experiment_yanc.yaml
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\__init__.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\data\cache_manager.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\data\data_loader.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\data\feature_processor.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\data\__init__.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\models\loss_functions.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\models\mlp_model.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\models\multi_label_model.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\models\__init__.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\modes\global_mode.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\modes\global_mode_optimized.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\modes\__init__.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\device_utils.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\experiment_utils.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\gpu_utils.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\logger.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\metrics.py
- E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\__init__.py

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\.gitignore

- Extension: 
- Language: unknown
- Size: 513 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```unknown
 1 | KuaiRand/
 2 | 
 3 | # Python cache
 4 | __pycache__/
 5 | *.pyc
 6 | *.pyo
 7 | *.pyd
 8 | 
 9 | # Jupyter Notebook checkpoints
10 | .ipynb_checkpoints/
11 | 
12 | # VSCode
13 | .vscode/
14 | *.code-workspace
15 | 
16 | # Conda/venv environments
17 | .conda/
18 | venv/
19 | env/
20 | ENV/
21 | # Conda/miniconda environments and installer
22 | envs/
23 | miniconda3/
24 | Miniconda3-latest-Linux-x86_64.sh
25 | 
26 | 
27 | # Model checkpoints and results
28 | results/[0-9]*/
29 | results/[0-9]*/checkpoints/
30 | *.pt
31 | *.pth
32 | 
33 | # Data archives
34 | *.tar.gz
35 | *.zip
36 | *.rar
37 | 
38 | # System files
39 | .DS_Store
40 | Thumbs.db
41 | 
42 | # Logs
43 | *.log
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\debug_ipex.py

- Extension: .py
- Language: python
- Size: 1451 bytes
- Created: 2025-08-17 23:02:15
- Modified: 2025-08-17 23:02:23

### Code

```python
 1 | import sys
 2 | import os
 3 | 
 4 | # æ¨¡æ‹Ÿ main.py çš„é¡¹ç›®è·¯å¾„è®¾ç½®
 5 | project_root = os.path.dirname(__file__)
 6 | sys.path.insert(0, project_root)
 7 | 
 8 | print("--- Starting IPEX Import Test ---")
 9 | print(f"Python executable: {sys.executable}")
10 | print(f"Python version: {sys.version}")
11 | 
12 | # --- æ­¥éª¤ 1: æ£€æŸ¥ Torch ---
13 | try:
14 |     print("\nAttempting to import torch...")
15 |     import torch
16 |     print(f"Torch version: {torch.__version__}")
17 |     print("SUCCESS: Torch import successful.")
18 | except Exception as e:
19 |     print(f"FATAL ERROR importing torch: {e}")
20 |     sys.exit(1)
21 | 
22 | # --- æ­¥éª¤ 2: æ£€æŸ¥ IPEX ---
23 | try:
24 |     print("\nAttempting to import intel_extension_for_pytorch...")
25 |     import intel_extension_for_pytorch as ipex
26 |     print(f"IPEX version: {ipex.__version__}")
27 |     print("SUCCESS: IPEX import successful.")
28 | except Exception as e:
29 |     print(f"FATAL ERROR importing intel_extension_for_pytorch: {e}")
30 |     # æ‰“å°æ›´è¯¦ç»†çš„è·¯å¾„ä¿¡æ¯ï¼Œå¸®åŠ©è¯Šæ–­
31 |     import traceback
32 |     traceback.print_exc()
33 |     sys.exit(1)
34 | 
35 | # --- æ­¥éª¤ 3: æ£€æŸ¥ XPU å¯ç”¨æ€§ ---
36 | print("\nChecking torch.xpu.is_available()...")
37 | try:
38 |     available = torch.xpu.is_available()
39 |     print(f"torch.xpu.is_available() returned: {available}")
40 |     if not available:
41 |         print("WARNING: IPEX imported but XPU device is not available!")
42 | except Exception as e:
43 |     print(f"ERROR checking torch.xpu.is_available(): {e}")
44 | 
45 | print("\n--- Test Finished ---")
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\Exp logs.md

- Extension: .md
- Language: markdown
- Size: 10651 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-18 00:23:53

### Code

```markdown
  1 | ## 2025-08-17
  2 | 
  3 | ### å¤šåç«¯GPUæ”¯æŒè°ƒè¯•ä¸æˆåŠŸè¿è¡Œ
  4 | **æ–°å¢æ ¸å¿ƒåŠŸèƒ½**ï¼š
  5 | 1. **ç»Ÿä¸€è®¾å¤‡é€‰æ‹©å·¥å…· (`device_utils.py`)**ï¼š
  6 |    - æ”¯æŒè‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨ç¡¬ä»¶ï¼š`cuda -> ipex -> xpu -> dml -> cpu`
  7 |    - åŒºåˆ†Intel IPEXå®Œå…¨ä¼˜åŒ–æ¨¡å¼(`ipex`)å’ŒåŸºç¡€XPUè®¾å¤‡æ”¾ç½®æ¨¡å¼(`xpu`)
  8 |    - è‡ªåŠ¨å¤„ç†AMPæ··åˆç²¾åº¦è®­ç»ƒçš„å…¼å®¹æ€§
  9 |    - æä¾›è™šæ‹ŸGradScalerå’Œautocastå­˜æ ¹ï¼Œç¡®ä¿æ‰€æœ‰åç«¯çš„ç»Ÿä¸€æ¥å£
 10 | åœ¨Windowsä¸ŠæˆåŠŸè¿è¡ŒDirectML (DMLï¼Œç–‘ä¼¼åœæ­¢ç»´æŠ¤)å’ŒIntel IPEX (XPU)åç«¯ã€‚è°ƒè¯•è¿‡ç¨‹ä¸€æ³¢ä¸‰æŠ˜ï¼Œä½†æœ€ç»ˆå–å¾—äº†æˆåŠŸã€‚è¡¨ç°å‚æ•°åœ¨tricksé‡Œé¢
 11 | 
 12 | #### è°ƒè¯•å†ç¨‹æ€»ç»“
 13 | 
 14 | 1.  **Condaç¯å¢ƒæ¿€æ´»å¤±è´¥**: æœ€åˆçš„`run_windows.bat`è„šæœ¬å› æ— æ³•é€šè¿‡è·¯å¾„æ­£ç¡®æ¿€æ´»Condaç¯å¢ƒè€Œå‡ºé”™ã€‚
 15 |     * **è§£å†³æ–¹æ¡ˆ**: ä¿®æ”¹è„šæœ¬ï¼Œä½¿ç”¨`conda activate --prefix "PATH"`å‘½ä»¤ï¼Œæ˜ç¡®æŒ‡å®šç¯å¢ƒè·¯å¾„è€Œéåç§°ã€‚
 16 | 
 17 | 2.  **DirectMLæ€§èƒ½é—®é¢˜ä¸ç®—å­å›é€€**:
 18 |     * **ç°è±¡**: å¼€å¯DMLåï¼Œè¿è¡Œé€Ÿåº¦è¿œä½äºçº¯CPUã€‚
 19 |     * **åŸå› **: æ—¥å¿—è­¦å‘Šæ˜¾ç¤ºï¼Œæ¨¡å‹ä¸­çš„`Dropout`å±‚å’Œ`clip_grad_norm_`æ¢¯åº¦è£å‰ªå‡½æ•°ä¸è¢«DMLåç«¯æ”¯æŒï¼Œå¯¼è‡´è®¡ç®—ä»»åŠ¡é¢‘ç¹åœ°ä»GPUâ€œå›é€€â€åˆ°CPUæ‰§è¡Œï¼Œè®¾å¤‡é—´çš„æ•°æ®æ‹·è´å¸¦æ¥äº†å·¨å¤§æ€§èƒ½å¼€é”€ã€‚
 20 |     * **ç»“è®º**: å¯¹äºå½“å‰æ¨¡å‹ï¼ŒDMLå°šä¸æˆç†Ÿï¼Œçº¯CPUæ˜¯æ›´ä¼˜é€‰æ‹©ã€‚
 21 | 
 22 | 3.  **IPEXç¯å¢ƒå®‰è£…ä¸DLLå†²çª**:
 23 |     * **ç°è±¡**: IPEXç¯å¢ƒæ— æ³•æ­£ç¡®åŠ è½½ï¼Œå‡ºç°`OSError: [WinError 127] æ‰¾ä¸åˆ°æŒ‡å®šçš„ç¨‹åº`ï¼ŒæŒ‡å‘`torch_python.dll`ã€‚
 24 |     * **åŸå› åˆ†æä¸æ€»ç»“**:
 25 |         > **æ ¸å¿ƒé—®é¢˜ï¼š`pip`ä¸`conda`ä¾èµ–ç®¡ç†å†²çª**
 26 |         > `pip install torch`ä¹‹åå†æ‰§è¡Œ`conda install scikit-learn`çš„å®‰è£…é¡ºåºæ˜¯å¯¼è‡´DLLåŠ è½½é”™è¯¯çš„ç›´æ¥åŸå› ã€‚
 27 |         > **æŠ€æœ¯åŸå› **: `conda`ä¸ä»…ç®¡ç†PythonåŒ…ï¼Œè¿˜ç®¡ç†å…¶åº•å±‚çš„éPythonä¾èµ–ï¼ˆå¦‚MKLæ•°å­¦åº“ã€C++è¿è¡Œæ—¶ï¼‰ã€‚è€Œ`pip`åªç®¡ç†PythonåŒ…ã€‚å½“`conda`å®‰è£…`scikit-learn`æ—¶ï¼Œå®ƒå¯èƒ½ä¼šä¸ºäº†æ»¡è¶³è‡ªèº«ä¾èµ–è€Œæ›´æ”¹ä¸€ä¸ªåº•å±‚åº“ï¼Œè¿™ä¸ªæ›´æ”¹æ°å¥½ä¸`pip`å®‰è£…çš„PyTorchæ‰€ä¾èµ–çš„åº•å±‚åº“ç‰ˆæœ¬å†²çªï¼Œä»è€Œå¯¼è‡´PyTorchæ— æ³•æ‰¾åˆ°æ‰€éœ€çš„DLLã€‚
 28 |         > **æ­£ç¡®çš„å®‰è£…ç­–ç•¥**:
 29 |         > 1.  **Condaä¼˜å…ˆ**: å°½å¯èƒ½ä½¿ç”¨`conda install`å®‰è£…æ‰€æœ‰ç§‘å­¦è®¡ç®—åŒ…ï¼Œæœ€å¥½åœ¨åˆ›å»ºç¯å¢ƒæ—¶é€šè¿‡ä¸€æ¡å‘½ä»¤å®Œæˆã€‚
 30 |         > 2.  **Pipå¤‡é€‰**: ä»…åœ¨Condaæ¸ é“æ— æ³•æ‰¾åˆ°æŸä¸ªåŒ…æ—¶ï¼Œæ‰ä½¿ç”¨`pip`ä½œä¸ºè¡¥å……ã€‚
 31 |         > **æœ€ç»ˆè§£å†³æ–¹æ¡ˆ (å®è·µ)**: ç”±äºè®¿é—®Intelçš„Condaæ¸ é“å­˜åœ¨ç½‘ç»œé—®é¢˜ï¼Œæœ€ç»ˆæˆåŠŸçš„ç­–ç•¥æ˜¯ï¼šåˆ›å»ºä¸€ä¸ªåªåŒ…å«Pythonçš„æœ€å°åŒ–Condaç¯å¢ƒï¼Œç„¶åå®Œå…¨ä½¿ç”¨`pip`å¹¶æŒ‡å®šæ­£ç¡®çš„XPUæºæ¥å®‰è£…PyTorchã€IPEXåŠå…¶ä»–æ‰€æœ‰ä¾èµ–ã€‚è¿™ç¡®ä¿äº†ç¯å¢ƒä¸­æ‰€æœ‰åŒ…çš„ä¾èµ–å…³ç³»éƒ½ç”±`pip`ç»Ÿä¸€ç®¡ç†ï¼Œä»è€Œé¿å…äº†å†²çªã€‚
 32 | 
 33 | 4.  **IPEX APIåŠä»£ç å…¼å®¹æ€§é—®é¢˜**:
 34 |     * **ç°è±¡**: æˆåŠŸå®‰è£…IPEXæˆ–å›é€€åˆ°CPUæ—¶ï¼Œå‡ºç°`ImportError` (æ— æ³•å¯¼å…¥`GradScaler`)ã€`TypeError` (autocastå‚æ•°é”™è¯¯, StubScalerè¿”å›å®ä¾‹è€Œéç±»)åŠ`FutureWarning` (æ—§ç‰ˆautocast API)ã€‚
 35 |     * **åŸå› **: IPEXçš„AMPå®ç°ä¸CUDAä¸åŒï¼Œä¸ä½¿ç”¨`GradScaler`è€Œæ˜¯é€šè¿‡`ipex.optimize()`å‡½æ•°æ¥ç®¡ï¼›å…¶`autocast`å‡½æ•°ç­¾åä¹Ÿä¸CUDAç‰ˆæœ¬æœ‰å·®å¼‚ï¼›ä»£ç åœ¨å›é€€è·¯å¾„ä¸­å­˜åœ¨é€»è¾‘é”™è¯¯ã€‚
 36 |     * **è§£å†³æ–¹æ¡ˆä¸å…¼å®¹æ€§æå‡**:
 37 |         * ä¿®æ”¹`device_utils.py`ï¼Œä½¿å…¶åœ¨IPEXæ¨¡å¼ä¸‹ä¸å†å°è¯•å¯¼å…¥`GradScaler`ï¼Œå¹¶ä¿®å¤`StubScaler`è¿”å›ç±»è€Œéå®ä¾‹çš„`TypeError`ã€‚
 38 |         * åœ¨`global_mode_optimized.py`ä¸­ï¼Œå½“æ£€æµ‹åˆ°è®¾å¤‡ä¸º`xpu`æ—¶ï¼Œæ˜¾å¼è°ƒç”¨`ipex.optimize()`æ¥ä¼˜åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨ã€‚
 39 |         * ä¿®æ”¹`_perform_training_step`å‡½æ•°ï¼Œä½¿å…¶åœ¨è°ƒç”¨`autocast`æ—¶èƒ½å…¼å®¹CUDAå’ŒIPEXçš„ä¸åŒå‚æ•°ï¼Œå¹¶ç»Ÿä¸€ä½¿ç”¨æ–°ç‰ˆAPIä»¥æ›¿æ¢å·²å¼ƒç”¨çš„`torch.cuda.amp.autocast`è°ƒç”¨ï¼Œä¿®å¤`FutureWarning`è­¦å‘Šã€‚
 40 | 
 41 | 
 42 | 5.  **æœ€ç»ˆæˆåŠŸ**: åœ¨ä¿®å¤äº†`multi_label_model.py`ä¸­ç¼ºå¤±çš„`set_train_mode`è¾…åŠ©å‡½æ•°åï¼Œé¡¹ç›®åœ¨IPEXåç«¯ä¸ŠæˆåŠŸå¼€å§‹è®­ç»ƒã€‚ğŸš€
 43 | 
 44 | ---
 45 | ## 20250803
 46 | 
 47 | ### Claude Code
 48 | 
 49 | <instruction>
 50 | deep thinkï¼šæˆ‘å¸Œæœ›ä½ å¸®æˆ‘è§£å†³gpuå’Œcpuåˆ©ç”¨æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒå¤ªæ…¢çš„é—®é¢˜ã€‚åœ¨ä½ æ¥æ‰‹é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘å·²ç»åšäº†ä¸€æ¬¡å°è¯•ï¼Œä½†å‡ºç°äº†bugã€‚ä½ å¯ä»¥é€‰æ‹©ã€è€ƒè™‘è‡ªå·±å¯»æ‰¾ä¼˜åŒ–æ–¹æ³•ï¼Œæˆ–è€…åœ¨è¿™ä¸ªæœ‰bugçš„æ–¹æ¡ˆåŸºç¡€ä¸Šä¿®å¤
 51 | </instruction>
 52 | 
 53 | <context>
 54 | /home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP/results/gpu_run_52019_detailed.log ç»“åˆé¢„è®­ç»ƒæ¯ä¸€ä¸ªepochèŠ±äº†20minï¼Œç³»ç»Ÿè´Ÿè½½: 1.12 (1åˆ†é’Ÿå¹³å‡)
 55 | CPUæ€»ä½“: 1.7% user, 98.2% idle
 56 | å®éªŒè¿›ç¨‹: 99.7% CPUä½¿ç”¨ç‡ (å•æ ¸æ»¡è½½)å’Œnvidia-smiæ˜¾ç¤ºgpuä½å ç”¨ï¼Œæˆ‘æ€€ç–‘æ²¡æœ‰æ­£ç¡®ä½¿ç”¨gpuè¿›è¡Œè®­ç»ƒï¼Œæˆ–è€…æ˜¯åœ¨å…¶ä»–ç¯èŠ‚å ç”¨äº†å¤ªå¤šæ—¶é—´ï¼ˆå¦‚cpuæ²¡æœ‰å¤šçº¿ç¨‹å·¥ä½œï¼‰ï¼Œè¯·ä½ è¯Šæ–­å¹¶æ”¹å–„
 57 | 
 58 | å·²åˆ›å»ºçš„ä¼˜åŒ–æ–‡ä»¶ï¼š
 59 | âœ… libs/modes/global_mode_optimized.py - ä¼˜åŒ–è®­ç»ƒå¼•æ“
 60 | âœ… configs/experiment_optimized.yaml - ä¼˜åŒ–é…ç½®
 61 | âœ… run_gpu_optimized.sh - ä¼˜åŒ–GPUä½œä¸šè„šæœ¬
 62 | âœ… monitor_gpu_optimized.sh - æ€§èƒ½ç›‘æ§å·¥å…·
 63 | âœ… performance_analysis.py - æ€§èƒ½å¯¹æ¯”åˆ†æ
 64 | âœ… GPU_OPTIMIZATION_GUIDE.md - å®Œæ•´ä½¿ç”¨æŒ‡å—
 65 | 
 66 | ä¼˜åŒ–åå®éªŒæ—¥å¿—ï¼š /home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP/results/gpu_run_52022_detailed.log
 67 | </context>
 68 | 
 69 | claude codeä¸ºæˆ‘ä¿®å¤åï¼Œplaytime losså‡ºç°äº†infï¼›å®éªŒè¿›å…¥åˆ°Trratmentå®éªŒé˜¶æ®µï¼Œæš´éœ²å‡ºæ–°çš„bugï¼Œå‚è§/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP/results/gpu_run_52049_detailed.log
 70 | 
 71 | ---
 72 | ## 20250803ï¼ˆcontinuedï¼‰
 73 | 
 74 | ### å…³é”®Bugä¿®å¤å’ŒJob 52068çŠ¶æ€æ›´æ–°
 75 | 
 76 | #### é—®é¢˜èƒŒæ™¯
 77 | 
 78 | ä»Job 52049æš´éœ²å‡ºçš„å…³é”®bugï¼š
 79 | 
 80 | 1.  **List Index Out of Range Error** - åœ¨ä»¿çœŸæ­¥éª¤ä¸­å‡ºç°ç´¢å¼•è¶Šç•Œé”™è¯¯
 81 | 2.  **Shape Mismatch in Batch Training** - ç‰¹å¾æ ·æœ¬æ•°ä¸æ ‡ç­¾æ ·æœ¬æ•°ä¸åŒ¹é…
 82 | 3.  **Tensor Conversion Error** - å¤šå…ƒç´ å¼ é‡è½¬æ¢ä¸ºæ ‡é‡å¤±è´¥
 83 | 
 84 | #### å®æ–½çš„å…³é”®ä¿®å¤
 85 | 
 86 | **ä¿®å¤1: å¼ é‡å½¢çŠ¶éªŒè¯å’Œå®‰å…¨ç´¢å¼• (global_mode_optimized.py:509-529)**
 87 | 
 88 | 
 89 | ç¡®ä¿combined_scoresæ˜¯æ­£ç¡®çš„å½¢çŠ¶å¹¶è·å–æœ‰æ•ˆç´¢å¼•
 90 | scores_squeezed = combined_scores.squeeze()
 91 | if scores_squeezed.dim() == 0:
 92 | scores_squeezed = scores_squeezed.unsqueeze(0)
 93 | elif scores_squeezed.dim() > 1:
 94 | scores_squeezed = scores_squeezed.flatten()
 95 | 
 96 | ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
 97 | if len(scores_squeezed) != len(candidates):
 98 | logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] åˆ†æ•°å¼ é‡é•¿åº¦ {len(scores_squeezed)} ä¸å€™é€‰è§†é¢‘æ•° {len(candidates)} ä¸åŒ¹é…")
 99 | safe_length = min(len(scores_squeezed), len(candidates))
100 | if safe_length == 0:
101 | continue  # è·³è¿‡è¿™ä¸ªç”¨æˆ·
102 | winner_idx = torch.argmax(scores_squeezed[:safe_length]).item()
103 | else:
104 | winner_idx = torch.argmax(scores_squeezed).item()
105 | 
106 | 
107 | **ä¿®å¤2: ç‰¹å¾-æ ‡ç­¾å¤§å°éªŒè¯å’ŒåŠ¨æ€è°ƒæ•´ (global_mode_optimized.py:565-593)**
108 | 
109 | 
110 | éªŒè¯ç‰¹å¾å’Œæ ‡ç­¾çš„æ ·æœ¬æ•°é‡æ˜¯å¦ä¸€è‡´
111 | n_features = all_features.size(0)
112 | n_targets = combined_targets[list(combined_targets.keys())[0]].size(0)
113 | 
114 | if n_features != n_targets:
115 | logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] ç‰¹å¾æ ·æœ¬æ•° {n_features} ä¸æ ‡ç­¾æ ·æœ¬æ•° {n_targets} ä¸åŒ¹é…ï¼Œè°ƒæ•´æ‰¹é‡è®­ç»ƒ")
116 | min_samples = min(n_features, n_targets)
117 | if min_samples == 0:
118 | logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ç”¨äºè®­ç»ƒï¼Œè·³è¿‡æ‰¹é‡è®­ç»ƒ")
119 | else:
120 | # è°ƒæ•´å¼ é‡å¤§å°
121 | all_features = all_features[:min_samples]
122 | for label_name in combined_targets:
123 | combined_targets[label_name] = combined_targets[label_name][:min_samples]
124 | 
125 | 
126 | **ä¿®å¤3: å¼ é‡è½¬æ¢å®‰å…¨å¤„ç† (global_mode_optimized.py:542-548)**
127 | 
128 | 
129 | ç¡®ä¿å¼ é‡æ˜¯æ ‡é‡ï¼Œç„¶åæå–å€¼
130 | if label_tensor.numel() == 1:
131 | reward_value = label_tensor.item()
132 | else:
133 | # å¦‚æœå¼ é‡æœ‰å¤šä¸ªå…ƒç´ ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ æˆ–æ±‚å’Œ
134 | reward_value = label_tensor.sum().item()
135 | 
136 | 
137 | #### Job 52068å½“å‰çŠ¶æ€
138 | 
139 | * **ä½œä¸šID**: 52068
140 | * **å½“å‰è¿›åº¦**: Step 72/100 (72%å®Œæˆ)
141 | * **å¼€å§‹æ—¶é—´**: 2025-08-03 18:06:18
142 | * **å·²è¿è¡Œæ—¶é—´**: çº¦3å°æ—¶46åˆ†é’Ÿ
143 | * **é¢„è®¡å®Œæˆ**: çº¦1å°æ—¶å
144 | 
145 | #### å…³é”®æŒ‡æ ‡ï¼ˆæˆªè‡³Step 71ï¼‰
146 | 
147 | * **Treatmentç»„æ€»æ’­æ”¾æ—¶é•¿**: 49,577,210ms
148 | * **Controlç»„æ€»æ’­æ”¾æ—¶é•¿**: 44,265,970ms
149 | * **Treatmentç»„æ€»ç‚¹å‡»æ•°**: 1,054
150 | * **Controlç»„æ€»ç‚¹å‡»æ•°**: 1,004
151 | * **å½“å‰GTEè¶‹åŠ¿**: Treatmentç»„è¡¨ç°ä¼˜äºControlç»„
152 | 
153 | #### é‡è¦è§‚å¯Ÿ
154 | 
155 | 1.  **ä¿®å¤æ•ˆæœ**: æ‰€æœ‰ä¹‹å‰çš„å…³é”®é”™è¯¯å·²è§£å†³ï¼Œä½œä¸šç¨³å®šè¿è¡Œ
156 | 2.  **æ•°æ®è€—å°½ç°è±¡**: ä»Step 66å¼€å§‹ï¼Œæ¯æ­¥å¤„ç†ç”¨æˆ·æ•°é™ä¸º0ï¼Œè¯´æ˜å¯ç”¨è§†é¢‘èµ„æºæ¥è¿‘è€—å°½
157 | 3.  **æ€§èƒ½ç¨³å®š**: æ²¡æœ‰å‡ºç°æ–°çš„é”™è¯¯æˆ–è­¦å‘Šï¼Œç³»ç»Ÿè¿è¡Œç¨³å®š
158 | 4.  **é¢„æœŸç»“æœ**: æŒ‰å½“å‰è¶‹åŠ¿ï¼Œå®éªŒå°†æˆåŠŸå®Œæˆå¹¶è¾“å‡ºGTEåˆ†æç»“æœ
159 | 
160 | #### æŠ€æœ¯æ”¹è¿›éªŒè¯
161 | 
162 | * âœ… **ç´¢å¼•å®‰å…¨**: å½»åº•è§£å†³äº†list index out of rangeé”™è¯¯
163 | * âœ… **å½¢çŠ¶åŒ¹é…**: è‡ªåŠ¨å¤„ç†ç‰¹å¾-æ ‡ç­¾ç»´åº¦ä¸ä¸€è‡´é—®é¢˜
164 | * âœ… **å¼ é‡è½¬æ¢**: å®‰å…¨å¤„ç†å¤šå…ƒç´ å¼ é‡è½¬æ¢
165 | * âœ… **é”™è¯¯æ¢å¤**: å¢å¼ºçš„è­¦å‘Šå’Œè·³è¿‡æœºåˆ¶
166 | * âœ… **ç¨³å®šæ€§**: é•¿æ—¶é—´è¿è¡Œæ— å´©æºƒ
167 | 
168 | #### åç»­å»ºè®®
169 | 
170 | 1.  **æ•°æ®èµ„æº**: è€ƒè™‘å¢åŠ å€™é€‰è§†é¢‘æ± æˆ–å®ç°è§†é¢‘é‡ç”¨æœºåˆ¶
171 | 2.  **æ—©æœŸåœæ­¢**: å½“è¿ç»­å¤šæ­¥å¤„ç†ç”¨æˆ·æ•°ä¸º0æ—¶ï¼Œå¯è€ƒè™‘æå‰ç»“æŸå®éªŒ
172 | 3.  **ç›‘æ§ä¼˜åŒ–**: æ·»åŠ èµ„æºä½¿ç”¨ç‡ç›‘æ§ï¼Œä¾¿äºåˆ†ææ•°æ®è€—å°½æ¨¡å¼
173 | 
174 | ---
175 | <instruction>
176 | åœ¨gpué›†ç¾¤ä¸Šç»§ç»­å®éªŒï¼Œå¹¶æ ¹æ®æœ€æ–°logå†³å®šä¸‹ä¸€æ­¥
177 | </instruction>
178 | 
179 | <context>
180 | ä½ å¯ä»¥é€šè¿‡READMEå’Œ/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP/Exp logs.mdç†Ÿæ‚‰è¯¥é¡¹ç›®å’Œå†å²æ›´æ”¹ã€‚æœ€æ–°çš„ä¸€æ­¥ï¼Œclaude codeä¸ºæˆ‘åœ¨logæ—¥å¿—è¾“å‡ºå¢åŠ äº†playtimeçš„è¯„ä»·ç›¸å¯¹è¯¯å·®æŒ‡æ ‡ï¼Œå¹¶å°è¯•ä¿®å¤äº†æ–°çš„bugï¼Œè¿˜æ²¡æœ‰åœ¨æœåŠ¡å™¨ä¸Šå°è¯•è¿‡
181 | @README ### 1. æäº¤GPUä½œä¸š ### 2. æŸ¥çœ‹ä½œä¸šçŠ¶æ€ ### 3. è¿æ¥GPUèŠ‚ç‚¹
182 | </context>
183 | 
184 | ---
185 | <instruction>
186 | deep thinking:
187 | è§£å†³contextæ‰€è¯´çš„é—®é¢˜ï¼›
188 | å¦å¤–ï¼Œæˆ‘æ€€ç–‘è®­ç»ƒè¿‡ç¨‹æ²¡æœ‰çœŸæ­£åœ¨gpuä¸Šè¿›è¡Œè¿ç®—ï¼Œè¯·ä½ æƒ³åŠæ³•è°ƒè¯•æ£€æŸ¥è¿™ä¸€ç‚¹ï¼›
189 | è®­ç»ƒæ•ˆæœæƒ¨ä¸å¿ç¹ï¼Œä½ å¯ä»¥ä»logçœ‹çœ‹é¢„æµ‹ç›¸å¯¹è¯¯å·®ï¼Œéœ€è¦è°ƒæ•´playtimeæ¨¡å‹ï¼šæˆ‘å·²ç»å¾®è°ƒäº†å‚æ•°ï¼Œå¯ä»¥è¯•è¯•çœ‹ï¼Œä½ å¯ä»¥æå‡ºæ›´å¤šåŠæ³•è°ƒè¯•æ•ˆæœå·®çš„åŸå› å¹¶æå‡ºæ”¹å–„æ¨¡å‹çš„å»ºè®®ã€‚
190 | </instruction>
191 | 
192 | <context>
193 | æŸ¥çœ‹/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP/results/gpu_run_52068_detailed.logï¼Œä½ ä¼šå‘ç°å¤§é‡çš„ä¸åŒ¹é…ï¼Œè°ƒæ•´ç´¢å¼•èŒƒå›´ã€ä¸åŒ¹é…ï¼Œè°ƒæ•´æ‰¹é‡è®­ç»ƒã€WARNINGå’Œ"/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP/libs/modes/global_mode_optimized.py:583: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
194 | with autocast():"
195 | </context>
196 | 
197 | ---
198 | ## 20250802ï¼ˆnightï¼‰
199 | 
200 | ### é‡æ–°æ„å»ºé¡¹ç›®ä¸ºRealdataEXP
201 | 
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\experiment_yanc.yaml

- Extension: .yaml
- Language: yaml
- Size: 3440 bytes
- Created: 2025-08-18 03:13:05
- Modified: 2025-08-18 03:13:05

### Code

```yaml
  1 | # base_dir: "/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP"
  2 | base_dir: "E:/MyDocument/Codes_notnut/_notpad/IEDA/RealdataEXP"
  3 | mode: 'global_optimized'  # ä½¿ç”¨ä¼˜åŒ–æ¨¡å¼
  4 | 
  5 | # --- Device Configuration ---
  6 | # Specifies the hardware backend.
  7 | # Options:
  8 | #   'auto': (Recommended) Automatically detects best available hardware in order: cuda -> ipex -> xpu -> dml -> cpu
  9 | #   'cuda': NVIDIA GPU with full AMP support.
 10 | #   'ipex': Intel GPU with full IPEX optimizations and AMP support.
 11 | #   'xpu':  Intel GPU with basic device placement only (No IPEX optimizations, no AMP).
 12 | #   'dml':  DirectML-compatible GPU (AMD, Intel, etc.) (No AMP).
 13 | #   'cpu':  CPU execution.
 14 | device: 'auto'
 15 | 
 16 | # æ•°æ®é›†é…ç½®
 17 | dataset:
 18 |   name: "KuaiRand-27K"
 19 |   path: "data/KuaiRand/27K"  # 1Kæ•°æ®åœ¨KuaiRand/1Kç›®å½•ä¸‹
 20 |   cache_path: "data/KuaiRand/cache"
 21 |   # --- æ–°å¢: DataLoaderä¼˜åŒ–å‚æ•° ---
 22 |   num_workers: 32  # ä½¿ç”¨32ä¸ªCPUæ ¸å¿ƒè¿›è¡Œå¹¶è¡Œæ•°æ®åŠ è½½
 23 |   pin_memory: true # é”å®šå†…å­˜ï¼ŒåŠ é€ŸCPUåˆ°GPUçš„æ•°æ®ä¼ è¾“
 24 | 
 25 | # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
 26 | use_amp: true
 27 | 
 28 | # ç‰¹å¾é…ç½®ï¼ˆ27Kæ•°æ®é›†ç‰¹å¾ï¼‰
 29 | feature:
 30 |   numerical:
 31 |     - "video_duration"
 32 |     - "server_width"
 33 |     - "server_height"
 34 |     - "show_cnt"
 35 |     - "play_cnt"
 36 |     - "play_user_num"
 37 |     - "complete_play_cnt"
 38 |     - "like_cnt"
 39 |     - "comment_cnt"
 40 |     - "share_cnt"
 41 |     - "collect_cnt"
 42 |     - "is_live_streamer"
 43 |     - "is_video_author"
 44 |     - "follow_user_num"
 45 |     - "fans_user_num"
 46 |     - "friend_user_num"
 47 |     - "register_days"
 48 |   categorical:
 49 |     - "user_active_degree"
 50 |     - "video_type"
 51 |     - "tag"
 52 | 
 53 | # æ ‡ç­¾é…ç½®ï¼ˆè°ƒæ•´äº†æ¨¡å‹å‚æ•°ä»¥æå‡æ•ˆæœï¼‰
 54 | labels:
 55 |   - name: "play_time"
 56 |     target: "play_time_ms"
 57 |     type: "numerical"
 58 |     loss_function: "logMAE"
 59 |     model: "MLP"
 60 |     model_params:
 61 |       hidden_layers: [256, 128, 64, 32]  # å¢åŠ æ¨¡å‹å®¹é‡
 62 |       dropout: 0.3  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
 63 |       # dropout: 0.0 # dmlä¼šå‡é€Ÿ
 64 |       embedding_dim: 32  # å¢åŠ åµŒå…¥ç»´åº¦
 65 |     learning_rate: 0.01  # ç¨å¾®å¢åŠ å­¦ä¹ ç‡
 66 |     weight_decay: 0.005
 67 |     alpha_T: 1.0
 68 |     alpha_C: 0.5
 69 |     
 70 |   - name: "click"
 71 |     target: "is_click"
 72 |     type: "binary"
 73 |     loss_function: "BCE"
 74 |     model: "MLP"
 75 |     model_params:
 76 |       hidden_layers: [128, 64, 32, 16]  # å¢åŠ æ¨¡å‹å®¹é‡
 77 |       dropout: 0.2  # é€‚åº¦dropout
 78 |       # dropout: 0.0
 79 |       embedding_dim: 16  # å¢åŠ åµŒå…¥ç»´åº¦
 80 |     learning_rate: 0.01  # ç¨å¾®å¢åŠ å­¦ä¹ ç‡
 81 |     weight_decay: 0.005
 82 |     alpha_T: 1.0
 83 |     alpha_C: 0.8
 84 | 
 85 | # é¢„è®­ç»ƒé…ç½®ï¼ˆä¼˜åŒ–ï¼‰
 86 | pretrain:
 87 |   enabled: true
 88 |   batch_size: 256  # å¢åŠ batch sizeä»¥æ›´å¥½åˆ©ç”¨GPU
 89 |   epochs: 150  
 90 |   learning_rate: 0.01
 91 |   weight_decay: 0.005
 92 |   early_stopping: 10
 93 |   # --- æ–°å¢é…ç½® ---
 94 |   # æŒ‡å®šè¦åŠ è½½çš„é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºnullåˆ™ä¸åŠ è½½
 95 |   load_checkpoint_path: null # example: "results/20250818_0110/checkpoints/pretrain_epoch_1.pt"
 96 |   # é¢„è®­ç»ƒæ•°æ®çš„éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
 97 |   val_split_ratio: 0.5
 98 |   # æ˜¯å¦åœ¨æ¯ä¸ªepochåç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿å›¾
 99 |   plot_loss_curves: true
100 | 
101 | # å…¨å±€ä»¿çœŸé…ç½®ï¼ˆä¼˜åŒ–ï¼‰
102 | global:
103 |   user_p_val: 0.2
104 |   batch_size: 128  # å¢åŠ batch size
105 |   n_candidate: 10
106 |   n_steps: 5  # å‡å°‘æ­¥æ•°ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•ä¼˜åŒ–æ•ˆæœ
107 |   validate_every: 1  # æ›´é¢‘ç¹çš„éªŒè¯
108 |   save_every: 25
109 |   learning_rate: 0.01
110 |   weight_decay: 0.005
111 | 
112 | # æ—¥å¿—é…ç½®
113 | logging:
114 |   level: "INFO"
115 |   log_dir: "results"
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\GPU_OPTIMIZATION_GUIDE.md

- Extension: .md
- Language: markdown
- Size: 5701 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```markdown
  1 | # GPUä¼˜åŒ–å®éªŒä½¿ç”¨æŒ‡å—
  2 | 
  3 | ## æ¦‚è¿°
  4 | 
  5 | æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬çš„RealdataEXPæ¡†æ¶ï¼Œè¯¥ç‰ˆæœ¬ä¸“é—¨é’ˆå¯¹GPUåˆ©ç”¨ç‡ä½ä¸‹çš„é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚
  6 | 
  7 | ## ä¸»è¦ä¼˜åŒ–å†…å®¹
  8 | 
  9 | ### 1. æ•°æ®åŠ è½½ä¼˜åŒ–
 10 | - **å¤šè¿›ç¨‹DataLoader**: ä½¿ç”¨8ä¸ªCPUæ ¸å¿ƒå¹¶è¡ŒåŠ è½½æ•°æ®
 11 | - **å†…å­˜é”å®š**: å¯ç”¨pin_memoryåŠ é€ŸCPUåˆ°GPUçš„æ•°æ®ä¼ è¾“
 12 | - **æŒä¹…åŒ–Worker**: ä¿æŒworkerè¿›ç¨‹ä»¥å‡å°‘å¯åŠ¨å¼€é”€
 13 | - **åˆ†ç‰‡æ–‡ä»¶æ”¯æŒ**: è‡ªåŠ¨æ£€æµ‹å’Œåˆå¹¶åˆ†ç‰‡æ•°æ®æ–‡ä»¶
 14 | 
 15 | ### 2. GPUè®­ç»ƒä¼˜åŒ–
 16 | - **æ··åˆç²¾åº¦è®­ç»ƒ**: ä½¿ç”¨AMPï¼ˆAutomatic Mixed Precisionï¼‰æå‡è®­ç»ƒé€Ÿåº¦
 17 | - **æ‰¹æ¬¡å¤§å°ä¼˜åŒ–**: å¢åŠ batch_sizeä»¥æ›´å¥½åˆ©ç”¨GPUå¹¶è¡Œèƒ½åŠ›
 18 | - **å¼ é‡ä¼˜åŒ–**: é¢„è½¬æ¢æ•°æ®ä¸ºtensorå‡å°‘è¿è¡Œæ—¶å¼€é”€
 19 | - **GPUå†…å­˜ç®¡ç†**: æ™ºèƒ½çš„å†…å­˜åˆ†é…å’Œé‡Šæ”¾
 20 | 
 21 | ### 3. ç›‘æ§å’Œè¯Šæ–­
 22 | - **å®æ—¶GPUç›‘æ§**: è‡ªåŠ¨è®°å½•GPUåˆ©ç”¨ç‡ã€å†…å­˜ä½¿ç”¨å’ŒåŠŸè€—
 23 | - **æ€§èƒ½è¯Šæ–­**: å†…ç½®GPUçŠ¶æ€æ£€æµ‹å’Œé€Ÿåº¦æµ‹è¯•
 24 | - **è¯¦ç»†æ—¥å¿—**: å¢å¼ºçš„æ—¥å¿—è¾“å‡ºä¾¿äºé—®é¢˜è¯Šæ–­
 25 | 
 26 | ## æ–‡ä»¶ç»“æ„
 27 | 
 28 | ```
 29 | RealdataEXP/
 30 | â”œâ”€â”€ libs/modes/global_mode_optimized.py    # ä¼˜åŒ–çš„è®­ç»ƒå¼•æ“
 31 | â”œâ”€â”€ configs/experiment_optimized.yaml      # ä¼˜åŒ–é…ç½®æ–‡ä»¶
 32 | â”œâ”€â”€ run_gpu_optimized.sh                   # ä¼˜åŒ–GPUä½œä¸šè„šæœ¬
 33 | â”œâ”€â”€ performance_analysis.py                # æ€§èƒ½åˆ†æå·¥å…·
 34 | â”œâ”€â”€ libs/utils/gpu_utils.py               # GPUè¯Šæ–­å·¥å…·
 35 | â””â”€â”€ GPU_OPTIMIZATION_GUIDE.md             # æœ¬ä½¿ç”¨æŒ‡å—
 36 | ```
 37 | 
 38 | ## ä½¿ç”¨æ–¹æ³•
 39 | 
 40 | ### 1. æäº¤ä¼˜åŒ–å®éªŒ
 41 | 
 42 | ```bash
 43 | # æäº¤GPUä¼˜åŒ–ä½œä¸š
 44 | sbatch run_gpu_optimized.sh
 45 | ```
 46 | 
 47 | ### 2. ç›‘æ§ä½œä¸šçŠ¶æ€
 48 | 
 49 | ```bash
 50 | # æŸ¥çœ‹ä½œä¸šé˜Ÿåˆ—
 51 | squeue -u $USER
 52 | 
 53 | # æŸ¥çœ‹ä½œä¸šè¯¦æƒ…
 54 | scontrol show job <JOB_ID>
 55 | 
 56 | # å®æ—¶æŸ¥çœ‹æ—¥å¿—
 57 | tail -f results/gpu_run_<JOB_ID>_detailed.log
 58 | ```
 59 | 
 60 | ### 3. åˆ†ææ€§èƒ½ç»“æœ
 61 | 
 62 | ```bash
 63 | # åˆ†æGPUåˆ©ç”¨ç‡å’Œè®­ç»ƒæ€§èƒ½
 64 | python performance_analysis.py --job-id <JOB_ID>
 65 | ```
 66 | 
 67 | ## é…ç½®å‚æ•°
 68 | 
 69 | ### ä¼˜åŒ–é…ç½®æ–‡ä»¶ (experiment_optimized.yaml)
 70 | 
 71 | ```yaml
 72 | # æ•°æ®åŠ è½½ä¼˜åŒ–
 73 | dataset:
 74 |   num_workers: 8        # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
 75 |   pin_memory: true      # å†…å­˜é”å®š
 76 | 
 77 | # æ··åˆç²¾åº¦è®­ç»ƒ
 78 | use_amp: true
 79 | 
 80 | # æ‰¹æ¬¡å¤§å°ä¼˜åŒ–
 81 | pretrain:
 82 |   batch_size: 512       # é¢„è®­ç»ƒæ‰¹æ¬¡å¤§å°
 83 | 
 84 | global:
 85 |   batch_size: 128       # ä»¿çœŸæ‰¹æ¬¡å¤§å°
 86 | ```
 87 | 
 88 | ### å…³é”®å‚æ•°è¯´æ˜
 89 | 
 90 | - **num_workers**: æ•°æ®åŠ è½½çš„CPUæ ¸å¿ƒæ•°ï¼Œæ¨è8-16
 91 | - **pin_memory**: æ˜¯å¦é”å®šå†…å­˜ï¼ŒGPUè®­ç»ƒæ—¶å»ºè®®å¯ç”¨
 92 | - **use_amp**: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¯æå‡30-50%é€Ÿåº¦
 93 | - **batch_size**: æ‰¹æ¬¡å¤§å°ï¼Œéœ€æ ¹æ®GPUå†…å­˜è°ƒæ•´
 94 | 
 95 | ## æ€§èƒ½å¯¹æ¯”
 96 | 
 97 | | æŒ‡æ ‡ | åŸç‰ˆæœ¬ | ä¼˜åŒ–ç‰ˆæœ¬ | æ”¹å–„ |
 98 | |------|--------|----------|------|
 99 | | é¢„è®­ç»ƒæ¯epochæ—¶é—´ | ~20åˆ†é’Ÿ | ~5åˆ†é’Ÿ | 75% |
100 | | GPUåˆ©ç”¨ç‡ | <10% | >80% | 8å€+ |
101 | | CPUåˆ©ç”¨ç‡ | å•æ ¸100% | å¤šæ ¸å¹³è¡¡ | æ˜¾è‘—æ”¹å–„ |
102 | | å†…å­˜æ•ˆç‡ | ä½ | é«˜ | æ˜¾è‘—æ”¹å–„ |
103 | 
104 | ## å¸¸è§é—®é¢˜è§£å†³
105 | 
106 | ### 1. GPUåˆ©ç”¨ç‡ä»ç„¶ä½ä¸‹
107 | 
108 | **å¯èƒ½åŸå› **:
109 | - num_workersè®¾ç½®è¿‡ä½
110 | - batch_sizeè¿‡å°
111 | - æ•°æ®é¢„å¤„ç†æˆä¸ºç“¶é¢ˆ
112 | 
113 | **è§£å†³æ–¹æ¡ˆ**:
114 | ```yaml
115 | dataset:
116 |   num_workers: 16       # å¢åŠ åˆ°16
117 | pretrain:
118 |   batch_size: 1024      # å¢åŠ æ‰¹æ¬¡å¤§å°
119 | ```
120 | 
121 | ### 2. GPUå†…å­˜ä¸è¶³
122 | 
123 | **ç—‡çŠ¶**: CUDA out of memoryé”™è¯¯
124 | 
125 | **è§£å†³æ–¹æ¡ˆ**:
126 | ```yaml
127 | pretrain:
128 |   batch_size: 256       # å‡å°‘æ‰¹æ¬¡å¤§å°
129 | global:
130 |   batch_size: 64
131 | ```
132 | 
133 | ### 3. æ•°æ®åŠ è½½é”™è¯¯
134 | 
135 | **ç—‡çŠ¶**: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶æˆ–åˆ†ç‰‡
136 | 
137 | **è§£å†³æ–¹æ¡ˆ**:
138 | - æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„
139 | - ç¡®è®¤åˆ†ç‰‡æ–‡ä»¶å‘½åæ ¼å¼: `filename_part1.csv`, `filename_part2.csv`
140 | - éªŒè¯æ•°æ®é›†é…ç½®ï¼š`KuaiRand-27K` æˆ– `KuaiRand-Pure`
141 | 
142 | ### 4. æ··åˆç²¾åº¦è®­ç»ƒé”™è¯¯
143 | 
144 | **ç—‡çŠ¶**: autocastç›¸å…³è­¦å‘Šæˆ–é”™è¯¯
145 | 
146 | **è§£å†³æ–¹æ¡ˆ**:
147 | ```yaml
148 | use_amp: false          # ä¸´æ—¶ç¦ç”¨æ··åˆç²¾åº¦
149 | ```
150 | 
151 | ## ç›‘æ§å’Œåˆ†æ
152 | 
153 | ### 1. å®æ—¶GPUç›‘æ§
154 | 
155 | ä½œä¸šè¿è¡Œæ—¶ä¼šè‡ªåŠ¨è®°å½•ï¼š
156 | - GPUåˆ©ç”¨ç‡
157 | - å†…å­˜ä½¿ç”¨ç‡
158 | - åŠŸè€—
159 | - æ¸©åº¦
160 | 
161 | ç›‘æ§æ–‡ä»¶ï¼š`results/gpu_utilization_<JOB_ID>.log`
162 | 
163 | ### 2. æ€§èƒ½åˆ†ææŠ¥å‘Š
164 | 
165 | ```bash
166 | python performance_analysis.py --job-id 52098
167 | ```
168 | 
169 | è¾“å‡ºåŒ…æ‹¬ï¼š
170 | - GPUåˆ©ç”¨ç‡ç»Ÿè®¡
171 | - è®­ç»ƒæ—¶é—´åˆ†æ
172 | - é”™è¯¯æ—¥å¿—æ±‡æ€»
173 | - æ€§èƒ½å›¾è¡¨
174 | 
175 | ## æœ€ä½³å®è·µ
176 | 
177 | ### 1. æ•°æ®å‡†å¤‡
178 | - ç¡®ä¿æ•°æ®æ–‡ä»¶å®Œæ•´
179 | - å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨åˆ†ç‰‡å­˜å‚¨
180 | - å®šæœŸæ¸…ç†ç¼“å­˜ç›®å½•
181 | 
182 | ### 2. èµ„æºé…ç½®
183 | - GPUå¯†é›†å‹ï¼šå¢åŠ batch_size
184 | - CPUå¯†é›†å‹ï¼šå¢åŠ num_workers
185 | - å†…å­˜å—é™ï¼šå‡å°‘æ‰¹æ¬¡å¤§å°
186 | 
187 | ### 3. å®éªŒè®¾è®¡
188 | - ä»å°è§„æ¨¡å¼€å§‹æµ‹è¯•
189 | - é€æ­¥å¢åŠ å‚æ•°è§„æ¨¡
190 | - ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ
191 | 
192 | ### 4. è°ƒè¯•ç­–ç•¥
193 | - å¯ç”¨è¯¦ç»†æ—¥å¿—
194 | - ä½¿ç”¨GPUè¯Šæ–­å·¥å…·
195 | - åˆ†ææ€§èƒ½ç“¶é¢ˆ
196 | 
197 | ## æŠ€æœ¯ç»†èŠ‚
198 | 
199 | ### æ•°æ®åŠ è½½ä¼˜åŒ–æœºåˆ¶
200 | 
201 | 1. **TabularDataset**: ä¸“é—¨ä¸ºè¡¨æ ¼æ•°æ®è®¾è®¡çš„Datasetç±»
202 | 2. **å¤šè¿›ç¨‹åŠ è½½**: ä½¿ç”¨å¤šä¸ªCPUæ ¸å¿ƒå¹¶è¡Œè¯»å–æ•°æ®
203 | 3. **å†…å­˜é”å®š**: å°†æ•°æ®é”å®šåœ¨å†…å­˜ä¸­ï¼Œé¿å…åˆ†é¡µ
204 | 4. **é¢„è½¬æ¢**: æå‰å°†æ•°æ®è½¬æ¢ä¸ºtensoræ ¼å¼
205 | 
206 | ### GPUè®­ç»ƒä¼˜åŒ–
207 | 
208 | 1. **æ··åˆç²¾åº¦**: è‡ªåŠ¨åœ¨float16å’Œfloat32ä¹‹é—´åˆ‡æ¢
209 | 2. **è®¡ç®—å›¾ä¼˜åŒ–**: å‡å°‘ä¸å¿…è¦çš„åŒæ­¥æ“ä½œ
210 | 3. **å†…å­˜ç®¡ç†**: æ™ºèƒ½çš„ç¼“å­˜å’Œé‡Šæ”¾ç­–ç•¥
211 | 4. **æ‰¹å¤„ç†**: ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†é€»è¾‘
212 | 
213 | ### å…¼å®¹æ€§å¤„ç†
214 | 
215 | - è‡ªåŠ¨æ£€æµ‹PyTorchç‰ˆæœ¬
216 | - å…¼å®¹æ–°æ—§autocast API
217 | - ä¼˜é›…çš„é”™è¯¯å¤„ç†å’Œé™çº§
218 | 
219 | ## æ›´æ–°æ—¥å¿—
220 | 
221 | ### v2.0 (2025-08-12)
222 | - å®ç°å¤šè¿›ç¨‹æ•°æ®åŠ è½½
223 | - æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
224 | - å¢å¼ºGPUç›‘æ§å’Œè¯Šæ–­
225 | - ä¿®å¤FutureWarningé—®é¢˜
226 | - æ”¯æŒ27Kæ•°æ®é›†çš„åˆ†ç‰‡æ–‡ä»¶
227 | 
228 | ### v1.0 (2025-08-03)
229 | - åŸºç¡€Globalæ¨¡å¼å®ç°
230 | - ç®€å•çš„æ•°æ®åŠ è½½å’Œè®­ç»ƒ
231 | 
232 | ---
233 | 
234 | *æœ€åæ›´æ–°ï¼š2025å¹´8æœˆ12æ—¥*
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\main.py

- Extension: .py
- Language: python
- Size: 3601 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-17 11:40:57

### Code

```python
  1 | #!/usr/bin/env python3
  2 | """
  3 | RealdataEXP å®éªŒæ¡†æ¶ä¸»å…¥å£ç¨‹åº
  4 | æ”¯æŒå¤šç§å®éªŒæ¨¡å¼ï¼šglobal, weighting, splittingç­‰
  5 | """
  6 | 
  7 | import os
  8 | import sys
  9 | import yaml
 10 | import argparse
 11 | import logging
 12 | from datetime import datetime
 13 | 
 14 | # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
 15 | project_root = os.path.dirname(__file__)
 16 | sys.path.insert(0, project_root)
 17 | 
 18 | from libs.utils import setup_logger, create_experiment_dir
 19 | from libs.modes import GlobalMode
 20 | from libs.modes.global_mode_optimized import GlobalModeOptimized
 21 | 
 22 | def load_config(config_path: str) -> dict:
 23 |     """åŠ è½½é…ç½®æ–‡ä»¶"""
 24 |     try:
 25 |         with open(config_path, 'r', encoding='utf-8') as f:
 26 |             config = yaml.safe_load(f)
 27 |         return config
 28 |     except Exception as e:
 29 |         raise RuntimeError(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
 30 | 
 31 | def main():
 32 |     """ä¸»å‡½æ•°"""
 33 |     parser = argparse.ArgumentParser(description='RealdataEXP å®éªŒæ¡†æ¶')
 34 |     parser.add_argument('--config', '-c', type=str, 
 35 |                       default='configs/experiment_optimized.yaml',
 36 |                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
 37 |     parser.add_argument('--mode', '-m', type=str,
 38 |                       help='å®éªŒæ¨¡å¼ (è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„modeè®¾ç½®)')
 39 |     
 40 |     args = parser.parse_args()
 41 |     
 42 |     # åŠ è½½é…ç½®
 43 |     config_path = os.path.join(os.path.dirname(__file__), args.config)
 44 |     config = load_config(config_path)
 45 |     
 46 |     # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
 47 |     if args.mode:
 48 |         config['mode'] = args.mode
 49 |     
 50 |     # ä»é…ç½®æ–‡ä»¶ä¸­è·å–è®¾å¤‡é€‰æ‹©
 51 |     device_choice = config.get('device', 'auto')
 52 | 
 53 |     # åˆ›å»ºå®éªŒç›®å½•
 54 |     base_dir = config.get('base_dir', os.path.dirname(__file__))
 55 |     exp_dir = create_experiment_dir(base_dir)
 56 |     
 57 |     # è®¾ç½®æ—¥å¿—
 58 |     log_file = os.path.join(exp_dir, 'run.log')
 59 |     logger = setup_logger(log_file, config.get('logging', {}).get('level', 'INFO'))
 60 |     
 61 |     logger.info("=" * 60)
 62 |     logger.info("RealdataEXP å®éªŒæ¡†æ¶å¯åŠ¨")
 63 |     logger.info("=" * 60)
 64 |     logger.info(f"å®éªŒæ¨¡å¼: {config['mode']}")
 65 |     logger.info(f"è®¾å¤‡é€‰æ‹© (æ¥è‡ªé…ç½®): {device_choice}")
 66 |     logger.info(f"å®éªŒç›®å½•: {exp_dir}")
 67 |     logger.info(f"é…ç½®æ–‡ä»¶: {config_path}")
 68 |     
 69 |     try:
 70 |         # æ ¹æ®æ¨¡å¼è¿è¡Œç›¸åº”çš„å®éªŒ
 71 |         mode = config['mode'].lower()
 72 |         
 73 |         if mode == 'global':
 74 |             logger.info("[æ¨¡å¼é€‰æ‹©] è¿è¡ŒGlobalæ¨¡å¼å®éªŒ")
 75 |             experiment = GlobalMode(config, exp_dir, device_choice=device_choice)
 76 |             experiment.run()
 77 |             
 78 |         elif mode == 'global_optimized':
 79 |             logger.info("[æ¨¡å¼é€‰æ‹©] è¿è¡ŒGlobalæ¨¡å¼ä¼˜åŒ–å®éªŒ")
 80 |             experiment = GlobalModeOptimized(config, exp_dir, device_choice=device_choice)
 81 |             experiment.run()
 82 |             
 83 |         elif mode == 'weighting':
 84 |             logger.error("[æ¨¡å¼é€‰æ‹©] Weightingæ¨¡å¼å°šæœªå®ç°")
 85 |             raise NotImplementedError("Weightingæ¨¡å¼å°šæœªå®ç°")
 86 |             
 87 |         elif mode == 'splitting':
 88 |             logger.error("[æ¨¡å¼é€‰æ‹©] Splittingæ¨¡å¼å°šæœªå®ç°") 
 89 |             raise NotImplementedError("Splittingæ¨¡å¼å°šæœªå®ç°")
 90 |             
 91 |         else:
 92 |             raise ValueError(f"ä¸æ”¯æŒçš„å®éªŒæ¨¡å¼: {mode}")
 93 |             
 94 |         logger.info("=" * 60)
 95 |         logger.info("å®éªŒæˆåŠŸå®Œæˆ!")
 96 |         logger.info("=" * 60)
 97 |         
 98 |     except Exception as e:
 99 |         logger.error(f"å®éªŒæ‰§è¡Œå¤±è´¥: {e}")
100 |         logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
101 |         sys.exit(1)
102 | 
103 | if __name__ == '__main__':
104 |     main()
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\performance_analysis.py

- Extension: .py
- Language: python
- Size: 9902 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
  1 | #!/usr/bin/env python3
  2 | """
  3 | æ€§èƒ½åˆ†æå·¥å…·
  4 | ç”¨äºåˆ†æGPUåˆ©ç”¨ç‡æ—¥å¿—å’Œå®éªŒæ€§èƒ½
  5 | """
  6 | 
  7 | import pandas as pd
  8 | import numpy as np
  9 | import matplotlib.pyplot as plt
 10 | import argparse
 11 | import os
 12 | import re
 13 | from datetime import datetime
 14 | 
 15 | def parse_gpu_log(log_file):
 16 |     """è§£æGPUåˆ©ç”¨ç‡æ—¥å¿—"""
 17 |     try:
 18 |         df = pd.read_csv(log_file)
 19 |         # æ¸…ç†åˆ—å
 20 |         df.columns = df.columns.str.strip()
 21 |         
 22 |         # è½¬æ¢æ•°æ®ç±»å‹
 23 |         df['utilization.gpu [%]'] = pd.to_numeric(df['utilization.gpu [%]'], errors='coerce')
 24 |         df['utilization.memory [%]'] = pd.to_numeric(df['utilization.memory [%]'], errors='coerce')
 25 |         df['memory.used [MiB]'] = pd.to_numeric(df['memory.used [MiB]'], errors='coerce')
 26 |         df['memory.total [MiB]'] = pd.to_numeric(df['memory.total [MiB]'], errors='coerce')
 27 |         df['power.draw [W]'] = pd.to_numeric(df['power.draw [W]'], errors='coerce')
 28 |         df['temperature.gpu [C]'] = pd.to_numeric(df['temperature.gpu [C]'], errors='coerce')
 29 |         
 30 |         return df
 31 |     except Exception as e:
 32 |         print(f"è§£æGPUæ—¥å¿—å¤±è´¥: {e}")
 33 |         return None
 34 | 
 35 | def parse_training_log(log_file):
 36 |     """è§£æè®­ç»ƒæ—¥å¿—ï¼Œæå–å…³é”®æ—¶é—´ä¿¡æ¯"""
 37 |     training_info = {
 38 |         'start_time': None,
 39 |         'end_time': None,
 40 |         'epochs': [],
 41 |         'steps': [],
 42 |         'errors': []
 43 |     }
 44 |     
 45 |     try:
 46 |         with open(log_file, 'r', encoding='utf-8') as f:
 47 |             for line in f:
 48 |                 # æå–æ—¶é—´æˆ³
 49 |                 timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
 50 |                 if timestamp_match:
 51 |                     timestamp = timestamp_match.group(1)
 52 |                     
 53 |                     # è®°å½•å¼€å§‹æ—¶é—´
 54 |                     if training_info['start_time'] is None and 'RealdataEXP å®éªŒæ¡†æ¶å¯åŠ¨' in line:
 55 |                         training_info['start_time'] = timestamp
 56 |                     
 57 |                     # è®°å½•epochä¿¡æ¯
 58 |                     if 'Epoch' in line and 'å¹³å‡æŸå¤±' in line:
 59 |                         epoch_match = re.search(r'Epoch (\d+)', line)
 60 |                         if epoch_match:
 61 |                             training_info['epochs'].append({
 62 |                                 'epoch': int(epoch_match.group(1)),
 63 |                                 'timestamp': timestamp,
 64 |                                 'line': line.strip()
 65 |                             })
 66 |                     
 67 |                     # è®°å½•stepä¿¡æ¯
 68 |                     if 'Step' in line and 'å¤„ç†ç”¨æˆ·' in line:
 69 |                         step_match = re.search(r'Step (\d+)', line)
 70 |                         if step_match:
 71 |                             training_info['steps'].append({
 72 |                                 'step': int(step_match.group(1)),
 73 |                                 'timestamp': timestamp,
 74 |                                 'line': line.strip()
 75 |                             })
 76 |                     
 77 |                     # è®°å½•é”™è¯¯
 78 |                     if 'ERROR' in line or 'Error' in line:
 79 |                         training_info['errors'].append({
 80 |                             'timestamp': timestamp,
 81 |                             'line': line.strip()
 82 |                         })
 83 |                     
 84 |                     # è®°å½•ç»“æŸæ—¶é—´
 85 |                     if 'å®éªŒå®Œæˆ' in line or 'å®éªŒæˆåŠŸå®Œæˆ' in line:
 86 |                         training_info['end_time'] = timestamp
 87 |     
 88 |     except Exception as e:
 89 |         print(f"è§£æè®­ç»ƒæ—¥å¿—å¤±è´¥: {e}")
 90 |     
 91 |     return training_info
 92 | 
 93 | def analyze_performance(gpu_log_file, training_log_file):
 94 |     """ç»¼åˆæ€§èƒ½åˆ†æ"""
 95 |     print("=" * 60)
 96 |     print("æ€§èƒ½åˆ†ææŠ¥å‘Š")
 97 |     print("=" * 60)
 98 |     
 99 |     # è§£æGPUæ—¥å¿—
100 |     gpu_df = parse_gpu_log(gpu_log_file)
101 |     if gpu_df is not None:
102 |         print("\n=== GPUåˆ©ç”¨ç‡åˆ†æ ===")
103 |         print(f"è®°å½•æ•°é‡: {len(gpu_df)}")
104 |         
105 |         gpu_util = gpu_df['utilization.gpu [%]'].dropna()
106 |         mem_util = gpu_df['utilization.memory [%]'].dropna()
107 |         
108 |         if len(gpu_util) > 0:
109 |             print(f"GPUåˆ©ç”¨ç‡ç»Ÿè®¡:")
110 |             print(f"  - å¹³å‡å€¼: {gpu_util.mean():.1f}%")
111 |             print(f"  - æœ€å¤§å€¼: {gpu_util.max():.1f}%")
112 |             print(f"  - æœ€å°å€¼: {gpu_util.min():.1f}%")
113 |             print(f"  - æ ‡å‡†å·®: {gpu_util.std():.1f}%")
114 |             
115 |             # åˆ©ç”¨ç‡åˆ†å¸ƒ
116 |             high_util = (gpu_util > 80).sum()
117 |             medium_util = ((gpu_util > 40) & (gpu_util <= 80)).sum()
118 |             low_util = (gpu_util <= 40).sum()
119 |             
120 |             print(f"GPUåˆ©ç”¨ç‡åˆ†å¸ƒ:")
121 |             print(f"  - é«˜åˆ©ç”¨ç‡(>80%): {high_util} æ¬¡ ({high_util/len(gpu_util)*100:.1f}%)")
122 |             print(f"  - ä¸­ç­‰åˆ©ç”¨ç‡(40-80%): {medium_util} æ¬¡ ({medium_util/len(gpu_util)*100:.1f}%)")
123 |             print(f"  - ä½åˆ©ç”¨ç‡(<=40%): {low_util} æ¬¡ ({low_util/len(gpu_util)*100:.1f}%)")
124 |         
125 |         if len(mem_util) > 0:
126 |             print(f"\nGPUå†…å­˜åˆ©ç”¨ç‡:")
127 |             print(f"  - å¹³å‡å€¼: {mem_util.mean():.1f}%")
128 |             print(f"  - æœ€å¤§å€¼: {mem_util.max():.1f}%")
129 |         
130 |         # åŠŸè€—å’Œæ¸©åº¦
131 |         power = gpu_df['power.draw [W]'].dropna()
132 |         temp = gpu_df['temperature.gpu [C]'].dropna()
133 |         
134 |         if len(power) > 0:
135 |             print(f"\nåŠŸè€—ç»Ÿè®¡:")
136 |             print(f"  - å¹³å‡åŠŸè€—: {power.mean():.1f}W")
137 |             print(f"  - æœ€å¤§åŠŸè€—: {power.max():.1f}W")
138 |         
139 |         if len(temp) > 0:
140 |             print(f"\næ¸©åº¦ç»Ÿè®¡:")
141 |             print(f"  - å¹³å‡æ¸©åº¦: {temp.mean():.1f}Â°C")
142 |             print(f"  - æœ€é«˜æ¸©åº¦: {temp.max():.1f}Â°C")
143 |     
144 |     # è§£æè®­ç»ƒæ—¥å¿—
145 |     training_info = parse_training_log(training_log_file)
146 |     
147 |     print("\n=== è®­ç»ƒè¿›åº¦åˆ†æ ===")
148 |     if training_info['start_time'] and training_info['end_time']:
149 |         start = datetime.strptime(training_info['start_time'], '%Y-%m-%d %H:%M:%S')
150 |         end = datetime.strptime(training_info['end_time'], '%Y-%m-%d %H:%M:%S')
151 |         duration = end - start
152 |         print(f"æ€»è®­ç»ƒæ—¶é—´: {duration}")
153 |     
154 |     if training_info['epochs']:
155 |         print(f"å®Œæˆçš„Epochæ•°: {len(training_info['epochs'])}")
156 |         if len(training_info['epochs']) >= 2:
157 |             # è®¡ç®—å¹³å‡epochæ—¶é—´
158 |             first_epoch = datetime.strptime(training_info['epochs'][0]['timestamp'], '%Y-%m-%d %H:%M:%S')
159 |             last_epoch = datetime.strptime(training_info['epochs'][-1]['timestamp'], '%Y-%m-%d %H:%M:%S')
160 |             epoch_duration = (last_epoch - first_epoch) / len(training_info['epochs'])
161 |             print(f"å¹³å‡Epochæ—¶é—´: {epoch_duration}")
162 |     
163 |     if training_info['steps']:
164 |         print(f"å®Œæˆçš„Stepæ•°: {len(training_info['steps'])}")
165 |     
166 |     if training_info['errors']:
167 |         print(f"\n=== é”™è¯¯åˆ†æ ===")
168 |         print(f"é”™è¯¯æ•°é‡: {len(training_info['errors'])}")
169 |         for error in training_info['errors'][:5]:  # æ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
170 |             print(f"  - {error['timestamp']}: {error['line'][:100]}...")
171 | 
172 | def create_gpu_plot(gpu_log_file, output_dir):
173 |     """åˆ›å»ºGPUåˆ©ç”¨ç‡å›¾è¡¨"""
174 |     gpu_df = parse_gpu_log(gpu_log_file)
175 |     if gpu_df is None:
176 |         return
177 |     
178 |     try:
179 |         plt.figure(figsize=(12, 8))
180 |         
181 |         # GPUåˆ©ç”¨ç‡
182 |         plt.subplot(2, 2, 1)
183 |         gpu_util = gpu_df['utilization.gpu [%]'].dropna()
184 |         if len(gpu_util) > 0:
185 |             plt.plot(gpu_util)
186 |             plt.title('GPUåˆ©ç”¨ç‡')
187 |             plt.ylabel('åˆ©ç”¨ç‡ (%)')
188 |             plt.grid(True)
189 |         
190 |         # å†…å­˜åˆ©ç”¨ç‡
191 |         plt.subplot(2, 2, 2)
192 |         mem_util = gpu_df['utilization.memory [%]'].dropna()
193 |         if len(mem_util) > 0:
194 |             plt.plot(mem_util, color='orange')
195 |             plt.title('GPUå†…å­˜åˆ©ç”¨ç‡')
196 |             plt.ylabel('å†…å­˜åˆ©ç”¨ç‡ (%)')
197 |             plt.grid(True)
198 |         
199 |         # åŠŸè€—
200 |         plt.subplot(2, 2, 3)
201 |         power = gpu_df['power.draw [W]'].dropna()
202 |         if len(power) > 0:
203 |             plt.plot(power, color='red')
204 |             plt.title('GPUåŠŸè€—')
205 |             plt.ylabel('åŠŸè€— (W)')
206 |             plt.grid(True)
207 |         
208 |         # æ¸©åº¦
209 |         plt.subplot(2, 2, 4)
210 |         temp = gpu_df['temperature.gpu [C]'].dropna()
211 |         if len(temp) > 0:
212 |             plt.plot(temp, color='green')
213 |             plt.title('GPUæ¸©åº¦')
214 |             plt.ylabel('æ¸©åº¦ (Â°C)')
215 |             plt.grid(True)
216 |         
217 |         plt.tight_layout()
218 |         
219 |         plot_file = os.path.join(output_dir, 'gpu_performance.png')
220 |         plt.savefig(plot_file, dpi=150, bbox_inches='tight')
221 |         print(f"\nGPUæ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {plot_file}")
222 |         
223 |     except Exception as e:
224 |         print(f"åˆ›å»ºå›¾è¡¨å¤±è´¥: {e}")
225 | 
226 | def main():
227 |     parser = argparse.ArgumentParser(description='æ€§èƒ½åˆ†æå·¥å…·')
228 |     parser.add_argument('--job-id', type=str, required=True, help='SLURMä½œä¸šID')
229 |     parser.add_argument('--results-dir', type=str, default='results', help='ç»“æœç›®å½•')
230 |     
231 |     args = parser.parse_args()
232 |     
233 |     # æ„å»ºæ–‡ä»¶è·¯å¾„
234 |     gpu_log_file = os.path.join(args.results_dir, f'gpu_utilization_{args.job_id}.log')
235 |     training_log_file = os.path.join(args.results_dir, f'gpu_run_{args.job_id}_detailed.log')
236 |     
237 |     # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
238 |     if not os.path.exists(gpu_log_file):
239 |         print(f"GPUæ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {gpu_log_file}")
240 |         return
241 |     
242 |     if not os.path.exists(training_log_file):
243 |         print(f"è®­ç»ƒæ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {training_log_file}")
244 |         return
245 |     
246 |     # æ‰§è¡Œåˆ†æ
247 |     analyze_performance(gpu_log_file, training_log_file)
248 |     
249 |     # åˆ›å»ºå›¾è¡¨
250 |     create_gpu_plot(gpu_log_file, args.results_dir)
251 | 
252 | if __name__ == '__main__':
253 |     main()
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\README.md

- Extension: .md
- Language: markdown
- Size: 9001 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```markdown
  1 | # RealdataEXP å®éªŒæ¡†æ¶
  2 | 
  3 | ## é¡¹ç›®æ¦‚è¿°
  4 | 
  5 | RealdataEXP æ˜¯ä¸€ä¸ªåŸºäºçœŸå®æ•°æ®çš„æ¨èç³»ç»Ÿå®éªŒæ¡†æ¶ï¼Œç”¨äºè®¡ç®—å…¨å±€å¤„ç†æ•ˆåº”ï¼ˆGlobal Treatment Effect, GTEï¼‰ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§å®éªŒæ¨¡å¼ï¼ŒåŒ…æ‹¬globalã€weightingã€splittingç­‰ï¼Œæ—¨åœ¨ä¸ºæ¨èç³»ç»Ÿçš„å› æœæ¨æ–­ç ”ç©¶æä¾›å®Œæ•´çš„å®éªŒå¹³å°ã€‚
  6 | 
  7 | ## é¡¹ç›®ç‰¹æ€§
  8 | 
  9 | - **å¤šæ¨¡å¼å®éªŒæ”¯æŒ**ï¼šæ”¯æŒglobalã€weightingã€splittingç­‰å¤šç§å®éªŒæ¨¡å¼
 10 | - **å¤šæ ‡ç­¾é¢„æµ‹**ï¼šåŒæ—¶æ”¯æŒç‚¹å‡»ç‡å’Œæ’­æ”¾æ—¶é•¿ç­‰å¤šä¸ªæ ‡ç­¾çš„é¢„æµ‹
 11 | - **çµæ´»çš„ç‰¹å¾å¤„ç†**ï¼šè‡ªåŠ¨å¤„ç†æ•°å€¼ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾ï¼Œæ”¯æŒone-hotç¼–ç 
 12 | - **ç¼“å­˜æœºåˆ¶**ï¼šä¼˜åŒ–æ•°æ®åŠ è½½æ€§èƒ½ï¼Œé¿å…é‡å¤è®¡ç®—
 13 | - **å®Œæ•´çš„å®éªŒè®°å½•**ï¼šè¯¦ç»†çš„æ—¥å¿—è®°å½•å’Œç»“æœä¿å­˜
 14 | - **å¯æ‰©å±•æ¶æ„**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•æ–°çš„å®éªŒæ¨¡å¼
 15 | 
 16 | ## ç³»ç»Ÿæ¶æ„
 17 | 
 18 | ```
 19 | RealdataEXP/
 20 | â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶
 21 | â”‚   â””â”€â”€ experiment.yaml        # å®éªŒé…ç½®
 22 | â”œâ”€â”€ data/                      # æ•°æ®ç›®å½•
 23 | â”‚   â””â”€â”€ KuaiRand/             # KuaiRandæ•°æ®é›†
 24 | â”‚       â”œâ”€â”€ Pure/             # Pureç‰ˆæœ¬æ•°æ®
 25 | â”‚       â”œâ”€â”€ 1K/               # 1Kç‰ˆæœ¬æ•°æ®
 26 | â”‚       â”œâ”€â”€ 27K/              # 27Kç‰ˆæœ¬æ•°æ®
 27 | â”‚       â””â”€â”€ cache/            # ç¼“å­˜ç›®å½•
 28 | â”œâ”€â”€ libs/                     # æ ¸å¿ƒä»£ç åº“
 29 | â”‚   â”œâ”€â”€ data/                 # æ•°æ®å¤„ç†æ¨¡å—
 30 | â”‚   â”‚   â”œâ”€â”€ data_loader.py    # æ•°æ®åŠ è½½å™¨
 31 | â”‚   â”‚   â”œâ”€â”€ feature_processor.py # ç‰¹å¾å¤„ç†å™¨
 32 | â”‚   â”‚   â””â”€â”€ cache_manager.py  # ç¼“å­˜ç®¡ç†å™¨
 33 | â”‚   â”œâ”€â”€ models/               # æ¨¡å‹æ¨¡å—
 34 | â”‚   â”‚   â”œâ”€â”€ mlp_model.py      # MLPæ¨¡å‹
 35 | â”‚   â”‚   â”œâ”€â”€ multi_label_model.py # å¤šæ ‡ç­¾æ¨¡å‹
 36 | â”‚   â”‚   â””â”€â”€ loss_functions.py # æŸå¤±å‡½æ•°
 37 | â”‚   â”œâ”€â”€ modes/                # å®éªŒæ¨¡å¼
 38 | â”‚   â”‚   â”œâ”€â”€ global_mode.py    # Globalæ¨¡å¼
 39 | â”‚   â”‚   â”œâ”€â”€ weighting.py      # Weightingæ¨¡å¼ï¼ˆå¾…å®ç°ï¼‰
 40 | â”‚   â”‚   â””â”€â”€ splitting.py      # Splittingæ¨¡å¼ï¼ˆå¾…å®ç°ï¼‰
 41 | â”‚   â””â”€â”€ utils/                # å·¥å…·æ¨¡å—
 42 | â”‚       â”œâ”€â”€ logger.py         # æ—¥å¿—å·¥å…·
 43 | â”‚       â”œâ”€â”€ metrics.py        # æŒ‡æ ‡è·Ÿè¸ª
 44 | â”‚       â””â”€â”€ experiment_utils.py # å®éªŒå·¥å…·
 45 | â”œâ”€â”€ results/                  # å®éªŒç»“æœ
 46 | â”‚   â””â”€â”€ [timestamp]/          # æŒ‰æ—¶é—´æˆ³ç»„ç»‡çš„å®éªŒç»“æœ
 47 | â”‚       â”œâ”€â”€ run.log          # è¿è¡Œæ—¥å¿—
 48 | â”‚       â”œâ”€â”€ result.json      # å®éªŒç»“æœ
 49 | â”‚       â””â”€â”€ checkpoints/     # æ¨¡å‹æ£€æŸ¥ç‚¹
 50 | â””â”€â”€ main.py                  # ä¸»å…¥å£ç¨‹åº
 51 | ```
 52 | 
 53 | ![RealdataEXP/å®éªŒæ¡†æ¶v2.pdf](RealdataEXP/å®éªŒæ¡†æ¶v2.pdf)
 54 | 
 55 | ## æ•°æ®é›†
 56 | 
 57 | é¡¹ç›®ä½¿ç”¨KuaiRandæ•°æ®é›†ï¼ŒåŒ…å«ï¼š
 58 | 
 59 | - **ç”¨æˆ·è¡Œä¸ºæ—¥å¿—**ï¼šè®°å½•ç”¨æˆ·ä¸è§†é¢‘çš„äº¤äº’è¡Œä¸º
 60 |   - æ€»æ ·æœ¬æ•°ï¼š2,622,668
 61 |   - ç‚¹å‡»ç‡ï¼š33.14%
 62 |   - å¹³å‡æ’­æ”¾æ—¶é•¿ï¼š15,676.54ms
 63 | 
 64 | - **ç”¨æˆ·ç‰¹å¾**ï¼šç”¨æˆ·çš„é™æ€ç”»åƒæ•°æ®
 65 |   - ç”¨æˆ·æ•°ï¼š27,285
 66 |   - è®­ç»ƒç”¨æˆ·ï¼š21,828
 67 |   - éªŒè¯ç”¨æˆ·ï¼š5,457
 68 | 
 69 | - **è§†é¢‘ç‰¹å¾**ï¼šè§†é¢‘çš„åŸºç¡€ä¿¡æ¯å’Œç»Ÿè®¡ç‰¹å¾
 70 |   - è§†é¢‘æ•°ï¼š7,583
 71 |   - ç‰¹å¾ç»´åº¦ï¼š157ï¼ˆæ•°å€¼ç‰¹å¾ï¼š34ï¼Œåˆ†ç±»ç‰¹å¾ï¼š123ï¼‰
 72 | 
 73 | ## æ ¸å¿ƒæ¨¡å—
 74 | 
 75 | ### 1. æ•°æ®åŠ è½½ä¸å¤„ç†æ¨¡å—
 76 | 
 77 | - **æ•°æ®æµ**ï¼šä»æ•°æ®é›†æå–ç”¨æˆ·IDï¼ŒæŸ¥æ‰¾ç”¨æˆ·äº¤äº’çš„è§†é¢‘åˆ—è¡¨
 78 | - **ç¼“å­˜æœºåˆ¶**ï¼šé¿å…é‡å¤è®¡ç®—ï¼Œæé«˜å¤„ç†æ•ˆç‡
 79 | - **ç‰¹å¾å¤„ç†**ï¼š
 80 |   - æ•°å€¼ç‰¹å¾ï¼šæ ‡å‡†åŒ–å¤„ç†
 81 |   - åˆ†ç±»ç‰¹å¾ï¼šone-hotç¼–ç ï¼ˆ`user_active_degree`ã€`video_type`ã€`tag`ï¼‰
 82 |   - ç¼ºå¤±å€¼å¤„ç†ï¼šæ•°å€¼ç‰¹å¾ç”¨0å¡«å……ï¼Œåˆ†ç±»ç‰¹å¾ä½œä¸ºæ–°ç±»åˆ«
 83 | 
 84 | ### 2. å¤šæ ‡ç­¾é¢„æµ‹æ¨¡å‹
 85 | 
 86 | - **ç‹¬ç«‹æ¨¡å‹æ¶æ„**ï¼šæ¯ä¸ªæ ‡ç­¾ä½¿ç”¨ç‹¬ç«‹çš„MLPæ¨¡å‹
 87 | - **æ”¯æŒçš„æ ‡ç­¾**ï¼š
 88 |   - `play_time`ï¼šæ’­æ”¾æ—¶é•¿é¢„æµ‹ï¼ˆä½¿ç”¨logMAEæŸå¤±å‡½æ•°ï¼‰
 89 |   - `click`ï¼šç‚¹å‡»é¢„æµ‹ï¼ˆä½¿ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼‰
 90 | - **æ¨¡å‹å‚æ•°**ï¼š
 91 |   - play_timeæ¨¡å‹ï¼š30,593å‚æ•°
 92 |   - clickæ¨¡å‹ï¼š12,737å‚æ•°
 93 | 
 94 | ### 3. Globalæ¨¡å¼å®éªŒ
 95 | 
 96 | Globalæ¨¡å¼æ˜¯æ¡†æ¶çš„æ ¸å¿ƒï¼Œå®ç°çœŸå®GTEçš„è®¡ç®—ï¼š
 97 | 
 98 | - **å¯¹ç§°ä»¿çœŸ**ï¼šTreatmentç»„å’ŒControlç»„ç‹¬ç«‹è¿è¡Œ
 99 | - **å®éªŒæµç¨‹**ï¼š
100 |   1. ç”¨æˆ·æ‰¹æ¬¡æŠ½æ ·
101 |   2. å€™é€‰è§†é¢‘ç”Ÿæˆ
102 |   3. æ¨¡å‹é¢„æµ‹ä¸åŠ æƒæ’åº
103 |   4. é€‰å‡ºèƒœå‡ºè§†é¢‘
104 |   5. è·å–çœŸå®åé¦ˆä¸æ¨¡å‹è®­ç»ƒ
105 |   6. æ›´æ–°çŠ¶æ€æ ‡è®°
106 | - **æ ‡è®°æœºåˆ¶**ï¼š
107 |   - `mask`ï¼šéªŒè¯é›†ç”¨æˆ·çš„è§†é¢‘æ ‡è®°
108 |   - `used`ï¼šå·²æ¨èè§†é¢‘çš„æ ‡è®°ï¼ˆä¸¤ç»„ç‹¬ç«‹ç»´æŠ¤ï¼‰
109 | 
110 | ## é…ç½®æ–‡ä»¶
111 | 
112 | `configs/experiment.yaml` åŒ…å«å®Œæ•´çš„å®éªŒé…ç½®ï¼š
113 | 
114 | ```yaml
115 | # å®éªŒæ¨¡å¼
116 | mode: 'global'
117 | 
118 | # æ•°æ®é›†é…ç½®
119 | dataset:
120 |   name: "KuaiRand-Pure"
121 |   path: "data/KuaiRand/Pure"
122 |   cache_path: "data/KuaiRand/cache"
123 | 
124 | # ç‰¹å¾é…ç½®
125 | feature:
126 |   numerical: [æ•°å€¼ç‰¹å¾åˆ—è¡¨]
127 |   categorical: [åˆ†ç±»ç‰¹å¾åˆ—è¡¨]
128 | 
129 | # å¤šæ ‡ç­¾é…ç½®
130 | labels:
131 |   - name: "play_time"
132 |     target: "play_time_ms"
133 |     type: "numerical"
134 |     loss_function: "logMAE"
135 |     # ... æ¨¡å‹å‚æ•°
136 |   - name: "click"
137 |     target: "is_click"
138 |     type: "binary"
139 |     loss_function: "BCE"
140 |     # ... æ¨¡å‹å‚æ•°
141 | 
142 | # Globalæ¨¡å¼é…ç½®
143 | global:
144 |   user_p_val: 0.2      # éªŒè¯é›†æ¯”ä¾‹
145 |   batch_size: 64       # æ‰¹æ¬¡å¤§å°
146 |   n_candidate: 10      # å€™é€‰è§†é¢‘æ•°
147 |   n_steps: 200         # ä»¿çœŸæ­¥æ•°
148 |   validate_every: 25   # éªŒè¯é¢‘ç‡
149 | ```
150 | 
151 | ## å®‰è£…å’Œä½¿ç”¨
152 | 
153 | ### ç¯å¢ƒè¦æ±‚
154 | 
155 | - Python 3.7+
156 | - PyTorch 2.0+
157 | - pandas 2.0+
158 | - numpy 2.0+
159 | - scikit-learn 1.6+
160 | - PyYAML
161 | 
162 | ### è¿è¡Œå®éªŒï¼ˆCPUï¼‰
163 | 
164 | ```bash
165 | # è¿è¡ŒGlobalæ¨¡å¼å®éªŒ
166 | python main.py --mode global
167 | 
168 | # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
169 | python main.py --config configs/my_experiment.yaml
170 | 
171 | # æŒ‡å®šå®éªŒæ¨¡å¼ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰
172 | python main.py --mode global --config configs/experiment.yaml
173 | ```
174 | 
175 | ### è¿è¡Œå®éªŒï¼ˆGPUï¼‰
176 | ## GPUé›†ç¾¤ä½¿ç”¨æŒ‡å—
177 | 
178 | æœ¬æ¡†æ¶æ”¯æŒåœ¨HKUST HPC4é›†ç¾¤ä¸Šä½¿ç”¨SLURMè¿›è¡ŒGPUåŠ é€Ÿè®­ç»ƒã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„GPUä½¿ç”¨æµç¨‹ã€‚
179 | 
180 | ### ç¯å¢ƒè¦æ±‚
181 | 
182 | - HKUST HPC4é›†ç¾¤è´¦æˆ·
183 | - é¡¹ç›®ç»„è´¦æˆ·ï¼š`sigroup`
184 | - PyTorch 2.0+ (è‡ªå¸¦CUDA runtime)
185 | - SLURMä½œä¸šè°ƒåº¦ç³»ç»Ÿ
186 | 
187 | ### 1. æäº¤GPUä½œä¸š
188 | 
189 | #### 1.1 ä½¿ç”¨é¢„é…ç½®è„šæœ¬
190 | 
191 | ```bash
192 | # æäº¤GPUä½œä¸š
193 | sbatch run_gpu.sh
194 | ```
195 | ### 2. æŸ¥çœ‹ä½œä¸šçŠ¶æ€
196 | 
197 | ```bash
198 | # æŸ¥çœ‹ç”¨æˆ·ä½œä¸šé˜Ÿåˆ—
199 | squeue -u $USER
200 | 
201 | # æŸ¥çœ‹ä½œä¸šè¯¦ç»†ä¿¡æ¯
202 | scontrol show job <ä½œä¸šID>
203 | 
204 | # å–æ¶ˆä½œä¸š
205 | scancel <ä½œä¸šID>
206 | ```
207 | 
208 | ### 3. è¿æ¥GPUèŠ‚ç‚¹
209 | 
210 | #### 3.1 è¿æ¥åˆ°å·²åˆ†é…çš„GPUèŠ‚ç‚¹
211 | 
212 | ```bash
213 | # è¿æ¥åˆ°æ­£åœ¨è¿è¡Œçš„ä½œä¸šèŠ‚ç‚¹ (ç¤ºä¾‹: ä½œä¸š52005åœ¨gpu01)
214 | srun --jobid=52098 -w gpu01 --overlap --pty bash -i
215 | ```
216 | 
217 | ### å®éªŒç»“æœ
218 | 
219 | å®éªŒç»“æœä¿å­˜åœ¨ `results/[timestamp]/` ç›®å½•ä¸‹ï¼š
220 | 
221 | - `run.log`ï¼šå®Œæ•´çš„è¿è¡Œæ—¥å¿—
222 | - `result.json`ï¼šå®éªŒç»“æœå’ŒæŒ‡æ ‡
223 | - `checkpoints/`ï¼šæ¨¡å‹æ£€æŸ¥ç‚¹å’Œç‰¹å¾å¤„ç†å™¨
224 | 
225 | ## å¼€å‘è¿›å±•
226 | 
227 | ### å·²å®ŒæˆåŠŸèƒ½
228 | 
229 | - âœ… æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ¨¡å—
230 | - âœ… å¤šæ ‡ç­¾é¢„æµ‹æ¨¡å‹æ¶æ„
231 | - âœ… Globalæ¨¡å¼æ ¸å¿ƒé€»è¾‘
232 | - âœ… ç‰¹å¾å¤„ç†å’Œç¼“å­˜æœºåˆ¶
233 | - âœ… å®éªŒæ—¥å¿—å’Œç»“æœä¿å­˜
234 | - âœ… é…ç½®ç®¡ç†ç³»ç»Ÿ
235 | 
236 | ### å¾…å®ç°åŠŸèƒ½
237 | 
238 | - â³ Weightingæ¨¡å¼å®éªŒ
239 | - â³ Splittingæ¨¡å¼å®éªŒ
240 | - â³ å®éªŒç»“æœå¯è§†åŒ–
241 | - â³ æ¨¡å‹æ€§èƒ½è¯„ä¼°æŒ‡æ ‡
242 | - â³ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
243 | 
244 | ## æŠ€æœ¯ç»†èŠ‚
245 | 
246 | ### æ•°æ®ç±»å‹å¤„ç†
247 | 
248 | æ¡†æ¶å®ç°äº†å¼ºå¥çš„æ•°æ®ç±»å‹è½¬æ¢æœºåˆ¶ï¼Œç¡®ä¿æ‰€æœ‰ç‰¹å¾æ•°æ®éƒ½èƒ½æ­£ç¡®è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼š
249 | 
250 | ```python
251 | def ensure_float_data(self, data: pd.DataFrame, columns: List[str]) -> np.ndarray:
252 |     """ç¡®ä¿æ•°æ®ä¸ºfloatç±»å‹å¹¶è½¬æ¢ä¸ºnumpyæ•°ç»„"""
253 |     # é€åˆ—å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
254 |     # å¤„ç†NaNå€¼å’Œéæ•°å€¼ç±»å‹
255 |     # è¿”å›float32æ•°ç»„
256 | ```
257 | 
258 | ### ç¼“å­˜ä¼˜åŒ–
259 | 
260 | ç”¨æˆ·-è§†é¢‘äº¤äº’åˆ—è¡¨ä½¿ç”¨pickleç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—ï¼š
261 | 
262 | ```python
263 | # é¦–æ¬¡è®¡ç®—æ—¶ä¿å­˜ç¼“å­˜
264 | cache_manager.save(user_video_lists, "user_video_lists")
265 | 
266 | # åç»­è¿è¡Œç›´æ¥åŠ è½½
267 | cached_data = cache_manager.load("user_video_lists")
268 | ```
269 | 
270 | ### æŸå¤±å‡½æ•°
271 | 
272 | æ”¯æŒå¤šç§æŸå¤±å‡½æ•°ï¼š
273 | 
274 | - **LogMAE**ï¼šç”¨äºæ’­æ”¾æ—¶é•¿ç­‰å¤§æ•°å€¼èŒƒå›´çš„è¿ç»­æ ‡ç­¾
275 | - **BCE**ï¼šç”¨äºç‚¹å‡»ç­‰äºŒå…ƒåˆ†ç±»æ ‡ç­¾
276 | - **MSE**ã€**MAE**ã€**CrossEntropy**ï¼šå…¶ä»–å¸¸ç”¨æŸå¤±å‡½æ•°
277 | 
278 | ## æ‰©å±•æŒ‡å—
279 | 
280 | ### æ·»åŠ æ–°çš„å®éªŒæ¨¡å¼
281 | 
282 | 1. åœ¨ `libs/modes/` ä¸‹åˆ›å»ºæ–°çš„æ¨¡å¼æ–‡ä»¶
283 | 2. ç»§æ‰¿åŸºç¡€å®éªŒç±»ï¼Œå®ç°æ ¸å¿ƒé€»è¾‘
284 | 3. åœ¨ `main.py` ä¸­æ·»åŠ æ¨¡å¼åˆ†æ´¾é€»è¾‘
285 | 4. æ›´æ–°é…ç½®æ–‡ä»¶æ¨¡æ¿
286 | 
287 | ### æ·»åŠ æ–°çš„æ¨¡å‹
288 | 
289 | 1. åœ¨ `libs/models/` ä¸‹åˆ›å»ºæ¨¡å‹æ–‡ä»¶
290 | 2. å®ç°PyTorchæ¨¡å‹æ¥å£
291 | 3. åœ¨å¤šæ ‡ç­¾æ¨¡å‹ç®¡ç†å™¨ä¸­æ³¨å†Œæ–°æ¨¡å‹
292 | 4. æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹å‚æ•°
293 | 
294 | ### æ·»åŠ æ–°çš„ç‰¹å¾
295 | 
296 | 1. åœ¨é…ç½®æ–‡ä»¶ä¸­å£°æ˜æ–°ç‰¹å¾
297 | 2. ç¡®ä¿æ•°æ®é›†åŒ…å«å¯¹åº”å­—æ®µ
298 | 3. æ ¹æ®ç‰¹å¾ç±»å‹é€‰æ‹©æ•°å€¼æˆ–åˆ†ç±»å¤„ç†
299 | 4. æµ‹è¯•ç‰¹å¾å¤„ç†å’Œæ¨¡å‹è®­ç»ƒæµç¨‹
300 | 
301 | ## è®¸å¯è¯
302 | 
303 | æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚è¯¦è§LICENSEæ–‡ä»¶ã€‚
304 | 
305 | ## è”ç³»æ–¹å¼
306 | 
307 | å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤Issueæˆ–è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚
308 | 
309 | ---
310 | 
311 | *æœ€åæ›´æ–°ï¼š2025å¹´8æœˆ3æ—¥*
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\requirements.txt

- Extension: .txt
- Language: plaintext
- Size: 68 bytes
- Created: 2025-08-18 03:02:53
- Modified: 2025-08-18 03:03:17

### Code

```plaintext
1 | numpy>=2.0
2 | pandas>=2.0
3 | scikit-learn>=1.6
4 | pyyaml
5 | tqdm
6 | matplotlib
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\run_gpu.sh

- Extension: .sh
- Language: bash
- Size: 3867 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```bash
  1 | #!/bin/bash
  2 | 
  3 | # GPU ä½œä¸šæäº¤è„šæœ¬ - Global Modeå®éªŒ
  4 | # ä½¿ç”¨æ–¹æ³•: sbatch run_gpu.sh
  5 | 
  6 | #SBATCH --account=sigroup     # ä½ çš„è´¦æˆ·å
  7 | #SBATCH --time=23:30:00       # è¿è¡Œæ—¶é—´é™åˆ¶ (23.5å°æ—¶) - å‡å°‘ä»¥æé«˜è°ƒåº¦ä¼˜å…ˆçº§
  8 | #SBATCH --partition=gpu-a30   # GPUåˆ†åŒº (A30 GPU)
  9 | #SBATCH --gpus-per-node=1     # æ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨1ä¸ªGPU
 10 | #SBATCH --cpus-per-task=32     # æ¯ä¸ªä»»åŠ¡ä½¿ç”¨32ä¸ªCPUæ ¸å¿ƒ
 11 | #SBATCH --mem=64G             # å†…å­˜éœ€æ±‚
 12 | #SBATCH --job-name=global_mode_gpu  # æ›´æ˜ç¡®çš„ä½œä¸šåç§°
 13 | #SBATCH --output=results/gpu_run_%j.out  # æ ‡å‡†è¾“å‡ºæ–‡ä»¶
 14 | #SBATCH --error=results/gpu_run_%j.err   # é”™è¯¯è¾“å‡ºæ–‡ä»¶
 15 | 
 16 | echo "============================================================"
 17 | echo "ä½œä¸šå¼€å§‹æ—¶é—´: $(date)"
 18 | echo "ä½œä¸šID: $SLURM_JOB_ID"
 19 | echo "èŠ‚ç‚¹åç§°: $SLURM_NODELIST"
 20 | echo "GPUæ•°é‡: $SLURM_GPUS_PER_NODE"
 21 | echo "============================================================"
 22 | 
 23 | # åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
 24 | cd /home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP
 25 | 
 26 | # åŠ è½½CUDAæ¨¡å—
 27 | echo "åŠ è½½CUDAæ¨¡å—..."
 28 | module load cuda
 29 | 
 30 | # ç¯å¢ƒæ£€æµ‹å’Œç¡®è®¤
 31 | echo "============================================================"
 32 | echo "CUDAç¯å¢ƒæ£€æµ‹"
 33 | echo "============================================================"
 34 | 
 35 | # æ£€æŸ¥GPUå¯ç”¨æ€§
 36 | echo "æ£€æŸ¥GPUçŠ¶æ€..."
 37 | if command -v nvidia-smi &> /dev/null; then
 38 |     nvidia-smi
 39 |     GPU_STATUS=$?
 40 |     if [ $GPU_STATUS -eq 0 ]; then
 41 |         echo "âœ… GPUæ£€æµ‹æˆåŠŸ"
 42 |     else
 43 |         echo "âŒ GPUæ£€æµ‹å¤±è´¥"
 44 |         exit 1
 45 |     fi
 46 | else
 47 |     echo "âŒ nvidia-smiå‘½ä»¤ä¸å¯ç”¨"
 48 |     exit 1
 49 | fi
 50 | 
 51 | # æ£€æŸ¥CUDAç‰ˆæœ¬
 52 | echo ""
 53 | echo "æ£€æŸ¥CUDAç‰ˆæœ¬..."
 54 | if command -v nvcc &> /dev/null; then
 55 |     nvcc --version
 56 |     echo "âœ… CUDAå·¥å…·åŒ…å¯ç”¨"
 57 | else
 58 |     echo "âš ï¸ CUDAç¼–è¯‘å™¨ä¸å¯ç”¨ï¼Œä½†GPUå¯èƒ½ä»ç„¶å¯ç”¨"
 59 | fi
 60 | 
 61 | # è¿è¡ŒPythonç¯å¢ƒæ£€æŸ¥
 62 | echo ""
 63 | echo "æ£€æŸ¥Pythonå’ŒPyTorchç¯å¢ƒ..."
 64 | python check_environment.py
 65 | 
 66 | # æ£€æŸ¥æ£€æµ‹ç»“æœ
 67 | PYTHON_CHECK=$?
 68 | if [ $PYTHON_CHECK -ne 0 ]; then
 69 |     echo "âŒ Pythonç¯å¢ƒæ£€æµ‹å¤±è´¥ï¼Œç»§ç»­è¿è¡Œä½†å¯èƒ½é‡åˆ°é—®é¢˜"
 70 |     echo "æ³¨æ„ï¼šåœ¨SLURMç¯å¢ƒä¸­ï¼ŒGPUåªæœ‰åœ¨ä½œä¸šåˆ†é…åæ‰å¯ç”¨"
 71 | fi
 72 | 
 73 | echo ""
 74 | echo "============================================================"
 75 | echo "ç¯å¢ƒæ£€æµ‹å®Œæˆ - æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼"
 76 | echo "============================================================"
 77 | 
 78 | # å¯¹äºæ‰¹å¤„ç†ä½œä¸šï¼Œè‡ªåŠ¨ç»§ç»­ï¼ˆä¸ç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼‰
 79 | echo "æ‰¹å¤„ç†æ¨¡å¼ï¼šè‡ªåŠ¨å¼€å§‹å®éªŒ..."
 80 | sleep 2
 81 | 
 82 | # è®¾ç½®ç¯å¢ƒå˜é‡
 83 | export CUDA_VISIBLE_DEVICES=0
 84 | export PYTHONPATH=$PYTHONPATH:/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP
 85 | 
 86 | echo ""
 87 | echo "============================================================"
 88 | echo "å¼€å§‹è¿è¡Œå®éªŒ"
 89 | echo "============================================================"
 90 | echo "ä½¿ç”¨é…ç½®æ–‡ä»¶: configs/experiment.yaml"
 91 | echo "è®¾å¤‡é…ç½®: auto (å°†è‡ªåŠ¨é€‰æ‹©GPU)"
 92 | 
 93 | # æœ€ç»ˆGPUæ£€æŸ¥ï¼ˆåœ¨SLURMåˆ†é…åï¼‰
 94 | echo "SLURMä½œä¸šåˆ†é…åçš„GPUçŠ¶æ€ï¼š"
 95 | if command -v nvidia-smi &> /dev/null; then
 96 |     nvidia-smi
 97 | else
 98 |     echo "nvidia-smiä¸å¯ç”¨ï¼Œä½†PyTorchåº”è¯¥ä»èƒ½æ£€æµ‹åˆ°GPU"
 99 | fi
100 | 
101 | echo ""
102 | echo "å¼€å§‹è¿è¡ŒGlobal Mode GPUå®éªŒ..."
103 | echo "é¢„æœŸè¿è¡Œæ—¶é—´ï¼šçº¦60-90åˆ†é’Ÿ"
104 | echo ""
105 | 
106 | # è¿è¡Œå®éªŒï¼Œå¢åŠ è¯¦ç»†è¾“å‡º
107 | python main.py --config configs/experiment.yaml --mode global 2>&1 | tee results/gpu_run_${SLURM_JOB_ID}_detailed.log
108 | 
109 | EXPERIMENT_STATUS=$?
110 | 
111 | echo ""
112 | echo "============================================================"
113 | if [ $EXPERIMENT_STATUS -eq 0 ]; then
114 |     echo "âœ… å®éªŒæˆåŠŸå®Œæˆï¼"
115 | else
116 |     echo "âŒ å®éªŒæ‰§è¡Œå‡ºé”™ï¼Œé€€å‡ºç : $EXPERIMENT_STATUS"
117 | fi
118 | echo "ä½œä¸šç»“æŸæ—¶é—´: $(date)"
119 | echo "è¯¦ç»†æ—¥å¿—ä¿å­˜åœ¨: results/gpu_run_${SLURM_JOB_ID}_detailed.log"
120 | echo "============================================================"
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\run_gpu_optimized.sh

- Extension: .sh
- Language: bash
- Size: 3470 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```bash
 1 | #!/bin/bash
 2 | 
 3 | # GPU ä½œä¸šæäº¤è„šæœ¬ - Global Modeä¼˜åŒ–å®éªŒ
 4 | # ä½¿ç”¨æ–¹æ³•: sbatch run_gpu_optimized.sh
 5 | 
 6 | #SBATCH --account=sigroup     # ä½ çš„è´¦æˆ·å
 7 | #SBATCH --time=23:30:00       # è¿è¡Œæ—¶é—´é™åˆ¶ (23.5å°æ—¶)
 8 | #SBATCH --partition=gpu-a30   # GPUåˆ†åŒº (A30 GPU)
 9 | #SBATCH --gpus-per-node=1     # æ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨1ä¸ªGPU
10 | #SBATCH --cpus-per-task=32    # æ¯ä¸ªä»»åŠ¡ä½¿ç”¨32ä¸ªCPUæ ¸å¿ƒ
11 | #SBATCH --mem=64G             # å†…å­˜éœ€æ±‚
12 | #SBATCH --job-name=global_optimized  # ä½œä¸šåç§°
13 | #SBATCH --output=results/gpu_run_%j.out  # æ ‡å‡†è¾“å‡ºæ–‡ä»¶
14 | #SBATCH --error=results/gpu_run_%j.err   # é”™è¯¯è¾“å‡ºæ–‡ä»¶
15 | 
16 | echo "============================================================"
17 | echo "ä½œä¸šå¼€å§‹æ—¶é—´: $(date)"
18 | echo "ä½œä¸šID: $SLURM_JOB_ID"
19 | echo "èŠ‚ç‚¹åç§°: $SLURM_NODELIST"
20 | echo "GPUæ•°é‡: $SLURM_GPUS_PER_NODE"
21 | echo "CPUæ ¸å¿ƒæ•°: $SLURM_CPUS_PER_TASK"
22 | echo "============================================================"
23 | 
24 | # åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
25 | cd /home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP
26 | 
27 | # åŠ è½½CUDAæ¨¡å—
28 | echo "åŠ è½½CUDAæ¨¡å—..."
29 | module load cuda
30 | 
31 | # --- GPUåˆ©ç”¨ç‡ç›‘æ§ ---
32 | echo "å¯åŠ¨GPUåˆ©ç”¨ç‡ç›‘æ§..."
33 | nvidia-smi --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.used,memory.total,power.draw,temperature.gpu --format=csv -l 10 > results/gpu_utilization_${SLURM_JOB_ID}.log &
34 | NVIDIASMI_PID=$!
35 | 
36 | echo "GPUç›‘æ§è¿›ç¨‹PID: $NVIDIASMI_PID"
37 | 
38 | # ç¯å¢ƒæ£€æµ‹
39 | echo ""
40 | echo "============================================================"
41 | echo "=== Pythonç¯å¢ƒæ£€æŸ¥ ==="
42 | echo "============================================================"
43 | 
44 | # æ£€æŸ¥Pythonå’ŒPyTorchç¯å¢ƒ
45 | python -c "
46 | import sys
47 | print('Pythonç‰ˆæœ¬:', sys.version)
48 | import torch
49 | print('PyTorchç‰ˆæœ¬:', torch.__version__)
50 | print('CUDAå¯ç”¨:', torch.cuda.is_available())
51 | if torch.cuda.is_available():
52 |     print('GPUæ•°é‡:', torch.cuda.device_count())
53 |     print('GPUåç§°:', torch.cuda.get_device_name(0))
54 |     print('GPUå†…å­˜: {:.1f}GB'.format(torch.cuda.get_device_properties(0).total_memory/1024**3))
55 | "
56 | 
57 | echo ""
58 | echo "=== å¼€å§‹è¿è¡Œä¼˜åŒ–å®éªŒ ==="
59 | echo "é…ç½®æ–‡ä»¶: configs/experiment_optimized.yaml"
60 | echo "å¼€å§‹æ—¶é—´: $(date)"
61 | 
62 | # è®¾ç½®ç¯å¢ƒå˜é‡
63 | export CUDA_VISIBLE_DEVICES=0
64 | export PYTHONPATH=$PYTHONPATH:/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP
65 | 
66 | # è¿è¡Œä¼˜åŒ–å®éªŒ
67 | python main.py --config configs/experiment_optimized.yaml --mode global_optimized 2>&1 | tee results/gpu_run_${SLURM_JOB_ID}_detailed.log
68 | 
69 | EXPERIMENT_STATUS=$?
70 | 
71 | echo ""
72 | echo "============================================================"
73 | # --- åœæ­¢GPUç›‘æ§ ---
74 | echo "åœæ­¢GPUåˆ©ç”¨ç‡ç›‘æ§ (PID: $NVIDIASMI_PID)..."
75 | kill $NVIDIASMI_PID 2>/dev/null
76 | 
77 | if [ $EXPERIMENT_STATUS -eq 0 ]; then
78 |     echo "âœ… å®éªŒæˆåŠŸå®Œæˆï¼"
79 | else
80 |     echo "âŒ å®éªŒæ‰§è¡Œå‡ºé”™ï¼Œé€€å‡ºç : $EXPERIMENT_STATUS"
81 | fi
82 | 
83 | echo "ä½œä¸šç»“æŸæ—¶é—´: $(date)"
84 | echo "è¯¦ç»†æ—¥å¿—ä¿å­˜åœ¨: results/gpu_run_${SLURM_JOB_ID}_detailed.log"
85 | echo "GPUåˆ©ç”¨ç‡æ—¥å¿—: results/gpu_utilization_${SLURM_JOB_ID}.log"
86 | 
87 | # è¾“å‡ºGPUåˆ©ç”¨ç‡ç»Ÿè®¡
88 | echo ""
89 | echo "=== GPUåˆ©ç”¨ç‡ç»Ÿè®¡ ==="
90 | if [ -f "results/gpu_utilization_${SLURM_JOB_ID}.log" ]; then
91 |     echo "GPUåˆ©ç”¨ç‡æ–‡ä»¶è¡Œæ•°: $(wc -l < results/gpu_utilization_${SLURM_JOB_ID}.log)"
92 |     echo "æœ€åå‡ æ¡GPUçŠ¶æ€:"
93 |     tail -5 results/gpu_utilization_${SLURM_JOB_ID}.log
94 | fi
95 | 
96 | echo "============================================================"
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\run_gpu_yanc.sh

- Extension: .sh
- Language: bash
- Size: 3687 bytes
- Created: 2025-08-18 02:37:58
- Modified: 2025-08-18 02:50:59

### Code

```bash
 1 | #!/bin/bash
 2 | 
 3 | # GPU ä½œä¸šæäº¤è„šæœ¬ - Global Modeä¼˜åŒ–å®éªŒ
 4 | # ä½¿ç”¨æ–¹æ³•: sbatch run_gpu_yanc.sh
 5 | 
 6 | ## æ–°æœåŠ¡å™¨SBATCHå‚æ•°è§„åˆ™
 7 | #SBATCH --partition=q_amd_share           # æŒ‡å®šåˆ†åŒº
 8 | #SBATCH --job-name=global_optimized        # æŒ‡å®šä½œä¸šåç§°
 9 | #SBATCH --nodes=1                          # ä½¿ç”¨èŠ‚ç‚¹æ•°
10 | #SBATCH --ntasks=1                         # æ€»è¿›ç¨‹æ•°
11 | #SBATCH --gpus=1                           # ä½¿ç”¨GPUæ•°
12 | #SBATCH --gpus-per-task=1                  # æ¯ä¸ªä»»åŠ¡æ‰€ä½¿ç”¨çš„GPUæ•°
13 | #SBATCH --output=results/gpu_run_%j.out    # è¾“å‡ºæ–‡ä»¶
14 | #SBATCH --error=results/gpu_run_%j.err     # é”™è¯¯æ–‡ä»¶
15 | #SBATCH --ntasks-per-node=1                # æ¯ä¸ªèŠ‚ç‚¹è¿›ç¨‹æ•°
16 | #SBATCH --cpus-per-task=32                 # æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨CPUæ ¸å¿ƒæ•°
17 | #SBATCH --exclusive                        # ç‹¬å èŠ‚ç‚¹
18 | 
19 | echo "============================================================"
20 | echo "ä½œä¸šå¼€å§‹æ—¶é—´: $(date)"
21 | echo "ä½œä¸šID: $SLURM_JOB_ID"
22 | echo "èŠ‚ç‚¹åç§°: $SLURM_NODELIST"
23 | echo "GPUæ•°é‡: $SLURM_GPUS_PER_NODE"
24 | echo "CPUæ ¸å¿ƒæ•°: $SLURM_CPUS_PER_TASK"
25 | echo "============================================================"
26 | 
27 | # åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
28 | cd /home/export/base/sc100352/sc100352/online1/RealdataEXP
29 | 
30 | # åŠ è½½CUDAæ¨¡å—
31 | echo "åŠ è½½CUDAæ¨¡å—..."
32 | module load cuda
33 | 
34 | # --- GPUåˆ©ç”¨ç‡ç›‘æ§ ---
35 | echo "å¯åŠ¨GPUåˆ©ç”¨ç‡ç›‘æ§..."
36 | nvidia-smi --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.used,memory.total,power.draw,temperature.gpu --format=csv -l 10 > results/gpu_utilization_${SLURM_JOB_ID}.log &
37 | NVIDIASMI_PID=$!
38 | 
39 | echo "GPUç›‘æ§è¿›ç¨‹PID: $NVIDIASMI_PID"
40 | 
41 | # ç¯å¢ƒæ£€æµ‹
42 | echo ""
43 | echo "============================================================"
44 | echo "=== Pythonç¯å¢ƒæ£€æŸ¥ ==="
45 | echo "============================================================"
46 | 
47 | # æ£€æŸ¥Pythonå’ŒPyTorchç¯å¢ƒ
48 | python -c "
49 | import sys
50 | print('Pythonç‰ˆæœ¬:', sys.version)
51 | import torch
52 | print('PyTorchç‰ˆæœ¬:', torch.__version__)
53 | print('CUDAå¯ç”¨:', torch.cuda.is_available())
54 | if torch.cuda.is_available():
55 |     print('GPUæ•°é‡:', torch.cuda.device_count())
56 |     print('GPUåç§°:', torch.cuda.get_device_name(0))
57 |     print('GPUå†…å­˜: {:.1f}GB'.format(torch.cuda.get_device_properties(0).total_memory/1024**3))
58 | "
59 | 
60 | echo ""
61 | echo "=== å¼€å§‹è¿è¡Œä¼˜åŒ–å®éªŒ ==="
62 | echo "é…ç½®æ–‡ä»¶: configs/experiment_yanc.yaml"
63 | echo "å¼€å§‹æ—¶é—´: $(date)"
64 | 
65 | # è®¾ç½®ç¯å¢ƒå˜é‡
66 | export CUDA_VISIBLE_DEVICES=0
67 | export PYTHONPATH=$PYTHONPATH:/home/export/base/sc100352/sc100352/online1/RealdataEXP
68 | 
69 | # è¿è¡Œä¼˜åŒ–å®éªŒ
70 | python main.py --config configs/experiment_yanc.yaml --mode global_optimized 2>&1 | tee results/gpu_run_${SLURM_JOB_ID}_detailed.log
71 | 
72 | EXPERIMENT_STATUS=$?
73 | 
74 | echo ""
75 | echo "============================================================"
76 | # --- åœæ­¢GPUç›‘æ§ ---
77 | echo "åœæ­¢GPUåˆ©ç”¨ç‡ç›‘æ§ (PID: $NVIDIASMI_PID)..."
78 | kill $NVIDIASMI_PID 2>/dev/null
79 | 
80 | if [ $EXPERIMENT_STATUS -eq 0 ]; then
81 |     echo "âœ… å®éªŒæˆåŠŸå®Œæˆï¼"
82 | else
83 |     echo "âŒ å®éªŒæ‰§è¡Œå‡ºé”™ï¼Œé€€å‡ºç : $EXPERIMENT_STATUS"
84 | fi
85 | 
86 | echo "ä½œä¸šç»“æŸæ—¶é—´: $(date)"
87 | echo "è¯¦ç»†æ—¥å¿—ä¿å­˜åœ¨: results/gpu_run_${SLURM_JOB_ID}_detailed.log"
88 | echo "GPUåˆ©ç”¨ç‡æ—¥å¿—: results/gpu_utilization_${SLURM_JOB_ID}.log"
89 | 
90 | # è¾“å‡ºGPUåˆ©ç”¨ç‡ç»Ÿè®¡
91 | echo ""
92 | echo "=== GPUåˆ©ç”¨ç‡ç»Ÿè®¡ ==="
93 | if [ -f "results/gpu_utilization_${SLURM_JOB_ID}.log" ]; then
94 |     echo "GPUåˆ©ç”¨ç‡æ–‡ä»¶è¡Œæ•°: $(wc -l < results/gpu_utilization_${SLURM_JOB_ID}.log)"
95 |     echo "æœ€åå‡ æ¡GPUçŠ¶æ€:"
96 |     tail -5 results/gpu_utilization_${SLURM_JOB_ID}.log
97 | fi
98 | 
99 | echo "============================================================"
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\run_windows.bat

- Extension: .bat
- Language: unknown
- Size: 1931 bytes
- Created: 2025-08-17 11:40:25
- Modified: 2025-08-17 12:36:42

### Code

```unknown
 1 | @echo off
 2 | REM =================================================================
 3 | REM == RealdataEXP Windows Unified Execution Script              ==
 4 | REM == (Device is now configured in the .yaml file)              ==
 5 | REM == (v2 - Patched for conda prefix activation)                ==
 6 | REM =================================================================
 7 | 
 8 | REM --- 1. Setup Environment Variables ---
 9 | set "PROJECT_DIR=E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP"
10 | set "CONDA_ENV_PATH=e:\MyDocument\Codes_notnut\_notpad\IEDA\.conda"
11 | 
12 | REM --- 2. Change to Project Directory ---
13 | echo Changing directory to %PROJECT_DIR%
14 | cd /d "%PROJECT_DIR%"
15 | if %errorlevel% neq 0 (
16 |     echo ERROR: Could not find the project directory. Please check the path.
17 |     pause
18 |     goto :eof
19 | )
20 | 
21 | REM --- 3. Activate Conda Environment ---
22 | echo.
23 | echo Activating Conda environment from: %CONDA_ENV_PATH%
24 | 
25 | REM --- MODIFIED LINE BELOW ---
26 | REM Use --prefix to explicitly tell conda this is a path, not a name.
27 | call conda activate --prefix "%CONDA_ENV_PATH%"
28 | 
29 | if %errorlevel% neq 0 (
30 |     echo ERROR: Failed to activate Conda environment.
31 |     echo Please verify the path is correct and conda is initialized.
32 |     echo You can list all environments with: conda info --envs
33 |     pause
34 |     goto :eof
35 | )
36 | echo Conda environment activated successfully.
37 | 
38 | REM --- 4. Set Python Path ---
39 | set "PYTHONPATH=%PROJECT_DIR%"
40 | echo PYTHONPATH set to: %PYTHONPATH%
41 | 
42 | REM --- 5. Run Experiment ---
43 | echo.
44 | echo [INFO] Running experiment with configuration from 'configs/experiment_optimized.yaml'.
45 | echo [INFO] Hardware device selection is specified inside the YAML file.
46 | echo.
47 | 
48 | python main.py --config configs/experiment_optimized.yaml
49 | 
50 | REM --- 6. Deactivate Environment and Exit ---
51 | echo.
52 | echo Experiment finished.
53 | echo Deactivating Conda environment.
54 | call conda deactivate
55 | 
56 | echo.
57 | echo Script complete. Press any key to exit.
58 | pause
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\configs\experiment.yaml

- Extension: .yaml
- Language: yaml
- Size: 4810 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-17 11:40:57

### Code

```yaml
  1 | 
  2 | base_dir: "/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP"
  3 | mode: 'global'  # å®éªŒæ¨¡å¼: global, weighting, splittingç­‰
  4 | 
  5 | # --- Device Configuration ---
  6 | # Specifies the hardware backend.
  7 | # Options:
  8 | #   'auto': (Recommended) Automatically detects best available hardware in order: cuda -> ipex -> xpu -> dml -> cpu
  9 | #   'cuda': NVIDIA GPU with full AMP support.
 10 | #   'ipex': Intel GPU with full IPEX optimizations and AMP support.
 11 | #   'xpu':  Intel GPU with basic device placement only (No IPEX optimizations, no AMP).
 12 | #   'dml':  DirectML-compatible GPU (AMD, Intel, etc.) (No AMP).
 13 | #   'cpu':  CPU execution.
 14 | device: 'auto'
 15 | 
 16 | # æ•°æ®é›†é…ç½®
 17 | dataset:
 18 |   name: "KuaiRand-Pure"  # æ•°æ®é›†åç§°: KuaiRand-Pure, KuaiRand-1K, KuaiRand-27K
 19 |   path: "data/KuaiRand/Pure"  # æ•°æ®é›†è·¯å¾„
 20 |   cache_path: "data/KuaiRand/cache"  # ç¼“å­˜ç›®å½•
 21 | 
 22 | # æ¨¡å‹è®­ç»ƒç›¸å…³ç‰¹å¾é…ç½®ï¼ˆæ ¹æ®æ•°æ®é›†ç»“æ„åˆ†ææŠ¥å‘Šï¼‰
 23 | feature:
 24 |   numerical:  # æ•°å€¼å‹ç‰¹å¾
 25 |     # è§†é¢‘åŸºç¡€ç‰¹å¾
 26 |     - "video_duration"        # è§†é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
 27 |     - "server_width"          # è§†é¢‘å®½åº¦
 28 |     - "server_height"         # è§†é¢‘é«˜åº¦
 29 |     # è§†é¢‘ç»Ÿè®¡ç‰¹å¾
 30 |     - "show_cnt"              # ç´¯è®¡æ›å…‰æ¬¡æ•°
 31 |     - "play_cnt"              # ç´¯è®¡æ’­æ”¾æ¬¡æ•°
 32 |     - "play_user_num"         # ç´¯è®¡æ’­æ”¾ç”¨æˆ·æ•°
 33 |     - "complete_play_cnt"     # ç´¯è®¡å®Œæ’­æ¬¡æ•°
 34 |     - "like_cnt"              # ç´¯è®¡ç‚¹èµæ•°
 35 |     - "comment_cnt"           # ç´¯è®¡è¯„è®ºæ•°
 36 |     - "share_cnt"             # ç´¯è®¡åˆ†äº«æ•°
 37 |     - "collect_cnt"           # ç´¯è®¡æ”¶è—æ•°
 38 |     # ç”¨æˆ·ç‰¹å¾
 39 |     - "is_live_streamer"      # æ˜¯å¦ä¸ºç›´æ’­ä¸»æ’­
 40 |     - "is_video_author"       # æ˜¯å¦ä¸ºè§†é¢‘åˆ›ä½œè€…
 41 |     - "follow_user_num"       # ç”¨æˆ·å…³æ³¨æ•°
 42 |     - "fans_user_num"         # ç”¨æˆ·ç²‰ä¸æ•°
 43 |     - "friend_user_num"       # ç”¨æˆ·å¥½å‹æ•°
 44 |     - "register_days"         # è´¦å·æ³¨å†Œå¤©æ•°
 45 |     # # ç”¨æˆ·onehotç‰¹å¾ï¼ˆä½œä¸ºæ•°å€¼ç‰¹å¾å¤„ç†ï¼‰
 46 |     # - "onehot_feat0"
 47 |     # - "onehot_feat1"
 48 |     # - "onehot_feat2"
 49 |     # - "onehot_feat3"
 50 |     # - "onehot_feat4"
 51 |     # - "onehot_feat5"
 52 |     # - "onehot_feat6"
 53 |     # - "onehot_feat7"
 54 |     # - "onehot_feat8"
 55 |     # - "onehot_feat9"
 56 |     # - "onehot_feat10"
 57 |     # - "onehot_feat11"
 58 |     # - "onehot_feat12"
 59 |     # - "onehot_feat13"
 60 |     # - "onehot_feat14"
 61 |     # - "onehot_feat15"
 62 |     # - "onehot_feat16"
 63 |     # - "onehot_feat17"
 64 |   categorical:  # åˆ†ç±»å‹ç‰¹å¾ï¼ˆå°†è½¬æ¢ä¸ºonehotå˜é‡ï¼‰
 65 |     - "user_active_degree"    # ç”¨æˆ·æ´»è·ƒåº¦ç­‰çº§
 66 |     - "video_type"            # è§†é¢‘ç±»å‹
 67 |     - "tag"                   # è§†é¢‘æ ‡ç­¾
 68 | 
 69 | # æ ‡ç­¾é…ç½®ï¼ˆå¤šæ ‡ç­¾é¢„æµ‹ï¼Œæ¯ä¸ªæ ‡ç­¾ä½¿ç”¨ç‹¬ç«‹æ¨¡å‹ï¼‰
 70 | labels:
 71 |   - name: "play_time"         # æ’­æ”¾æ—¶é•¿
 72 |     target: "play_time_ms"    # å¯¹åº”çš„æ•°æ®é›†å­—æ®µå
 73 |     type: "numerical"         # æ ‡ç­¾ç±»å‹: binary, numerical
 74 |     loss_function: "logMAE"   # æ’­æ”¾æ—¶é•¿ä½¿ç”¨logMAEæŸå¤±å‡½æ•°
 75 |     model: "MLP"              # æ¨¡å‹ç±»å‹
 76 |     model_params:
 77 |       hidden_layers: [128, 64, 32]  # 3å±‚MLPæ¶æ„
 78 |       dropout: 0.2                  # dropoutç‡
 79 |       embedding_dim: 16             # åˆ†ç±»ç‰¹å¾çš„åµŒå…¥ç»´åº¦
 80 |     learning_rate: 0.0001
 81 |     weight_decay: 0.0001
 82 |     alpha_T: 1.0                    # Treatmentç»„çš„alphaæƒé‡
 83 |     alpha_C: 0.5                    # Controlç»„çš„alphaæƒé‡
 84 |     
 85 |   - name: "click"             # ç‚¹å‡»
 86 |     target: "is_click"        # å¯¹åº”çš„æ•°æ®é›†å­—æ®µå
 87 |     type: "binary"            # æ ‡ç­¾ç±»å‹: binary, numerical
 88 |     loss_function: "BCE"      # ç‚¹å‡»ä½¿ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°
 89 |     model: "MLP"              # æ¨¡å‹ç±»å‹
 90 |     model_params:
 91 |       hidden_layers: [64, 32, 16]   # 3å±‚MLPæ¶æ„
 92 |       dropout: 0.1                  # dropoutç‡
 93 |       embedding_dim: 8              # åˆ†ç±»ç‰¹å¾çš„åµŒå…¥ç»´åº¦
 94 |     learning_rate: 0.0001
 95 |     weight_decay: 0.0001
 96 |     alpha_T: 1.0                    # Treatmentç»„çš„alphaæƒé‡
 97 |     alpha_C: 0.8                    # Controlç»„çš„alphaæƒé‡
 98 | 
 99 | # é¢„è®­ç»ƒé…ç½®
100 | pretrain:
101 |   enabled: true  # æ˜¯å¦å¯ç”¨é¢„è®­ç»ƒ
102 |   batch_size: 64
103 |   epochs: 1     # å‡å°‘epochæ•°é‡ï¼Œä»50å‡å°‘åˆ°10
104 |   learning_rate: 0.001
105 |   weight_decay: 0.0001
106 |   early_stopping: 3  # æå‰åœæ­¢çš„æ£€æŸ¥ç‚¹æ•°ï¼Œä»5å‡å°‘åˆ°3
107 | 
108 | # å…¨å±€ä»¿çœŸé…ç½® (ä»…åœ¨mode='global'æ—¶ä½¿ç”¨)
109 | global:
110 |   user_p_val: 0.2  # éªŒè¯é›†ç”¨æˆ·æ¯”ä¾‹
111 |   batch_size: 64   # æ¯æ­¥æŠ½æ ·çš„ç”¨æˆ·æ•°
112 |   n_candidate: 10  # æ¯ä¸ªç”¨æˆ·çš„å€™é€‰è§†é¢‘æ•°
113 |   n_steps: 5     # ä»¿çœŸæ€»æ­¥æ•°ï¼Œä»1000å‡å°‘åˆ°200
114 |   validate_every: 1  # æ¯éš”å¤šå°‘æ­¥éªŒè¯ä¸€æ¬¡ï¼Œä»50å‡å°‘åˆ°25
115 |   save_every: 50    # æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
116 |   learning_rate: 0.0005
117 |   weight_decay: 0.0001
118 | 
119 | # æ—¥å¿—é…ç½®
120 | logging:
121 |   level: "INFO"
122 |   log_dir: "results"
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\configs\experiment_optimized copy.yaml

- Extension: .yaml
- Language: yaml
- Size: 3446 bytes
- Created: 2025-08-18 10:14:34
- Modified: 2025-08-18 09:26:30

### Code

```yaml
  1 | # base_dir: "/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP"
  2 | base_dir: "E:/MyDocument/Codes_notnut/_notpad/IEDA/RealdataEXP"
  3 | mode: 'global_optimized'  # ä½¿ç”¨ä¼˜åŒ–æ¨¡å¼
  4 | 
  5 | # --- Device Configuration ---
  6 | # Specifies the hardware backend.
  7 | # Options:
  8 | #   'auto': (Recommended) Automatically detects best available hardware in order: cuda -> ipex -> xpu -> dml -> cpu
  9 | #   'cuda': NVIDIA GPU with full AMP support.
 10 | #   'ipex': Intel GPU with full IPEX optimizations and AMP support.
 11 | #   'xpu':  Intel GPU with basic device placement only (No IPEX optimizations, no AMP).
 12 | #   'dml':  DirectML-compatible GPU (AMD, Intel, etc.) (No AMP).
 13 | #   'cpu':  CPU execution.
 14 | device: 'auto'
 15 | 
 16 | # æ•°æ®é›†é…ç½®
 17 | dataset:
 18 |   name: "KuaiRand-Pure"
 19 |   path: "data/KuaiRand/Pure"  # Pureæ•°æ®åœ¨KuaiRand/Pureç›®å½•ä¸‹
 20 |   cache_path: "data/KuaiRand/cache"
 21 |   # --- æ–°å¢: DataLoaderä¼˜åŒ–å‚æ•° ---
 22 |   num_workers: 12  # ä½¿ç”¨12ä¸ªCPUæ ¸å¿ƒè¿›è¡Œå¹¶è¡Œæ•°æ®åŠ è½½
 23 |   pin_memory: true # é”å®šå†…å­˜ï¼ŒåŠ é€ŸCPUåˆ°GPUçš„æ•°æ®ä¼ è¾“
 24 | 
 25 | # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
 26 | use_amp: true
 27 | 
 28 | # ç‰¹å¾é…ç½®ï¼ˆ27Kæ•°æ®é›†ç‰¹å¾ï¼‰
 29 | feature:
 30 |   numerical:
 31 |     - "video_duration"
 32 |     - "server_width"
 33 |     - "server_height"
 34 |     - "show_cnt"
 35 |     - "play_cnt"
 36 |     - "play_user_num"
 37 |     - "complete_play_cnt"
 38 |     - "like_cnt"
 39 |     - "comment_cnt"
 40 |     - "share_cnt"
 41 |     - "collect_cnt"
 42 |     - "is_live_streamer"
 43 |     - "is_video_author"
 44 |     - "follow_user_num"
 45 |     - "fans_user_num"
 46 |     - "friend_user_num"
 47 |     - "register_days"
 48 |   categorical:
 49 |     - "user_active_degree"
 50 |     - "video_type"
 51 |     - "tag"
 52 | 
 53 | # æ ‡ç­¾é…ç½®ï¼ˆè°ƒæ•´äº†æ¨¡å‹å‚æ•°ä»¥æå‡æ•ˆæœï¼‰
 54 | labels:
 55 |   - name: "play_time"
 56 |     target: "play_time_ms"
 57 |     type: "numerical"
 58 |     loss_function: "logMAE"
 59 |     model: "MLP"
 60 |     model_params:
 61 |       hidden_layers: [256, 128, 64, 32]  # å¢åŠ æ¨¡å‹å®¹é‡
 62 |       dropout: 0.3  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
 63 |       # dropout: 0.0 # dmlä¼šå‡é€Ÿ
 64 |       embedding_dim: 32  # å¢åŠ åµŒå…¥ç»´åº¦
 65 |     learning_rate: 0.01  # ç¨å¾®å¢åŠ å­¦ä¹ ç‡
 66 |     weight_decay: 0.005
 67 |     alpha_T: 1.0
 68 |     alpha_C: 0.5
 69 |     
 70 |   - name: "click"
 71 |     target: "is_click"
 72 |     type: "binary"
 73 |     loss_function: "BCE"
 74 |     model: "MLP"
 75 |     model_params:
 76 |       hidden_layers: [128, 64, 32, 16]  # å¢åŠ æ¨¡å‹å®¹é‡
 77 |       dropout: 0.2  # é€‚åº¦dropout
 78 |       # dropout: 0.0
 79 |       embedding_dim: 16  # å¢åŠ åµŒå…¥ç»´åº¦
 80 |     learning_rate: 0.01  # ç¨å¾®å¢åŠ å­¦ä¹ ç‡
 81 |     weight_decay: 0.005
 82 |     alpha_T: 1.0
 83 |     alpha_C: 0.8
 84 | 
 85 | # é¢„è®­ç»ƒé…ç½®ï¼ˆä¼˜åŒ–ï¼‰
 86 | pretrain:
 87 |   enabled: true
 88 |   batch_size: 64  # å¢åŠ batch sizeä»¥æ›´å¥½åˆ©ç”¨GPU
 89 |   epochs: 150  
 90 |   learning_rate: 0.001
 91 |   weight_decay: 0.001
 92 |   early_stopping: 10
 93 |   # --- æ–°å¢é…ç½® ---
 94 |   # æŒ‡å®šè¦åŠ è½½çš„é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºnullåˆ™ä¸åŠ è½½
 95 |   load_checkpoint_path: null # example: "results/20250818_0110/checkpoints/pretrain_epoch_1.pt"
 96 |   # é¢„è®­ç»ƒæ•°æ®çš„éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
 97 |   val_split_ratio: 0.5
 98 |   # æ˜¯å¦åœ¨æ¯ä¸ªepochåç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿å›¾
 99 |   plot_loss_curves: true
100 | 
101 | # å…¨å±€ä»¿çœŸé…ç½®ï¼ˆä¼˜åŒ–ï¼‰
102 | global:
103 |   user_p_val: 0.2
104 |   batch_size: 128  # å¢åŠ batch size
105 |   n_candidate: 10
106 |   n_steps: 5  # å‡å°‘æ­¥æ•°ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•ä¼˜åŒ–æ•ˆæœ
107 |   validate_every: 1  # æ›´é¢‘ç¹çš„éªŒè¯
108 |   save_every: 25
109 |   learning_rate: 0.01
110 |   weight_decay: 0.005
111 | 
112 | # æ—¥å¿—é…ç½®
113 | logging:
114 |   level: "INFO"
115 |   log_dir: "results"
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\configs\experiment_optimized.yaml

- Extension: .yaml
- Language: yaml
- Size: 3438 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-18 10:14:55

### Code

```yaml
  1 | # base_dir: "/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP"
  2 | base_dir: "E:/MyDocument/Codes_notnut/_notpad/IEDA/RealdataEXP"
  3 | mode: 'global_optimized'  # ä½¿ç”¨ä¼˜åŒ–æ¨¡å¼
  4 | 
  5 | # --- Device Configuration ---
  6 | # Specifies the hardware backend.
  7 | # Options:
  8 | #   'auto': (Recommended) Automatically detects best available hardware in order: cuda -> ipex -> xpu -> dml -> cpu
  9 | #   'cuda': NVIDIA GPU with full AMP support.
 10 | #   'ipex': Intel GPU with full IPEX optimizations and AMP support.
 11 | #   'xpu':  Intel GPU with basic device placement only (No IPEX optimizations, no AMP).
 12 | #   'dml':  DirectML-compatible GPU (AMD, Intel, etc.) (No AMP).
 13 | #   'cpu':  CPU execution.
 14 | device: 'auto'
 15 | 
 16 | # æ•°æ®é›†é…ç½®
 17 | dataset:
 18 |   name: "KuaiRand-1K"
 19 |   path: "data/KuaiRand/1K"  # 1Kæ•°æ®åœ¨KuaiRand/1Kç›®å½•ä¸‹
 20 |   cache_path: "data/KuaiRand/cache"
 21 |   # --- æ–°å¢: DataLoaderä¼˜åŒ–å‚æ•° ---
 22 |   num_workers: 12  # ä½¿ç”¨12ä¸ªCPUæ ¸å¿ƒè¿›è¡Œå¹¶è¡Œæ•°æ®åŠ è½½
 23 |   pin_memory: true # é”å®šå†…å­˜ï¼ŒåŠ é€ŸCPUåˆ°GPUçš„æ•°æ®ä¼ è¾“
 24 | 
 25 | # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
 26 | use_amp: true
 27 | 
 28 | # ç‰¹å¾é…ç½®ï¼ˆ27Kæ•°æ®é›†ç‰¹å¾ï¼‰
 29 | feature:
 30 |   numerical:
 31 |     - "video_duration"
 32 |     - "server_width"
 33 |     - "server_height"
 34 |     - "show_cnt"
 35 |     - "play_cnt"
 36 |     - "play_user_num"
 37 |     - "complete_play_cnt"
 38 |     - "like_cnt"
 39 |     - "comment_cnt"
 40 |     - "share_cnt"
 41 |     - "collect_cnt"
 42 |     - "is_live_streamer"
 43 |     - "is_video_author"
 44 |     - "follow_user_num"
 45 |     - "fans_user_num"
 46 |     - "friend_user_num"
 47 |     - "register_days"
 48 |   categorical:
 49 |     - "user_active_degree"
 50 |     - "video_type"
 51 |     - "tag"
 52 | 
 53 | # æ ‡ç­¾é…ç½®ï¼ˆè°ƒæ•´äº†æ¨¡å‹å‚æ•°ä»¥æå‡æ•ˆæœï¼‰
 54 | labels:
 55 |   - name: "play_time"
 56 |     target: "play_time_ms"
 57 |     type: "numerical"
 58 |     loss_function: "logMAE"
 59 |     model: "MLP"
 60 |     model_params:
 61 |       hidden_layers: [256, 128, 64, 32]  # å¢åŠ æ¨¡å‹å®¹é‡
 62 |       dropout: 0.3  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
 63 |       # dropout: 0.0 # dmlä¼šå‡é€Ÿ
 64 |       embedding_dim: 32  # å¢åŠ åµŒå…¥ç»´åº¦
 65 |     learning_rate: 0.01  # ç¨å¾®å¢åŠ å­¦ä¹ ç‡
 66 |     weight_decay: 0.005
 67 |     alpha_T: 1.0
 68 |     alpha_C: 0.5
 69 |     
 70 |   - name: "click"
 71 |     target: "is_click"
 72 |     type: "binary"
 73 |     loss_function: "BCE"
 74 |     model: "MLP"
 75 |     model_params:
 76 |       hidden_layers: [128, 64, 32, 16]  # å¢åŠ æ¨¡å‹å®¹é‡
 77 |       dropout: 0.2  # é€‚åº¦dropout
 78 |       # dropout: 0.0
 79 |       embedding_dim: 16  # å¢åŠ åµŒå…¥ç»´åº¦
 80 |     learning_rate: 0.01  # ç¨å¾®å¢åŠ å­¦ä¹ ç‡
 81 |     weight_decay: 0.005
 82 |     alpha_T: 1.0
 83 |     alpha_C: 0.8
 84 | 
 85 | # é¢„è®­ç»ƒé…ç½®ï¼ˆä¼˜åŒ–ï¼‰
 86 | pretrain:
 87 |   enabled: true
 88 |   batch_size: 64  # å¢åŠ batch sizeä»¥æ›´å¥½åˆ©ç”¨GPU
 89 |   epochs: 150  
 90 |   learning_rate: 0.001
 91 |   weight_decay: 0.001
 92 |   early_stopping: 10
 93 |   # --- æ–°å¢é…ç½® ---
 94 |   # æŒ‡å®šè¦åŠ è½½çš„é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºnullåˆ™ä¸åŠ è½½
 95 |   load_checkpoint_path: null # example: "results/20250818_0110/checkpoints/pretrain_epoch_1.pt"
 96 |   # é¢„è®­ç»ƒæ•°æ®çš„éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
 97 |   val_split_ratio: 0.5
 98 |   # æ˜¯å¦åœ¨æ¯ä¸ªepochåç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿å›¾
 99 |   plot_loss_curves: true
100 | 
101 | # å…¨å±€ä»¿çœŸé…ç½®ï¼ˆä¼˜åŒ–ï¼‰
102 | global:
103 |   user_p_val: 0.2
104 |   batch_size: 128  # å¢åŠ batch size
105 |   n_candidate: 10
106 |   n_steps: 5  # å‡å°‘æ­¥æ•°ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•ä¼˜åŒ–æ•ˆæœ
107 |   validate_every: 1  # æ›´é¢‘ç¹çš„éªŒè¯
108 |   save_every: 25
109 |   learning_rate: 0.01
110 |   weight_decay: 0.005
111 | 
112 | # æ—¥å¿—é…ç½®
113 | logging:
114 |   level: "INFO"
115 |   log_dir: "results"
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\configs\experiment_yanc.yaml

- Extension: .yaml
- Language: yaml
- Size: 3439 bytes
- Created: 2025-08-18 02:45:40
- Modified: 2025-08-18 09:02:55

### Code

```yaml
  1 | # base_dir: "/home/zhixuanhu/IEDA_WeightedTraining/RealdataEXP"
  2 | base_dir: "E:/MyDocument/Codes_notnut/_notpad/IEDA/RealdataEXP"
  3 | mode: 'global_optimized'  # ä½¿ç”¨ä¼˜åŒ–æ¨¡å¼
  4 | 
  5 | # --- Device Configuration ---
  6 | # Specifies the hardware backend.
  7 | # Options:
  8 | #   'auto': (Recommended) Automatically detects best available hardware in order: cuda -> ipex -> xpu -> dml -> cpu
  9 | #   'cuda': NVIDIA GPU with full AMP support.
 10 | #   'ipex': Intel GPU with full IPEX optimizations and AMP support.
 11 | #   'xpu':  Intel GPU with basic device placement only (No IPEX optimizations, no AMP).
 12 | #   'dml':  DirectML-compatible GPU (AMD, Intel, etc.) (No AMP).
 13 | #   'cpu':  CPU execution.
 14 | device: 'auto'
 15 | 
 16 | # æ•°æ®é›†é…ç½®
 17 | dataset:
 18 |   name: "KuaiRand-1K"
 19 |   path: "data/KuaiRand/1K"  # 1Kæ•°æ®åœ¨KuaiRand/1Kç›®å½•ä¸‹
 20 |   cache_path: "data/KuaiRand/cache"
 21 |   # --- æ–°å¢: DataLoaderä¼˜åŒ–å‚æ•° ---
 22 |   num_workers: 32  # ä½¿ç”¨32ä¸ªCPUæ ¸å¿ƒè¿›è¡Œå¹¶è¡Œæ•°æ®åŠ è½½
 23 |   pin_memory: true # é”å®šå†…å­˜ï¼ŒåŠ é€ŸCPUåˆ°GPUçš„æ•°æ®ä¼ è¾“
 24 | 
 25 | # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
 26 | use_amp: true
 27 | 
 28 | # ç‰¹å¾é…ç½®ï¼ˆ27Kæ•°æ®é›†ç‰¹å¾ï¼‰
 29 | feature:
 30 |   numerical:
 31 |     - "video_duration"
 32 |     - "server_width"
 33 |     - "server_height"
 34 |     - "show_cnt"
 35 |     - "play_cnt"
 36 |     - "play_user_num"
 37 |     - "complete_play_cnt"
 38 |     - "like_cnt"
 39 |     - "comment_cnt"
 40 |     - "share_cnt"
 41 |     - "collect_cnt"
 42 |     - "is_live_streamer"
 43 |     - "is_video_author"
 44 |     - "follow_user_num"
 45 |     - "fans_user_num"
 46 |     - "friend_user_num"
 47 |     - "register_days"
 48 |   categorical:
 49 |     - "user_active_degree"
 50 |     - "video_type"
 51 |     - "tag"
 52 | 
 53 | # æ ‡ç­¾é…ç½®ï¼ˆè°ƒæ•´äº†æ¨¡å‹å‚æ•°ä»¥æå‡æ•ˆæœï¼‰
 54 | labels:
 55 |   - name: "play_time"
 56 |     target: "play_time_ms"
 57 |     type: "numerical"
 58 |     loss_function: "logMAE"
 59 |     model: "MLP"
 60 |     model_params:
 61 |       hidden_layers: [256, 128, 64, 32]  # å¢åŠ æ¨¡å‹å®¹é‡
 62 |       dropout: 0.3  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
 63 |       # dropout: 0.0 # dmlä¼šå‡é€Ÿ
 64 |       embedding_dim: 32  # å¢åŠ åµŒå…¥ç»´åº¦
 65 |     learning_rate: 0.01  # ç¨å¾®å¢åŠ å­¦ä¹ ç‡
 66 |     weight_decay: 0.005
 67 |     alpha_T: 1.0
 68 |     alpha_C: 0.5
 69 |     
 70 |   - name: "click"
 71 |     target: "is_click"
 72 |     type: "binary"
 73 |     loss_function: "BCE"
 74 |     model: "MLP"
 75 |     model_params:
 76 |       hidden_layers: [128, 64, 32, 16]  # å¢åŠ æ¨¡å‹å®¹é‡
 77 |       dropout: 0.2  # é€‚åº¦dropout
 78 |       # dropout: 0.0
 79 |       embedding_dim: 16  # å¢åŠ åµŒå…¥ç»´åº¦
 80 |     learning_rate: 0.01  # ç¨å¾®å¢åŠ å­¦ä¹ ç‡
 81 |     weight_decay: 0.005
 82 |     alpha_T: 1.0
 83 |     alpha_C: 0.8
 84 | 
 85 | # é¢„è®­ç»ƒé…ç½®ï¼ˆä¼˜åŒ–ï¼‰
 86 | pretrain:
 87 |   enabled: true
 88 |   batch_size: 256  # å¢åŠ batch sizeä»¥æ›´å¥½åˆ©ç”¨GPU
 89 |   epochs: 150  
 90 |   learning_rate: 0.001
 91 |   weight_decay: 0.005
 92 |   early_stopping: 10
 93 |   # --- æ–°å¢é…ç½® ---
 94 |   # æŒ‡å®šè¦åŠ è½½çš„é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºnullåˆ™ä¸åŠ è½½
 95 |   load_checkpoint_path: null # example: "results/20250818_0110/checkpoints/pretrain_epoch_1.pt"
 96 |   # é¢„è®­ç»ƒæ•°æ®çš„éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
 97 |   val_split_ratio: 0.5
 98 |   # æ˜¯å¦åœ¨æ¯ä¸ªepochåç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿å›¾
 99 |   plot_loss_curves: true
100 | 
101 | # å…¨å±€ä»¿çœŸé…ç½®ï¼ˆä¼˜åŒ–ï¼‰
102 | global:
103 |   user_p_val: 0.2
104 |   batch_size: 128  # å¢åŠ batch size
105 |   n_candidate: 10
106 |   n_steps: 5  # å‡å°‘æ­¥æ•°ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•ä¼˜åŒ–æ•ˆæœ
107 |   validate_every: 1  # æ›´é¢‘ç¹çš„éªŒè¯
108 |   save_every: 25
109 |   learning_rate: 0.01
110 |   weight_decay: 0.005
111 | 
112 | # æ—¥å¿—é…ç½®
113 | logging:
114 |   level: "INFO"
115 |   log_dir: "results"
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\__init__.py

- Extension: .py
- Language: python
- Size: 89 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
1 | """
2 | RealdataEXP æ ¸å¿ƒåº“
3 | """
4 | 
5 | __version__ = "1.0.0"
6 | __author__ = "RealdataEXP Team"
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\data\cache_manager.py

- Extension: .py
- Language: python
- Size: 2507 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
 1 | """
 2 | ç¼“å­˜ç®¡ç†å™¨
 3 | æä¾›æ•°æ®çš„æŒä¹…åŒ–ç¼“å­˜åŠŸèƒ½ï¼Œé¿å…é‡å¤è®¡ç®—
 4 | """
 5 | 
 6 | import os
 7 | import pickle
 8 | import logging
 9 | from typing import Any, Optional
10 | 
11 | logger = logging.getLogger(__name__)
12 | 
13 | class CacheManager:
14 |     """ç¼“å­˜ç®¡ç†å™¨"""
15 |     
16 |     def __init__(self, cache_dir: str):
17 |         self.cache_dir = cache_dir
18 |         self._ensure_cache_dir()
19 |         
20 |     def _ensure_cache_dir(self):
21 |         """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
22 |         if not os.path.exists(self.cache_dir):
23 |             os.makedirs(self.cache_dir)
24 |             logger.info(f"[ç¼“å­˜] åˆ›å»ºç¼“å­˜ç›®å½•: {self.cache_dir}")
25 |     
26 |     def _get_cache_path(self, key: str) -> str:
27 |         """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
28 |         return os.path.join(self.cache_dir, f"{key}.pkl")
29 |     
30 |     def save(self, data: Any, key: str) -> None:
31 |         """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
32 |         cache_path = self._get_cache_path(key)
33 |         try:
34 |             with open(cache_path, 'wb') as f:
35 |                 pickle.dump(data, f)
36 |             logger.info(f"[ç¼“å­˜] æ•°æ®å·²ä¿å­˜: {key}")
37 |         except Exception as e:
38 |             logger.error(f"[ç¼“å­˜] ä¿å­˜å¤±è´¥ {key}: {e}")
39 |             
40 |     def load(self, key: str) -> Optional[Any]:
41 |         """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
42 |         cache_path = self._get_cache_path(key)
43 |         
44 |         if not os.path.exists(cache_path):
45 |             return None
46 |             
47 |         try:
48 |             with open(cache_path, 'rb') as f:
49 |                 data = pickle.load(f)
50 |             logger.info(f"[ç¼“å­˜] æ•°æ®å·²åŠ è½½: {key}")
51 |             return data
52 |         except Exception as e:
53 |             logger.error(f"[ç¼“å­˜] åŠ è½½å¤±è´¥ {key}: {e}")
54 |             return None
55 |     
56 |     def exists(self, key: str) -> bool:
57 |         """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
58 |         cache_path = self._get_cache_path(key)
59 |         return os.path.exists(cache_path)
60 |     
61 |     def clear(self, key: str) -> None:
62 |         """æ¸…é™¤æŒ‡å®šç¼“å­˜"""
63 |         cache_path = self._get_cache_path(key)
64 |         if os.path.exists(cache_path):
65 |             os.remove(cache_path)
66 |             logger.info(f"[ç¼“å­˜] ç¼“å­˜å·²æ¸…é™¤: {key}")
67 |     
68 |     def clear_all(self) -> None:
69 |         """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
70 |         if os.path.exists(self.cache_dir):
71 |             for filename in os.listdir(self.cache_dir):
72 |                 if filename.endswith('.pkl'):
73 |                     os.remove(os.path.join(self.cache_dir, filename))
74 |             logger.info("[ç¼“å­˜] æ‰€æœ‰ç¼“å­˜å·²æ¸…é™¤")
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\data\data_loader.py

- Extension: .py
- Language: python
- Size: 11904 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
  1 | """
  2 | KuaiRandæ•°æ®é›†åŠ è½½å™¨
  3 | è´Ÿè´£ä»åŸå§‹æ•°æ®æ–‡ä»¶ä¸­åŠ è½½ç”¨æˆ·è¡Œä¸ºæ—¥å¿—ã€ç”¨æˆ·ç‰¹å¾å’Œè§†é¢‘ç‰¹å¾
  4 | æ”¯æŒåˆ†ç‰‡æ–‡ä»¶è‡ªåŠ¨åˆå¹¶å’Œä¼˜åŒ–çš„PyTorch Datasetæ¥å£
  5 | """
  6 | 
  7 | import os
  8 | import pandas as pd
  9 | import numpy as np
 10 | import logging
 11 | from typing import Dict, List, Tuple, Optional
 12 | import torch
 13 | from torch.utils.data import Dataset
 14 | from .cache_manager import CacheManager
 15 | 
 16 | logger = logging.getLogger(__name__)
 17 | 
 18 | class TabularDataset(Dataset):
 19 |     """ç”¨äºè¡¨æ ¼æ•°æ®çš„PyTorch Dataset"""
 20 |     
 21 |     def __init__(self, features: pd.DataFrame, labels: pd.DataFrame, label_configs: List[Dict]):
 22 |         """
 23 |         Args:
 24 |             features (pd.DataFrame): åŒ…å«æ‰€æœ‰è¾“å…¥ç‰¹å¾çš„DataFrame
 25 |             labels (pd.DataFrame): åŒ…å«æ‰€æœ‰ç›®æ ‡æ ‡ç­¾çš„DataFrame
 26 |             label_configs (List[Dict]): æ ‡ç­¾çš„é…ç½®ä¿¡æ¯
 27 |         """
 28 |         self.features = torch.tensor(features.values, dtype=torch.float32)
 29 |         self.labels = {}
 30 |         self.label_configs = label_configs
 31 |         
 32 |         for config in self.label_configs:
 33 |             target_col = config['target']
 34 |             if target_col in labels.columns:
 35 |                 self.labels[config['name']] = torch.tensor(labels[target_col].values, dtype=torch.float32).unsqueeze(1)
 36 |             
 37 |     def __len__(self):
 38 |         return len(self.features)
 39 |         
 40 |     def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
 41 |         feature_vector = self.features[idx]
 42 |         target_dict = {name: label_tensor[idx] for name, label_tensor in self.labels.items()}
 43 |         return feature_vector, target_dict
 44 | 
 45 | class KuaiRandDataLoader:
 46 |     """KuaiRandæ•°æ®é›†åŠ è½½å™¨"""
 47 |     
 48 |     def __init__(self, config: Dict):
 49 |         self.config = config
 50 |         self.dataset_path = config['dataset']['path']
 51 |         self.cache_manager = CacheManager(config['dataset']['cache_path'])
 52 |         
 53 |         # æ ¹æ®æ•°æ®é›†åç§°é€‰æ‹©æ•°æ®æ–‡ä»¶æ˜ å°„
 54 |         dataset_name = config['dataset']['name']
 55 |         if dataset_name == "KuaiRand-Pure":
 56 |             self.data_files = {
 57 |                 'log_random': 'data/log_random_4_22_to_5_08_pure.csv',
 58 |                 'log_standard_early': 'data/log_standard_4_08_to_4_21_pure.csv', 
 59 |                 'log_standard_late': 'data/log_standard_4_22_to_5_08_pure.csv',
 60 |                 'user_features': 'data/user_features_pure.csv',
 61 |                 'video_basic': 'data/video_features_basic_pure.csv',
 62 |                 'video_statistic': 'data/video_features_statistic_pure.csv'
 63 |             }
 64 |         elif dataset_name == "KuaiRand-27K":
 65 |             self.data_files = {
 66 |                 'log_random': 'data/log_random_4_22_to_5_08_27k.csv',
 67 |                 'log_standard_early': 'data/log_standard_4_08_to_4_21_27k.csv', 
 68 |                 'log_standard_late': 'data/log_standard_4_22_to_5_08_27k.csv',
 69 |                 'user_features': 'data/user_features_27k.csv',
 70 |                 'video_basic': 'data/video_features_basic_27k.csv',
 71 |                 'video_statistic': 'data/video_features_statistic_27k.csv'
 72 |             }
 73 |         else:
 74 |             raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
 75 |         
 76 |         # å†…å­˜ä¸­çš„æ•°æ®
 77 |         self.user_video_lists = {}  # user_id -> list of video_ids
 78 |         self.merged_data = None
 79 |         self.train_users = None
 80 |         self.val_users = None
 81 |         
 82 |     def load_all_data(self) -> Dict[str, pd.DataFrame]:
 83 |         """åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒåˆ†ç‰‡æ–‡ä»¶è‡ªåŠ¨åˆå¹¶"""
 84 |         logger.info("[æ•°æ®åŠ è½½] å¼€å§‹åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶...")
 85 |         
 86 |         data = {}
 87 |         total_files = len(self.data_files)
 88 |         
 89 |         for i, (key, file_path) in enumerate(self.data_files.items(), 1):
 90 |             full_path = os.path.join(self.dataset_path, file_path)
 91 |             logger.info(f"[æ•°æ®åŠ è½½] ({i}/{total_files}) æ­£åœ¨åŠ è½½ {key}: {file_path}")
 92 |             
 93 |             # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åˆ†ç‰‡æ–‡ä»¶
 94 |             base_name = os.path.splitext(file_path)[0]
 95 |             base_path = os.path.join(self.dataset_path, base_name)
 96 |             
 97 |             # æŸ¥æ‰¾åˆ†ç‰‡æ–‡ä»¶
 98 |             part_files = []
 99 |             for part_num in range(1, 10):  # æœ€å¤šæ”¯æŒ9ä¸ªåˆ†ç‰‡
100 |                 part_file = f"{base_path}_part{part_num}.csv"
101 |                 if os.path.exists(part_file):
102 |                     part_files.append(part_file)
103 |                 else:
104 |                     break
105 |             
106 |             if part_files:
107 |                 # å­˜åœ¨åˆ†ç‰‡æ–‡ä»¶ï¼Œè¿›è¡Œåˆå¹¶
108 |                 logger.info(f"[æ•°æ®åŠ è½½] å‘ç° {key} çš„åˆ†ç‰‡æ–‡ä»¶ {len(part_files)} ä¸ªï¼Œå¼€å§‹åˆå¹¶...")
109 |                 dfs = []
110 |                 for part_file in part_files:
111 |                     logger.info(f"[æ•°æ®åŠ è½½] æ­£åœ¨åŠ è½½åˆ†ç‰‡: {os.path.basename(part_file)}")
112 |                     df_part = pd.read_csv(part_file)
113 |                     logger.info(f"[æ•°æ®åŠ è½½] åˆ†ç‰‡å½¢çŠ¶: {df_part.shape}")
114 |                     dfs.append(df_part)
115 |                 
116 |                 # åˆå¹¶æ‰€æœ‰åˆ†ç‰‡
117 |                 data[key] = pd.concat(dfs, ignore_index=True)
118 |                 logger.info(f"[æ•°æ®åŠ è½½] {key} åˆ†ç‰‡åˆå¹¶å®Œæˆï¼Œæ€»å½¢çŠ¶: {data[key].shape}")
119 |             else:
120 |                 # æ²¡æœ‰åˆ†ç‰‡æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½
121 |                 if not os.path.exists(full_path):
122 |                     raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
123 |                     
124 |                 data[key] = pd.read_csv(full_path)
125 |                 logger.info(f"[æ•°æ®åŠ è½½] {key} åŠ è½½å®Œæˆï¼Œå½¢çŠ¶: {data[key].shape}")
126 |             
127 |         logger.info("[æ•°æ®åŠ è½½] æ‰€æœ‰æ•°æ®æ–‡ä»¶åŠ è½½å®Œæˆ")
128 |         return data
129 |     
130 |     def merge_features(self, log_data: pd.DataFrame, user_features: pd.DataFrame,
131 |                       video_basic: pd.DataFrame, video_statistic: pd.DataFrame) -> pd.DataFrame:
132 |         """åˆå¹¶æ‰€æœ‰ç‰¹å¾æ•°æ®"""
133 |         logger.info("[ç‰¹å¾åˆå¹¶] å¼€å§‹åˆå¹¶ç”¨æˆ·å’Œè§†é¢‘ç‰¹å¾...")
134 |         
135 |         # åˆå¹¶ç”¨æˆ·ç‰¹å¾
136 |         merged = log_data.merge(user_features, on='user_id', how='left')
137 |         logger.info(f"[ç‰¹å¾åˆå¹¶] åˆå¹¶ç”¨æˆ·ç‰¹å¾åå½¢çŠ¶: {merged.shape}")
138 |         
139 |         # åˆå¹¶è§†é¢‘åŸºç¡€ç‰¹å¾
140 |         merged = merged.merge(video_basic, on='video_id', how='left')
141 |         logger.info(f"[ç‰¹å¾åˆå¹¶] åˆå¹¶è§†é¢‘åŸºç¡€ç‰¹å¾åå½¢çŠ¶: {merged.shape}")
142 |         
143 |         # åˆå¹¶è§†é¢‘ç»Ÿè®¡ç‰¹å¾
144 |         merged = merged.merge(video_statistic, on='video_id', how='left')
145 |         logger.info(f"[ç‰¹å¾åˆå¹¶] åˆå¹¶è§†é¢‘ç»Ÿè®¡ç‰¹å¾åå½¢çŠ¶: {merged.shape}")
146 |         
147 |         logger.info("[ç‰¹å¾åˆå¹¶] ç‰¹å¾åˆå¹¶å®Œæˆ")
148 |         return merged
149 |     
150 |     def create_user_video_lists(self, merged_data: pd.DataFrame) -> Dict[int, List[int]]:
151 |         """åˆ›å»ºç”¨æˆ·-è§†é¢‘äº¤äº’åˆ—è¡¨ï¼ˆç¼“å­˜æœºåˆ¶ï¼‰"""
152 |         cache_key = "user_video_lists"
153 |         
154 |         # å°è¯•ä»ç¼“å­˜åŠ è½½
155 |         cached_data = self.cache_manager.load(cache_key)
156 |         if cached_data is not None:
157 |             logger.info("[ç¼“å­˜] ä»ç¼“å­˜åŠ è½½ç”¨æˆ·-è§†é¢‘äº¤äº’åˆ—è¡¨")
158 |             return cached_data
159 |             
160 |         logger.info("[ç”¨æˆ·è§†é¢‘åˆ—è¡¨] å¼€å§‹åˆ›å»ºç”¨æˆ·-è§†é¢‘äº¤äº’åˆ—è¡¨...")
161 |         
162 |         user_video_lists = {}
163 |         for user_id in merged_data['user_id'].unique():
164 |             video_list = merged_data[merged_data['user_id'] == user_id]['video_id'].tolist()
165 |             user_video_lists[user_id] = video_list
166 |             
167 |         logger.info(f"[ç”¨æˆ·è§†é¢‘åˆ—è¡¨] åˆ›å»ºå®Œæˆï¼Œå…± {len(user_video_lists)} ä¸ªç”¨æˆ·")
168 |         
169 |         # ä¿å­˜åˆ°ç¼“å­˜
170 |         self.cache_manager.save(user_video_lists, cache_key)
171 |         logger.info("[ç¼“å­˜] ç”¨æˆ·-è§†é¢‘äº¤äº’åˆ—è¡¨å·²ä¿å­˜åˆ°ç¼“å­˜")
172 |         
173 |         return user_video_lists
174 |     
175 |     def split_users(self, user_list: List[int], val_ratio: float) -> Tuple[List[int], List[int]]:
176 |         """å°†ç”¨æˆ·åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†"""
177 |         logger.info(f"[ç”¨æˆ·åˆ’åˆ†] å¼€å§‹åˆ’åˆ†ç”¨æˆ·ï¼ŒéªŒè¯é›†æ¯”ä¾‹: {val_ratio}")
178 |         
179 |         np.random.shuffle(user_list)
180 |         split_idx = int(len(user_list) * (1 - val_ratio))
181 |         
182 |         train_users = user_list[:split_idx]
183 |         val_users = user_list[split_idx:]
184 |         
185 |         logger.info(f"[ç”¨æˆ·åˆ’åˆ†] è®­ç»ƒç”¨æˆ·æ•°: {len(train_users)}, éªŒè¯ç”¨æˆ·æ•°: {len(val_users)}")
186 |         return train_users, val_users
187 |     
188 |     def add_mask_and_used_flags(self, merged_data: pd.DataFrame, val_users: List[int]) -> pd.DataFrame:
189 |         """æ·»åŠ maskå’Œusedæ ‡è®°ä½"""
190 |         logger.info("[æ ‡è®°ä½] æ·»åŠ maskå’Œusedæ ‡è®°ä½...")
191 |         
192 |         # æ·»åŠ maskæ ‡è®°ï¼šéªŒè¯é›†ç”¨æˆ·çš„è§†é¢‘æ ‡è®°ä¸º1
193 |         merged_data['mask'] = merged_data['user_id'].isin(val_users).astype(int)
194 |         
195 |         # æ·»åŠ usedæ ‡è®°ï¼šåˆå§‹åŒ–ä¸º0
196 |         merged_data['used'] = 0
197 |         
198 |         mask_count = merged_data['mask'].sum()
199 |         total_count = len(merged_data)
200 |         
201 |         logger.info(f"[æ ‡è®°ä½] mask=1çš„æ ·æœ¬æ•°: {mask_count}/{total_count} ({mask_count/total_count:.2%})")
202 |         logger.info("[æ ‡è®°ä½] æ ‡è®°ä½æ·»åŠ å®Œæˆ")
203 |         
204 |         return merged_data
205 |     
206 |     def load_and_prepare_data(self) -> Tuple[pd.DataFrame, Dict[int, List[int]], List[int], List[int]]:
207 |         """åŠ è½½å¹¶å‡†å¤‡æ‰€æœ‰æ•°æ®"""
208 |         logger.info("[æ•°æ®å‡†å¤‡] å¼€å§‹æ•°æ®åŠ è½½å’Œå‡†å¤‡æµç¨‹...")
209 |         
210 |         # åŠ è½½åŸå§‹æ•°æ®
211 |         raw_data = self.load_all_data()
212 |         
213 |         # åˆå¹¶æ—¥å¿—æ•°æ®
214 |         logger.info("[æ•°æ®åˆå¹¶] åˆå¹¶å¤šä¸ªæ—¥å¿—æ–‡ä»¶...")
215 |         log_combined = pd.concat([
216 |             raw_data['log_random'],
217 |             raw_data['log_standard_early'], 
218 |             raw_data['log_standard_late']
219 |         ], ignore_index=True)
220 |         logger.info(f"[æ•°æ®åˆå¹¶] åˆå¹¶åæ—¥å¿—æ•°æ®å½¢çŠ¶: {log_combined.shape}")
221 |         
222 |         # åˆå¹¶ç‰¹å¾
223 |         merged_data = self.merge_features(
224 |             log_combined, 
225 |             raw_data['user_features'],
226 |             raw_data['video_basic'],
227 |             raw_data['video_statistic']
228 |         )
229 |         
230 |         # åˆ›å»ºç”¨æˆ·-è§†é¢‘äº¤äº’åˆ—è¡¨
231 |         user_video_lists = self.create_user_video_lists(merged_data)
232 |         
233 |         # ç”¨æˆ·åˆ’åˆ†
234 |         all_users = list(merged_data['user_id'].unique())
235 |         train_users, val_users = self.split_users(all_users, self.config['global']['user_p_val'])
236 |         
237 |         # æ·»åŠ æ ‡è®°ä½
238 |         merged_data = self.add_mask_and_used_flags(merged_data, val_users)
239 |         
240 |         logger.info("[æ•°æ®å‡†å¤‡] æ•°æ®å‡†å¤‡æµç¨‹å®Œæˆ")
241 |         
242 |         # å­˜å‚¨åˆ°å®ä¾‹å˜é‡
243 |         self.merged_data = merged_data
244 |         self.user_video_lists = user_video_lists
245 |         self.train_users = train_users
246 |         self.val_users = val_users
247 |         
248 |         return merged_data, user_video_lists, train_users, val_users
249 |     
250 |     def get_dataset_stats(self) -> Dict:
251 |         """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
252 |         if self.merged_data is None:
253 |             raise ValueError("æ•°æ®å°šæœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_and_prepare_data()")
254 |             
255 |         stats = {
256 |             'total_samples': len(self.merged_data),
257 |             'unique_users': self.merged_data['user_id'].nunique(),
258 |             'unique_videos': self.merged_data['video_id'].nunique(),
259 |             'train_users': len(self.train_users),
260 |             'val_users': len(self.val_users),
261 |             'click_rate': self.merged_data['is_click'].mean(),
262 |             'avg_play_time': self.merged_data['play_time_ms'].mean(),
263 |             'features_used': {
264 |                 'numerical': self.config['feature']['numerical'],
265 |                 'categorical': self.config['feature']['categorical']
266 |             }
267 |         }
268 |         
269 |         return stats
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\data\feature_processor.py

- Extension: .py
- Language: python
- Size: 10644 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
  1 | """
  2 | ç‰¹å¾å¤„ç†å™¨
  3 | è´Ÿè´£ç‰¹å¾çš„é¢„å¤„ç†ã€ç¼–ç å’Œæ ‡å‡†åŒ–
  4 | """
  5 | 
  6 | import pandas as pd
  7 | import numpy as np
  8 | import logging
  9 | from typing import Dict, List, Tuple, Optional
 10 | from sklearn.preprocessing import LabelEncoder, StandardScaler
 11 | import pickle
 12 | import os
 13 | 
 14 | logger = logging.getLogger(__name__)
 15 | 
 16 | class FeatureProcessor:
 17 |     """ç‰¹å¾å¤„ç†å™¨"""
 18 |     
 19 |     def __init__(self, config: Dict):
 20 |         self.config = config
 21 |         self.numerical_features = config['feature']['numerical']
 22 |         self.categorical_features = config['feature']['categorical']
 23 |         
 24 |         # ç¼–ç å™¨å’Œç¼©æ”¾å™¨
 25 |         self.label_encoders = {}
 26 |         self.scaler = StandardScaler()
 27 |         self.categorical_mappings = {}
 28 |         
 29 |         # å¤„ç†åçš„ç‰¹å¾ç»´åº¦ä¿¡æ¯
 30 |         self.total_numerical_dim = 0
 31 |         self.categorical_dims = {}
 32 |         self.total_categorical_dim = 0
 33 |         
 34 |     def _handle_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:
 35 |         """å¤„ç†ç¼ºå¤±å€¼"""
 36 |         logger.info("[ç‰¹å¾å¤„ç†] å¤„ç†ç¼ºå¤±å€¼...")
 37 |         
 38 |         processed_data = data.copy()
 39 |         
 40 |         # æ•°å€¼ç‰¹å¾ï¼šç”¨0å¡«å……
 41 |         for feature in self.numerical_features:
 42 |             if feature in processed_data.columns:
 43 |                 missing_count = processed_data[feature].isna().sum()
 44 |                 if missing_count > 0:
 45 |                     logger.info(f"[ç‰¹å¾å¤„ç†] {feature}: ç”¨0å¡«å…… {missing_count} ä¸ªç¼ºå¤±å€¼")
 46 |                     processed_data[feature] = processed_data[feature].fillna(0)
 47 |         
 48 |         # åˆ†ç±»ç‰¹å¾ï¼šå°†NAä½œä¸ºæ–°ç±»åˆ«
 49 |         for feature in self.categorical_features:
 50 |             if feature in processed_data.columns:
 51 |                 missing_count = processed_data[feature].isna().sum()
 52 |                 if missing_count > 0:
 53 |                     logger.info(f"[ç‰¹å¾å¤„ç†] {feature}: å°† {missing_count} ä¸ªç¼ºå¤±å€¼æ ‡è®°ä¸º'MISSING'")
 54 |                     processed_data[feature] = processed_data[feature].fillna('MISSING')
 55 |         
 56 |         logger.info("[ç‰¹å¾å¤„ç†] ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")
 57 |         return processed_data
 58 |     
 59 |     def _process_categorical_features(self, data: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
 60 |         """å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Œè½¬æ¢ä¸ºone-hotç¼–ç """
 61 |         logger.info("[ç‰¹å¾å¤„ç†] å¤„ç†åˆ†ç±»ç‰¹å¾...")
 62 |         
 63 |         processed_data = data.copy()
 64 |         
 65 |         for feature in self.categorical_features:
 66 |             if feature not in processed_data.columns:
 67 |                 logger.warning(f"[ç‰¹å¾å¤„ç†] ç‰¹å¾ {feature} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
 68 |                 continue
 69 |                 
 70 |             if fit:
 71 |                 # è®­ç»ƒé˜¶æ®µï¼šæ‹Ÿåˆç¼–ç å™¨
 72 |                 unique_values = processed_data[feature].unique()
 73 |                 logger.info(f"[ç‰¹å¾å¤„ç†] {feature}: {len(unique_values)} ä¸ªå”¯ä¸€å€¼")
 74 |                 
 75 |                 # åˆ›å»ºone-hotç¼–ç 
 76 |                 one_hot = pd.get_dummies(processed_data[feature], prefix=feature)
 77 |                 self.categorical_mappings[feature] = one_hot.columns.tolist()
 78 |                 self.categorical_dims[feature] = len(one_hot.columns)
 79 |                 
 80 |                 # åˆå¹¶åˆ°ä¸»æ•°æ®æ¡†
 81 |                 processed_data = pd.concat([processed_data, one_hot], axis=1)
 82 |                 processed_data = processed_data.drop(feature, axis=1)
 83 |                 
 84 |                 logger.info(f"[ç‰¹å¾å¤„ç†] {feature} -> {len(one_hot.columns)} ä¸ªone-hotç‰¹å¾")
 85 |             else:
 86 |                 # é¢„æµ‹é˜¶æ®µï¼šä½¿ç”¨å·²æœ‰çš„ç¼–ç å™¨
 87 |                 if feature in self.categorical_mappings:
 88 |                     one_hot = pd.get_dummies(processed_data[feature], prefix=feature)
 89 |                     
 90 |                     # ç¡®ä¿æ‰€æœ‰è®­ç»ƒæ—¶çš„åˆ—éƒ½å­˜åœ¨
 91 |                     for col in self.categorical_mappings[feature]:
 92 |                         if col not in one_hot.columns:
 93 |                             one_hot[col] = 0
 94 |                     
 95 |                     # åªä¿ç•™è®­ç»ƒæ—¶çš„åˆ—
 96 |                     one_hot = one_hot[self.categorical_mappings[feature]]
 97 |                     
 98 |                     # åˆå¹¶åˆ°ä¸»æ•°æ®æ¡†
 99 |                     processed_data = pd.concat([processed_data, one_hot], axis=1)
100 |                     processed_data = processed_data.drop(feature, axis=1)
101 |         
102 |         self.total_categorical_dim = sum(self.categorical_dims.values())
103 |         logger.info(f"[ç‰¹å¾å¤„ç†] åˆ†ç±»ç‰¹å¾å¤„ç†å®Œæˆï¼Œæ€»ç»´åº¦: {self.total_categorical_dim}")
104 |         
105 |         return processed_data
106 |     
107 |     def _process_numerical_features(self, data: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
108 |         """å¤„ç†æ•°å€¼ç‰¹å¾ï¼Œè¿›è¡Œæ ‡å‡†åŒ–"""
109 |         logger.info("[ç‰¹å¾å¤„ç†] å¤„ç†æ•°å€¼ç‰¹å¾...")
110 |         
111 |         processed_data = data.copy()
112 |         
113 |         # æå–æ•°å€¼ç‰¹å¾
114 |         available_numerical = [f for f in self.numerical_features if f in processed_data.columns]
115 |         missing_numerical = [f for f in self.numerical_features if f not in processed_data.columns]
116 |         
117 |         if missing_numerical:
118 |             logger.warning(f"[ç‰¹å¾å¤„ç†] ç¼ºå¤±çš„æ•°å€¼ç‰¹å¾: {missing_numerical}")
119 |             # åªä½¿ç”¨å¯ç”¨çš„æ•°å€¼ç‰¹å¾
120 |             self.numerical_features = available_numerical
121 |         
122 |         if available_numerical:
123 |             if fit:
124 |                 # è®­ç»ƒé˜¶æ®µï¼šæ‹Ÿåˆæ ‡å‡†åŒ–å™¨
125 |                 self.scaler.fit(processed_data[available_numerical])
126 |                 logger.info(f"[ç‰¹å¾å¤„ç†] æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–å™¨å·²æ‹Ÿåˆï¼Œç‰¹å¾æ•°: {len(available_numerical)}")
127 |             
128 |             # åº”ç”¨æ ‡å‡†åŒ–
129 |             processed_data[available_numerical] = self.scaler.transform(processed_data[available_numerical])
130 |             self.total_numerical_dim = len(available_numerical)
131 |             
132 |             logger.info(f"[ç‰¹å¾å¤„ç†] æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆï¼Œç»´åº¦: {self.total_numerical_dim}")
133 |         else:
134 |             logger.warning("[ç‰¹å¾å¤„ç†] æ²¡æœ‰å¯ç”¨çš„æ•°å€¼ç‰¹å¾")
135 |             self.total_numerical_dim = 0
136 |         
137 |         return processed_data
138 |     
139 |     def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame:
140 |         """æ‹Ÿåˆå¹¶è½¬æ¢ç‰¹å¾ï¼ˆè®­ç»ƒé˜¶æ®µï¼‰"""
141 |         logger.info("[ç‰¹å¾å¤„ç†] å¼€å§‹ç‰¹å¾æ‹Ÿåˆå’Œè½¬æ¢...")
142 |         
143 |         # å¤„ç†ç¼ºå¤±å€¼
144 |         processed_data = self._handle_missing_values(data)
145 |         
146 |         # å¤„ç†åˆ†ç±»ç‰¹å¾
147 |         processed_data = self._process_categorical_features(processed_data, fit=True)
148 |         
149 |         # å¤„ç†æ•°å€¼ç‰¹å¾
150 |         processed_data = self._process_numerical_features(processed_data, fit=True)
151 |         
152 |         total_dim = self.total_numerical_dim + self.total_categorical_dim
153 |         logger.info(f"[ç‰¹å¾å¤„ç†] ç‰¹å¾å¤„ç†å®Œæˆï¼Œæ€»ç»´åº¦: {total_dim} (æ•°å€¼: {self.total_numerical_dim}, åˆ†ç±»: {self.total_categorical_dim})")
154 |         
155 |         # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
156 |         feature_columns = self.get_feature_columns()
157 |         for col in feature_columns:
158 |             if col in processed_data.columns:
159 |                 processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce').fillna(0)
160 |         
161 |         logger.info("[ç‰¹å¾å¤„ç†] æ•°æ®ç±»å‹è½¬æ¢å®Œæˆ")
162 |         return processed_data
163 |     
164 |     def transform(self, data: pd.DataFrame) -> pd.DataFrame:
165 |         """è½¬æ¢ç‰¹å¾ï¼ˆé¢„æµ‹é˜¶æ®µï¼‰"""
166 |         logger.info("[ç‰¹å¾å¤„ç†] åº”ç”¨å·²æ‹Ÿåˆçš„ç‰¹å¾è½¬æ¢...")
167 |         
168 |         # å¤„ç†ç¼ºå¤±å€¼
169 |         processed_data = self._handle_missing_values(data)
170 |         
171 |         # å¤„ç†åˆ†ç±»ç‰¹å¾
172 |         processed_data = self._process_categorical_features(processed_data, fit=False)
173 |         
174 |         # å¤„ç†æ•°å€¼ç‰¹å¾
175 |         processed_data = self._process_numerical_features(processed_data, fit=False)
176 |         
177 |         # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
178 |         feature_columns = self.get_feature_columns()
179 |         for col in feature_columns:
180 |             if col in processed_data.columns:
181 |                 processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce').fillna(0)
182 |         
183 |         logger.info("[ç‰¹å¾å¤„ç†] ç‰¹å¾è½¬æ¢å®Œæˆ")
184 |         return processed_data
185 |     
186 |     def get_feature_columns(self) -> List[str]:
187 |         """è·å–å¤„ç†åçš„ç‰¹å¾åˆ—å"""
188 |         feature_columns = []
189 |         
190 |         # æ•°å€¼ç‰¹å¾åˆ—
191 |         available_numerical = [f for f in self.numerical_features]
192 |         feature_columns.extend(available_numerical)
193 |         
194 |         # åˆ†ç±»ç‰¹å¾çš„one-hotåˆ—
195 |         for feature in self.categorical_features:
196 |             if feature in self.categorical_mappings:
197 |                 feature_columns.extend(self.categorical_mappings[feature])
198 |         
199 |         return feature_columns
200 |     
201 |     def save_processors(self, save_dir: str) -> None:
202 |         """ä¿å­˜ç‰¹å¾å¤„ç†å™¨"""
203 |         os.makedirs(save_dir, exist_ok=True)
204 |         
205 |         # ä¿å­˜æ ‡å‡†åŒ–å™¨
206 |         with open(os.path.join(save_dir, 'scaler.pkl'), 'wb') as f:
207 |             pickle.dump(self.scaler, f)
208 |         
209 |         # ä¿å­˜åˆ†ç±»ç‰¹å¾æ˜ å°„
210 |         with open(os.path.join(save_dir, 'categorical_mappings.pkl'), 'wb') as f:
211 |             pickle.dump(self.categorical_mappings, f)
212 |         
213 |         # ä¿å­˜ç»´åº¦ä¿¡æ¯
214 |         dim_info = {
215 |             'total_numerical_dim': self.total_numerical_dim,
216 |             'categorical_dims': self.categorical_dims,
217 |             'total_categorical_dim': self.total_categorical_dim
218 |         }
219 |         with open(os.path.join(save_dir, 'dim_info.pkl'), 'wb') as f:
220 |             pickle.dump(dim_info, f)
221 |             
222 |         logger.info(f"[ç‰¹å¾å¤„ç†] å¤„ç†å™¨å·²ä¿å­˜åˆ°: {save_dir}")
223 |     
224 |     def load_processors(self, save_dir: str) -> None:
225 |         """åŠ è½½ç‰¹å¾å¤„ç†å™¨"""
226 |         # åŠ è½½æ ‡å‡†åŒ–å™¨
227 |         with open(os.path.join(save_dir, 'scaler.pkl'), 'rb') as f:
228 |             self.scaler = pickle.load(f)
229 |         
230 |         # åŠ è½½åˆ†ç±»ç‰¹å¾æ˜ å°„
231 |         with open(os.path.join(save_dir, 'categorical_mappings.pkl'), 'rb') as f:
232 |             self.categorical_mappings = pickle.load(f)
233 |         
234 |         # åŠ è½½ç»´åº¦ä¿¡æ¯
235 |         with open(os.path.join(save_dir, 'dim_info.pkl'), 'rb') as f:
236 |             dim_info = pickle.load(f)
237 |             self.total_numerical_dim = dim_info['total_numerical_dim']
238 |             self.categorical_dims = dim_info['categorical_dims']
239 |             self.total_categorical_dim = dim_info['total_categorical_dim']
240 |             
241 |         logger.info(f"[ç‰¹å¾å¤„ç†] å¤„ç†å™¨å·²ä» {save_dir} åŠ è½½")
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\data\__init__.py

- Extension: .py
- Language: python
- Size: 255 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
1 | """
2 | æ•°æ®åŠ è½½ã€å¤„ç†ä¸ç®¡ç†æ¨¡å—
3 | """
4 | 
5 | from .data_loader import KuaiRandDataLoader
6 | from .feature_processor import FeatureProcessor
7 | from .cache_manager import CacheManager
8 | 
9 | __all__ = ['KuaiRandDataLoader', 'FeatureProcessor', 'CacheManager']
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\models\loss_functions.py

- Extension: .py
- Language: python
- Size: 1692 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
 1 | """
 2 | æŸå¤±å‡½æ•°å®šä¹‰
 3 | åŒ…å«LogMAEå’Œå…¶ä»–è‡ªå®šä¹‰æŸå¤±å‡½æ•°
 4 | """
 5 | 
 6 | import torch
 7 | import torch.nn as nn
 8 | import torch.nn.functional as F
 9 | 
10 | class LogMAELoss(nn.Module):
11 |     """Log Mean Absolute ErroræŸå¤±å‡½æ•°
12 |     ç”¨äºæ’­æ”¾æ—¶é•¿ç­‰å…·æœ‰å¤§æ•°å€¼èŒƒå›´çš„è¿ç»­æ ‡ç­¾
13 |     """
14 |     
15 |     def __init__(self, epsilon: float = 1e-8):
16 |         super(LogMAELoss, self).__init__()
17 |         self.epsilon = epsilon
18 |     
19 |     def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
20 |         """
21 |         è®¡ç®—LogMAEæŸå¤±
22 |         
23 |         Args:
24 |             pred: é¢„æµ‹å€¼ [batch_size, 1]
25 |             target: çœŸå®å€¼ [batch_size, 1]
26 |         
27 |         Returns:
28 |             loss: LogMAEæŸå¤±å€¼
29 |         """
30 |         # ç¡®ä¿é¢„æµ‹å€¼å’Œç›®æ ‡å€¼éƒ½æ˜¯æ­£æ•°
31 |         pred = torch.clamp(pred, min=self.epsilon)
32 |         target = torch.clamp(target, min=self.epsilon)
33 |         
34 |         # è®¡ç®—logåçš„MAE
35 |         log_pred = torch.log(pred + self.epsilon)
36 |         log_target = torch.log(target + self.epsilon)
37 |         
38 |         loss = F.l1_loss(log_pred, log_target)
39 |         return loss
40 | 
41 | def get_loss_function(loss_name: str, **kwargs):
42 |     """è·å–æŸå¤±å‡½æ•°"""
43 |     if loss_name.lower() == 'logmae':
44 |         return LogMAELoss(**kwargs)
45 |     elif loss_name.lower() == 'bce':
46 |         return nn.BCEWithLogitsLoss(**kwargs)
47 |     elif loss_name.lower() == 'mse':
48 |         return nn.MSELoss(**kwargs)
49 |     elif loss_name.lower() == 'mae':
50 |         return nn.L1Loss(**kwargs)
51 |     elif loss_name.lower() == 'crossentropy':
52 |         return nn.CrossEntropyLoss(**kwargs)
53 |     else:
54 |         raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {loss_name}")
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\models\mlp_model.py

- Extension: .py
- Language: python
- Size: 2088 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
 1 | """
 2 | MLPæ¨¡å‹å®ç°
 3 | ç”¨äºå•ä¸ªæ ‡ç­¾çš„é¢„æµ‹
 4 | """
 5 | 
 6 | import torch
 7 | import torch.nn as nn
 8 | import torch.nn.functional as F
 9 | from typing import List
10 | 
11 | class MLPModel(nn.Module):
12 |     """å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹"""
13 |     
14 |     def __init__(self, input_dim: int, hidden_layers: List[int], 
15 |                  output_dim: int = 1, dropout: float = 0.1):
16 |         super(MLPModel, self).__init__()
17 |         
18 |         self.input_dim = input_dim
19 |         self.hidden_layers = hidden_layers
20 |         self.output_dim = output_dim
21 |         self.dropout = dropout
22 |         
23 |         # æ„å»ºç½‘ç»œå±‚
24 |         layers = []
25 |         prev_dim = input_dim
26 |         
27 |         # éšè—å±‚
28 |         for hidden_dim in hidden_layers:
29 |             layers.append(nn.Linear(prev_dim, hidden_dim))
30 |             layers.append(nn.ReLU())
31 |             layers.append(nn.Dropout(dropout))
32 |             prev_dim = hidden_dim
33 |         
34 |         # è¾“å‡ºå±‚
35 |         layers.append(nn.Linear(prev_dim, output_dim))
36 |         
37 |         self.network = nn.Sequential(*layers)
38 |         
39 |         # æƒé‡åˆå§‹åŒ–
40 |         self._initialize_weights()
41 |     
42 |     def _initialize_weights(self):
43 |         """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
44 |         for module in self.modules():
45 |             if isinstance(module, nn.Linear):
46 |                 nn.init.xavier_uniform_(module.weight)
47 |                 if module.bias is not None:
48 |                     nn.init.zeros_(module.bias)
49 |     
50 |     def forward(self, x: torch.Tensor) -> torch.Tensor:
51 |         """å‰å‘ä¼ æ’­"""
52 |         return self.network(x)
53 |     
54 |     def get_model_info(self) -> dict:
55 |         """è·å–æ¨¡å‹ä¿¡æ¯"""
56 |         total_params = sum(p.numel() for p in self.parameters())
57 |         trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
58 |         
59 |         return {
60 |             'input_dim': self.input_dim,
61 |             'hidden_layers': self.hidden_layers,
62 |             'output_dim': self.output_dim,
63 |             'dropout': self.dropout,
64 |             'total_params': total_params,
65 |             'trainable_params': trainable_params
66 |         }
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\models\multi_label_model.py

- Extension: .py
- Language: python
- Size: 8903 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-18 01:38:44

### Code

```python
  1 | """
  2 | å¤šæ ‡ç­¾é¢„æµ‹æ¨¡å‹
  3 | ç®¡ç†å¤šä¸ªç‹¬ç«‹çš„é¢„æµ‹æ¨¡å‹
  4 | """
  5 | 
  6 | import torch
  7 | import torch.nn as nn
  8 | import torch.optim as optim
  9 | import logging
 10 | from typing import Dict, List, Tuple, Any
 11 | from .mlp_model import MLPModel
 12 | from .loss_functions import get_loss_function
 13 | 
 14 | logger = logging.getLogger(__name__)
 15 | 
 16 | class MultiLabelModel:
 17 |     """å¤šæ ‡ç­¾é¢„æµ‹æ¨¡å‹ç®¡ç†å™¨"""
 18 |     
 19 |     def __init__(self, config: Dict, input_dim: int, device: torch.device):
 20 |         self.config = config
 21 |         self.input_dim = input_dim
 22 |         self.device = device
 23 |         self.labels = config['labels']
 24 |         
 25 |         # ä¸ºæ¯ä¸ªæ ‡ç­¾åˆ›å»ºç‹¬ç«‹çš„æ¨¡å‹
 26 |         self.models = {}
 27 |         self.optimizers = {}
 28 |         self.loss_functions = {}
 29 |         self.schedulers = {}
 30 |         
 31 |         self._build_models()
 32 |         
 33 |     def _build_models(self):
 34 |         """æ„å»ºæ‰€æœ‰æ ‡ç­¾çš„æ¨¡å‹"""
 35 |         logger.info("[æ¨¡å‹æ„å»º] å¼€å§‹æ„å»ºå¤šæ ‡ç­¾é¢„æµ‹æ¨¡å‹...")
 36 |         
 37 |         for label_config in self.labels:
 38 |             label_name = label_config['name']
 39 |             logger.info(f"[æ¨¡å‹æ„å»º] æ„å»º {label_name} æ¨¡å‹...")
 40 |             
 41 |             # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
 42 |             model = MLPModel(
 43 |                 input_dim=self.input_dim,
 44 |                 hidden_layers=label_config['model_params']['hidden_layers'],
 45 |                 output_dim=1,
 46 |                 dropout=label_config['model_params']['dropout']
 47 |             ).to(self.device)
 48 |             
 49 |             # åˆ›å»ºä¼˜åŒ–å™¨
 50 |             optimizer = optim.Adam(
 51 |                 model.parameters(),
 52 |                 lr=label_config['learning_rate'],
 53 |                 weight_decay=label_config['weight_decay']
 54 |             )
 55 |             
 56 |             # åˆ›å»ºæŸå¤±å‡½æ•°
 57 |             loss_fn = get_loss_function(label_config['loss_function'])
 58 |             
 59 |             # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
 60 |             scheduler = optim.lr_scheduler.ReduceLROnPlateau(
 61 |                 optimizer, mode='min', factor=0.5, patience=5
 62 |             )
 63 |             
 64 |             self.models[label_name] = model
 65 |             self.optimizers[label_name] = optimizer
 66 |             self.loss_functions[label_name] = loss_fn
 67 |             self.schedulers[label_name] = scheduler
 68 |             
 69 |             # æ‰“å°æ¨¡å‹ä¿¡æ¯
 70 |             model_info = model.get_model_info()
 71 |             logger.info(f"[æ¨¡å‹æ„å»º] {label_name} æ¨¡å‹: {model_info['total_params']} å‚æ•°")
 72 |         
 73 |         logger.info(f"[æ¨¡å‹æ„å»º] å¤šæ ‡ç­¾æ¨¡å‹æ„å»ºå®Œæˆï¼Œå…± {len(self.models)} ä¸ªæ¨¡å‹")
 74 |     
 75 |     def forward(self, x: torch.Tensor, label_name: str) -> torch.Tensor:
 76 |         """å•ä¸ªæ ‡ç­¾çš„å‰å‘ä¼ æ’­"""
 77 |         if label_name not in self.models:
 78 |             raise ValueError(f"æ ‡ç­¾ {label_name} çš„æ¨¡å‹ä¸å­˜åœ¨")
 79 |         
 80 |         return self.models[label_name](x)
 81 | 
 82 |     def set_train_mode(self):
 83 |         """å°†æ‰€æœ‰æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
 84 |         for model in self.models.values():
 85 |             model.train()
 86 | 
 87 |     def set_eval_mode(self):
 88 |         """å°†æ‰€æœ‰æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
 89 |         for model in self.models.values():
 90 |             model.eval()
 91 |     
 92 |     def predict_all(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
 93 |         """é¢„æµ‹æ‰€æœ‰æ ‡ç­¾"""
 94 |         self.set_eval_mode()
 95 |         predictions = {}
 96 |         
 97 |         with torch.no_grad():
 98 |             for label_name in self.models:
 99 |                 pred = self.forward(x, label_name)
100 |                 
101 |                 # æ ¹æ®æ ‡ç­¾ç±»å‹å¤„ç†è¾“å‡º
102 |                 label_config = next(lc for lc in self.labels if lc['name'] == label_name)
103 |                 if label_config['type'] == 'binary':
104 |                     pred = torch.sigmoid(pred)
105 |                 elif label_config['type'] == 'numerical':
106 |                     pred = torch.clamp(pred, min=0)  # ç¡®ä¿éè´Ÿ
107 |                 
108 |                 predictions[label_name] = pred
109 |         
110 |         return predictions
111 |     
112 |     def predict(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
113 |         """é¢„æµ‹æ–¹æ³• - ä¸predict_allç›¸åŒï¼Œä¿æŒæ¥å£å…¼å®¹æ€§"""
114 |         return self.predict_all(x)
115 |     
116 |     def compute_losses(self, x: torch.Tensor, targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
117 |         """è®¡ç®—æ‰€æœ‰æ ‡ç­¾çš„æŸå¤±"""
118 |         losses = {}
119 |         
120 |         for label_name in self.models:
121 |             if label_name in targets:
122 |                 pred = self.forward(x, label_name)
123 |                 target = targets[label_name]
124 |                 loss = self.loss_functions[label_name](pred, target)
125 |                 losses[label_name] = loss
126 |         
127 |         return losses
128 |     
129 |     def train_step(self, x: torch.Tensor, targets: Dict[str, torch.Tensor]) -> Dict[str, float]:
130 |         """è®­ç»ƒæ­¥éª¤"""
131 |         # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
132 |         self.set_train_mode()
133 |         
134 |         # è®¡ç®—æŸå¤±å¹¶æ›´æ–°æ¨¡å‹
135 |         losses = {}
136 |         for label_name in self.models:
137 |             if label_name in targets:
138 |                 # æ¸…é›¶æ¢¯åº¦
139 |                 self.optimizers[label_name].zero_grad()
140 |                 
141 |                 # å‰å‘ä¼ æ’­
142 |                 pred = self.forward(x, label_name)
143 |                 target = targets[label_name]
144 |                 
145 |                 # è®¡ç®—æŸå¤±
146 |                 loss = self.loss_functions[label_name](pred, target)
147 |                 
148 |                 # åå‘ä¼ æ’­
149 |                 loss.backward()
150 |                 
151 |                 # æ¢¯åº¦è£å‰ª
152 |                 torch.nn.utils.clip_grad_norm_(self.models[label_name].parameters(), max_norm=1.0)
153 |                 
154 |                 # æ›´æ–°å‚æ•°
155 |                 self.optimizers[label_name].step()
156 |                 
157 |                 losses[label_name] = loss.item()
158 |         
159 |         return losses
160 |     
161 |     def evaluate(self, x: torch.Tensor, targets: Dict[str, torch.Tensor]) -> Dict[str, float]:
162 |         """è¯„ä¼°æ¨¡å‹ï¼Œè¿”å›æ¯ä¸ªæ ‡ç­¾çš„æŸå¤±å€¼"""
163 |         self.set_eval_mode()
164 |         with torch.no_grad():
165 |             losses = self.compute_losses(x, targets)
166 |             return {name: loss.item() for name, loss in losses.items()}
167 |     
168 |     def get_combined_score(self, x: torch.Tensor, alpha_weights: Dict[str, float]) -> torch.Tensor:
169 |         """æ ¹æ®alphaæƒé‡è®¡ç®—ç»„åˆåˆ†æ•°"""
170 |         predictions = self.predict_all(x)
171 |         
172 |         combined_score = torch.zeros(x.size(0), 1, device=self.device)
173 |         
174 |         for label_name, alpha in alpha_weights.items():
175 |             if label_name in predictions:
176 |                 pred = predictions[label_name]
177 |                 combined_score += alpha * pred
178 |         
179 |         return combined_score
180 |     
181 |     def save_models(self, save_dir: str, step_or_epoch_name):
182 |         """ä¿å­˜æ‰€æœ‰æ¨¡å‹ï¼Œæ”¯æŒæ­¥éª¤æ•°å­—æˆ–epochåç§°"""
183 |         import os
184 |         os.makedirs(save_dir, exist_ok=True)
185 |         
186 |         checkpoint = {
187 |             'step_or_epoch': step_or_epoch_name,
188 |             'config': self.config,
189 |             'input_dim': self.input_dim
190 |         }
191 |         
192 |         for label_name in self.models:
193 |             checkpoint[f'{label_name}_model'] = self.models[label_name].state_dict()
194 |             checkpoint[f'{label_name}_optimizer'] = self.optimizers[label_name].state_dict()
195 |             checkpoint[f'{label_name}_scheduler'] = self.schedulers[label_name].state_dict()
196 |         
197 |         # æ ¹æ®å‚æ•°ç±»å‹å†³å®šæ–‡ä»¶å
198 |         if isinstance(step_or_epoch_name, int):
199 |             save_path = os.path.join(save_dir, f'step_{step_or_epoch_name}.pt')
200 |         else:
201 |             save_path = os.path.join(save_dir, f'{step_or_epoch_name}.pt')
202 |         
203 |         torch.save(checkpoint, save_path)
204 |         logger.info(f"[æ¨¡å‹ä¿å­˜] æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
205 |     
206 |     def load_models(self, checkpoint_path: str):
207 |         """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
208 |         checkpoint = torch.load(checkpoint_path, map_location=self.device)
209 |         
210 |         for label_name in self.models:
211 |             if f'{label_name}_model' in checkpoint:
212 |                 self.models[label_name].load_state_dict(checkpoint[f'{label_name}_model'])
213 |             if f'{label_name}_optimizer' in checkpoint:
214 |                 self.optimizers[label_name].load_state_dict(checkpoint[f'{label_name}_optimizer'])
215 |             if f'{label_name}_scheduler' in checkpoint:
216 |                 self.schedulers[label_name].load_state_dict(checkpoint[f'{label_name}_scheduler'])
217 |         
218 |         logger.info(f"[æ¨¡å‹åŠ è½½] æ¨¡å‹å·²ä» {checkpoint_path} åŠ è½½")
219 |         return checkpoint.get('step_or_epoch', checkpoint.get('step', 0))
220 |     
221 |     def update_schedulers(self, metrics: Dict[str, float]):
222 |         """æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨"""
223 |         for label_name, metric in metrics.items():
224 |             if label_name in self.schedulers:
225 |                 self.schedulers[label_name].step(metric)
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\models\__init__.py

- Extension: .py
- Language: python
- Size: 288 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
 1 | """
 2 | æ¨¡å‹æ¨¡å—
 3 | åŒ…å«å¤šæ ‡ç­¾é¢„æµ‹æ¨¡å‹å’ŒæŸå¤±å‡½æ•°
 4 | """
 5 | 
 6 | from .mlp_model import MLPModel
 7 | from .multi_label_model import MultiLabelModel
 8 | from .loss_functions import LogMAELoss, get_loss_function
 9 | 
10 | __all__ = ['MLPModel', 'MultiLabelModel', 'LogMAELoss', 'get_loss_function']
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\modes\global_mode.py

- Extension: .py
- Language: python
- Size: 19966 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-17 11:40:57

### Code

```python
  1 | """
  2 | Globalæ¨¡å¼å®ç°
  3 | è®¡ç®—çœŸå®GTEçš„æ ¸å¿ƒæ¨¡å—ï¼ˆGTä¸GCå¯¹ç§°è¿è¡Œï¼‰
  4 | """
  5 | 
  6 | import os
  7 | import torch
  8 | import torch.utils.data as data_utils
  9 | import pandas as pd
 10 | import numpy as np
 11 | import logging
 12 | from typing import Dict, List, Tuple, Any
 13 | from ..data import KuaiRandDataLoader, FeatureProcessor
 14 | from ..models import MultiLabelModel
 15 | from ..utils import MetricsTracker, save_results, get_device_and_amp_helpers
 16 | import random
 17 | 
 18 | logger = logging.getLogger(__name__)
 19 | 
 20 | class GlobalMode:
 21 |     """Globalæ¨¡å¼å®éªŒç®¡ç†å™¨"""
 22 |     
 23 |     def __init__(self, config: Dict, exp_dir: str, device_choice: str = 'auto'):
 24 |         self.config = config
 25 |         self.exp_dir = exp_dir
 26 | 
 27 |         # ä½¿ç”¨æ–°çš„è®¾å¤‡é€‰æ‹©è¾…åŠ©å‡½æ•°
 28 |         self.device, self.autocast, GradScalerClass = get_device_and_amp_helpers(device_choice)
 29 |         
 30 |         # åˆå§‹åŒ–ç»„ä»¶
 31 |         self.data_loader = KuaiRandDataLoader(config)
 32 |         self.feature_processor = FeatureProcessor(config)
 33 |         self.multi_label_model = None
 34 |         
 35 |         # æ•°æ®å­˜å‚¨
 36 |         self.merged_data = None
 37 |         self.user_video_lists = None
 38 |         self.train_users = None
 39 |         self.val_users = None
 40 |         self.processed_data = None
 41 |         
 42 |         # ä»¿çœŸçŠ¶æ€
 43 |         self.used_videos = set()  # è®°å½•å·²ä½¿ç”¨çš„è§†é¢‘
 44 |         
 45 |         # æŒ‡æ ‡è·Ÿè¸ª
 46 |         self.metrics_tracker = MetricsTracker()
 47 |         
 48 |         # ä»¿çœŸç»“æœ
 49 |         self.total_label_T = {label['name']: 0.0 for label in config['labels']}  # Treatmentç»„ç´¯è®¡æ”¶ç›Š
 50 |         self.total_label_C = {label['name']: 0.0 for label in config['labels']}  # Controlç»„ç´¯è®¡æ”¶ç›Š
 51 |         
 52 |         logger.info(f"[Globalæ¨¡å¼] åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
 53 |     
 54 |     def ensure_float_data(self, data: pd.DataFrame, columns: List[str]) -> np.ndarray:
 55 |         """ç¡®ä¿æ•°æ®ä¸ºfloatç±»å‹å¹¶è½¬æ¢ä¸ºnumpyæ•°ç»„"""
 56 |         try:
 57 |             # æå–æŒ‡å®šåˆ—
 58 |             subset = data[columns].copy()
 59 |             
 60 |             # é€åˆ—å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
 61 |             for col in columns:
 62 |                 if col in subset.columns:
 63 |                     # å…ˆå°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
 64 |                     subset[col] = pd.to_numeric(subset[col], errors='coerce')
 65 |                     # å¡«å……NaNå€¼
 66 |                     subset[col] = subset[col].fillna(0.0)
 67 |                     # ç¡®ä¿æ˜¯floatç±»å‹
 68 |                     subset[col] = subset[col].astype(np.float32)
 69 |             
 70 |             # è½¬æ¢ä¸ºnumpyæ•°ç»„
 71 |             array = subset.values.astype(np.float32)
 72 |             
 73 |             # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éæ•°å€¼ç±»å‹
 74 |             if array.dtype == np.object_:
 75 |                 logger.error(f"[æ•°æ®è½¬æ¢] æ•°æ®ä¸­ä»æœ‰éæ•°å€¼ç±»å‹ï¼Œåˆ—: {columns}")
 76 |                 # å¼ºåˆ¶è½¬æ¢æ¯ä¸ªå…ƒç´ 
 77 |                 array = np.array([[float(x) if pd.notna(x) and str(x).replace('.','').replace('-','').isdigit() else 0.0 
 78 |                                  for x in row] for row in array], dtype=np.float32)
 79 |             
 80 |             return array
 81 |             
 82 |         except Exception as e:
 83 |             logger.error(f"[æ•°æ®è½¬æ¢] è½¬æ¢å¤±è´¥: {e}")
 84 |             # åˆ›å»ºé›¶çŸ©é˜µä½œä¸ºå¤‡é€‰
 85 |             return np.zeros((len(data), len(columns)), dtype=np.float32)
 86 |     
 87 |     def load_and_prepare_data(self):
 88 |         """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
 89 |         logger.info("[Globalæ¨¡å¼] å¼€å§‹æ•°æ®åŠ è½½å’Œå‡†å¤‡...")
 90 |         
 91 |         # åŠ è½½åŸå§‹æ•°æ®
 92 |         self.merged_data, self.user_video_lists, self.train_users, self.val_users = \
 93 |             self.data_loader.load_and_prepare_data()
 94 |         
 95 |         # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
 96 |         stats = self.data_loader.get_dataset_stats()
 97 |         logger.info(f"[æ•°æ®ç»Ÿè®¡] æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
 98 |         logger.info(f"[æ•°æ®ç»Ÿè®¡] å”¯ä¸€ç”¨æˆ·æ•°: {stats['unique_users']}")
 99 |         logger.info(f"[æ•°æ®ç»Ÿè®¡] å”¯ä¸€è§†é¢‘æ•°: {stats['unique_videos']}")
100 |         logger.info(f"[æ•°æ®ç»Ÿè®¡] è®­ç»ƒç”¨æˆ·æ•°: {stats['train_users']}")
101 |         logger.info(f"[æ•°æ®ç»Ÿè®¡] éªŒè¯ç”¨æˆ·æ•°: {stats['val_users']}")
102 |         logger.info(f"[æ•°æ®ç»Ÿè®¡] ç‚¹å‡»ç‡: {stats['click_rate']:.4f}")
103 |         logger.info(f"[æ•°æ®ç»Ÿè®¡] å¹³å‡æ’­æ”¾æ—¶é•¿: {stats['avg_play_time']:.2f}ms")
104 |         
105 |         # ç‰¹å¾å¤„ç†
106 |         logger.info("[ç‰¹å¾å¤„ç†] å¼€å§‹ç‰¹å¾é¢„å¤„ç†...")
107 |         self.processed_data = self.feature_processor.fit_transform(self.merged_data)
108 |         
109 |         # è·å–ç‰¹å¾åˆ—
110 |         feature_columns = self.feature_processor.get_feature_columns()
111 |         input_dim = len(feature_columns)
112 |         logger.info(f"[ç‰¹å¾å¤„ç†] ç‰¹å¾ç»´åº¦: {input_dim}")
113 |         logger.info(f"[ç‰¹å¾å¤„ç†] ç‰¹å¾åˆ—: {feature_columns}")
114 |         
115 |         # åˆå§‹åŒ–å¤šæ ‡ç­¾æ¨¡å‹
116 |         self.multi_label_model = MultiLabelModel(
117 |             config=self.config,
118 |             input_dim=input_dim,
119 |             device=self.device
120 |         )
121 |         
122 |         logger.info("[Globalæ¨¡å¼] æ•°æ®å‡†å¤‡å®Œæˆ")
123 |     
124 |     def pretrain_models(self):
125 |         """é¢„è®­ç»ƒæ¨¡å‹"""
126 |         if not self.config['pretrain']['enabled']:
127 |             logger.info("[é¢„è®­ç»ƒ] è·³è¿‡é¢„è®­ç»ƒé˜¶æ®µ")
128 |             return
129 |         
130 |         logger.info("[é¢„è®­ç»ƒ] å¼€å§‹é¢„è®­ç»ƒé˜¶æ®µ...")
131 |         
132 |         # å‡†å¤‡è®­ç»ƒæ•°æ®
133 |         train_data = self.processed_data[self.processed_data['mask'] == 0].copy()
134 |         logger.info(f"[é¢„è®­ç»ƒ] è®­ç»ƒæ•°æ®é‡: {len(train_data)}")
135 |         
136 |         # åˆ›å»ºæ•°æ®åŠ è½½å™¨
137 |         batch_size = self.config['pretrain']['batch_size']
138 |         
139 |         for epoch in range(self.config['pretrain']['epochs']):
140 |             logger.info(f"[é¢„è®­ç»ƒ] Epoch {epoch+1}/{self.config['pretrain']['epochs']}")
141 |             
142 |             # éšæœºæ‰“ä¹±æ•°æ®
143 |             train_data_shuffled = train_data.sample(frac=1).reset_index(drop=True)
144 |             
145 |             epoch_losses = {label['name']: [] for label in self.config['labels']}
146 |             
147 |             # æ‰¹æ¬¡è®­ç»ƒ
148 |             for i in range(0, len(train_data_shuffled), batch_size):
149 |                 batch_data = train_data_shuffled[i:i+batch_size]
150 |                 
151 |                 if len(batch_data) == 0:
152 |                     continue
153 |                 
154 |                 # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
155 |                 feature_columns = self.feature_processor.get_feature_columns()
156 |                 
157 |                 # ä½¿ç”¨æ–°çš„æ•°æ®è½¬æ¢å‡½æ•°
158 |                 feature_array = self.ensure_float_data(batch_data, feature_columns)
159 |                 X = torch.FloatTensor(feature_array).to(self.device)
160 |                 
161 |                 targets = {}
162 |                 for label_config in self.config['labels']:
163 |                     label_name = label_config['name']
164 |                     target_col = label_config['target']
165 |                     y = torch.FloatTensor(batch_data[target_col].values).unsqueeze(1).to(self.device)
166 |                     targets[label_name] = y
167 |                 
168 |                 # è®­ç»ƒæ­¥éª¤
169 |                 losses = self.multi_label_model.train_step(X, targets)
170 |                 
171 |                 for label_name, loss in losses.items():
172 |                     epoch_losses[label_name].append(loss)
173 |             
174 |             # è®°å½•epochç»“æœ
175 |             avg_losses = {name: np.mean(losses) for name, losses in epoch_losses.items() if losses}
176 |             loss_str = ", ".join([f"{name}: {loss:.6f}" for name, loss in avg_losses.items()])
177 |             logger.info(f"[é¢„è®­ç»ƒ] Epoch {epoch+1} å¹³å‡æŸå¤± - {loss_str}")
178 |         
179 |         logger.info("[é¢„è®­ç»ƒ] é¢„è®­ç»ƒé˜¶æ®µå®Œæˆ")
180 |     
181 |     def sample_candidate_videos(self, user_id: int, n_candidate: int) -> List[int]:
182 |         """ä¸ºç”¨æˆ·é‡‡æ ·å€™é€‰è§†é¢‘"""
183 |         if user_id not in self.user_video_lists:
184 |             return []
185 |         
186 |         # è·å–ç”¨æˆ·çš„æ‰€æœ‰è§†é¢‘
187 |         user_videos = self.user_video_lists[user_id]
188 |         
189 |         # ç­›é€‰å¯ç”¨è§†é¢‘ï¼ˆmask=0 ä¸” used=0ï¼‰
190 |         available_videos = []
191 |         for video_id in user_videos:
192 |             video_data = self.processed_data[
193 |                 (self.processed_data['user_id'] == user_id) & 
194 |                 (self.processed_data['video_id'] == video_id)
195 |             ]
196 |             
197 |             if len(video_data) > 0:
198 |                 mask = video_data.iloc[0]['mask']
199 |                 if mask == 0 and video_id not in self.used_videos:
200 |                     available_videos.append(video_id)
201 |         
202 |         # éšæœºé‡‡æ ·
203 |         if len(available_videos) <= n_candidate:
204 |             return available_videos
205 |         else:
206 |             return random.sample(available_videos, n_candidate)
207 |     
208 |     def get_user_video_features(self, user_id: int, video_ids: List[int]) -> torch.Tensor:
209 |         """è·å–ç”¨æˆ·-è§†é¢‘å¯¹çš„ç‰¹å¾"""
210 |         features_list = []
211 |         feature_columns = self.feature_processor.get_feature_columns()
212 |         
213 |         for video_id in video_ids:
214 |             # è·å–è¯¥ç”¨æˆ·-è§†é¢‘å¯¹çš„ç‰¹å¾
215 |             row = self.processed_data[
216 |                 (self.processed_data['user_id'] == user_id) & 
217 |                 (self.processed_data['video_id'] == video_id)
218 |             ]
219 |             
220 |             if len(row) > 0:
221 |                 # ä½¿ç”¨æ–°çš„æ•°æ®è½¬æ¢å‡½æ•°
222 |                 feature_array = self.ensure_float_data(row, feature_columns)
223 |                 if len(feature_array) > 0:
224 |                     features_list.append(feature_array[0])
225 |         
226 |         if features_list:
227 |             feature_array = np.array(features_list, dtype=np.float32)
228 |             return torch.FloatTensor(feature_array).to(self.device)
229 |         else:
230 |             return torch.empty(0, len(feature_columns)).to(self.device)
231 |     
232 |     def get_real_labels(self, user_id: int, video_ids: List[int]) -> Dict[str, torch.Tensor]:
233 |         """è·å–çœŸå®æ ‡ç­¾"""
234 |         labels = {label['name']: [] for label in self.config['labels']}
235 |         
236 |         for video_id in video_ids:
237 |             # è·å–çœŸå®æ ‡ç­¾
238 |             row = self.merged_data[
239 |                 (self.merged_data['user_id'] == user_id) & 
240 |                 (self.merged_data['video_id'] == video_id)
241 |             ]
242 |             
243 |             if len(row) > 0:
244 |                 for label_config in self.config['labels']:
245 |                     label_name = label_config['name']
246 |                     target_col = label_config['target']
247 |                     label_value = row[target_col].values[0]
248 |                     labels[label_name].append(label_value)
249 |         
250 |         # è½¬æ¢ä¸ºtensor
251 |         result = {}
252 |         for label_name, values in labels.items():
253 |             if values:
254 |                 result[label_name] = torch.FloatTensor(values).unsqueeze(1).to(self.device)
255 |         
256 |         return result
257 |     
258 |     def run_single_simulation(self, is_treatment: bool, step: int, batch_users: List[int]) -> Dict[str, float]:
259 |         """è¿è¡Œå•æ¬¡ä»¿çœŸæ­¥éª¤"""
260 |         prefix = "Treatment" if is_treatment else "Control"
261 |         logger.info(f"[{prefix}ä»¿çœŸ] Step {step}: å¼€å§‹å¤„ç† {len(batch_users)} ä¸ªç”¨æˆ·")
262 |         
263 |         step_rewards = {label['name']: 0.0 for label in self.config['labels']}
264 |         processed_users = 0
265 |         
266 |         for user_id in batch_users:
267 |             # 1. å€™é€‰è§†é¢‘ç”Ÿæˆ
268 |             candidates = self.sample_candidate_videos(user_id, self.config['global']['n_candidate'])
269 |             
270 |             if len(candidates) == 0:
271 |                 continue  # è¯¥ç”¨æˆ·æ²¡æœ‰å¯ç”¨è§†é¢‘
272 |             
273 |             # 2. è·å–ç‰¹å¾
274 |             X = self.get_user_video_features(user_id, candidates)
275 |             
276 |             if X.size(0) == 0:
277 |                 continue  # æ²¡æœ‰æœ‰æ•ˆç‰¹å¾
278 |             
279 |             # 3. æ¨¡å‹é¢„æµ‹ä¸åŠ æƒæ’åº
280 |             alpha_weights = {}
281 |             for label_config in self.config['labels']:
282 |                 label_name = label_config['name']
283 |                 alpha_key = 'alpha_T' if is_treatment else 'alpha_C'
284 |                 alpha_weights[label_name] = label_config[alpha_key]
285 |             
286 |             combined_scores = self.multi_label_model.get_combined_score(X, alpha_weights)
287 |             
288 |             # 4. é€‰å‡ºèƒœå‡ºè§†é¢‘
289 |             winner_idx = torch.argmax(combined_scores.squeeze()).item()
290 |             winner_video = candidates[winner_idx]
291 |             
292 |             # 5. è·å–çœŸå®åé¦ˆ
293 |             real_labels = self.get_real_labels(user_id, [winner_video])
294 |             winner_features = X[winner_idx:winner_idx+1]
295 |             
296 |             # 6. æ¨¡å‹è®­ç»ƒ
297 |             if real_labels:
298 |                 _ = self.multi_label_model.train_step(winner_features, real_labels)
299 |                 
300 |                 # ç´¯åŠ æ”¶ç›Š
301 |                 for label_name, label_tensor in real_labels.items():
302 |                     reward_value = label_tensor.item()
303 |                     step_rewards[label_name] += reward_value
304 |             
305 |             # 7. æ›´æ–°usedçŠ¶æ€
306 |             self.used_videos.add(winner_video)
307 |             
308 |             processed_users += 1
309 |         
310 |         logger.info(f"[{prefix}ä»¿çœŸ] Step {step}: å¤„ç†äº† {processed_users} ä¸ªç”¨æˆ·")
311 |         return step_rewards
312 |     
313 |     def validate_models(self, step: int):
314 |         """éªŒè¯æ¨¡å‹æ€§èƒ½"""
315 |         logger.info(f"[éªŒè¯] Step {step}: å¼€å§‹éªŒè¯")
316 |         
317 |         # ä½¿ç”¨éªŒè¯é›†ç”¨æˆ·
318 |         val_sample_size = min(100, len(self.val_users))  # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡
319 |         val_users_sample = random.sample(self.val_users, val_sample_size)
320 |         
321 |         total_losses = {label['name']: [] for label in self.config['labels']}
322 |         
323 |         for user_id in val_users_sample:
324 |             # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰è§†é¢‘ï¼ˆä¸è€ƒè™‘usedå’Œmaskï¼‰
325 |             user_videos = self.user_video_lists.get(user_id, [])
326 |             
327 |             if len(user_videos) == 0:
328 |                 continue
329 |             
330 |             # éšæœºé€‰æ‹©å‡ ä¸ªè§†é¢‘è¿›è¡ŒéªŒè¯
331 |             sample_videos = random.sample(user_videos, min(5, len(user_videos)))
332 |             
333 |             X = self.get_user_video_features(user_id, sample_videos)
334 |             real_labels = self.get_real_labels(user_id, sample_videos)
335 |             
336 |             if X.size(0) > 0 and real_labels:
337 |                 losses = self.multi_label_model.evaluate(X, real_labels)
338 |                 for label_name, loss in losses.items():
339 |                     total_losses[label_name].append(loss)
340 |         
341 |         # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
342 |         avg_val_losses = {}
343 |         for label_name, losses in total_losses.items():
344 |             if losses:
345 |                 avg_val_losses[f'val_{label_name}_loss'] = np.mean(losses)
346 |         
347 |         self.metrics_tracker.update(avg_val_losses, step)
348 |         self.metrics_tracker.log_current("éªŒè¯")
349 |         
350 |         # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
351 |         self.multi_label_model.update_schedulers(avg_val_losses)
352 |     
353 |     def run_global_simulation(self):
354 |         """è¿è¡ŒGlobalä»¿çœŸ"""
355 |         logger.info("[Globalä»¿çœŸ] å¼€å§‹å…¨å±€ä»¿çœŸæµç¨‹...")
356 |         
357 |         n_steps = self.config['global']['n_steps']
358 |         batch_size = self.config['global']['batch_size']
359 |         validate_every = self.config['global']['validate_every']
360 |         save_every = self.config['global']['save_every']
361 |         
362 |         for step in range(1, n_steps + 1):
363 |             logger.info(f"[Globalä»¿çœŸ] ===== Step {step}/{n_steps} =====")
364 |             
365 |             # 1. ç”¨æˆ·æ‰¹æ¬¡æŠ½æ ·ï¼ˆGTå’ŒGCä½¿ç”¨ç›¸åŒçš„ç”¨æˆ·æ‰¹æ¬¡ï¼‰
366 |             batch_users = random.sample(self.train_users, min(batch_size, len(self.train_users)))
367 |             
368 |             # 2. Treatmentä»¿çœŸï¼ˆGTï¼‰
369 |             logger.info("[GTæµç¨‹] å¼€å§‹Treatmentç»„ä»¿çœŸ...")
370 |             step_rewards_T = self.run_single_simulation(True, step, batch_users)
371 |             
372 |             # ç´¯åŠ åˆ°æ€»æ”¶ç›Š
373 |             for label_name, reward in step_rewards_T.items():
374 |                 self.total_label_T[label_name] += reward
375 |             
376 |             # 3. Controlä»¿çœŸï¼ˆGCï¼‰
377 |             logger.info("[GCæµç¨‹] å¼€å§‹Controlç»„ä»¿çœŸ...")
378 |             step_rewards_C = self.run_single_simulation(False, step, batch_users)
379 |             
380 |             # ç´¯åŠ åˆ°æ€»æ”¶ç›Š
381 |             for label_name, reward in step_rewards_C.items():
382 |                 self.total_label_C[label_name] += reward
383 |             
384 |             # 4. è®°å½•æ­¥éª¤æŒ‡æ ‡
385 |             step_metrics = {}
386 |             for label_name in step_rewards_T:
387 |                 step_metrics[f'step_{label_name}_T'] = step_rewards_T[label_name]
388 |                 step_metrics[f'step_{label_name}_C'] = step_rewards_C[label_name]
389 |                 step_metrics[f'total_{label_name}_T'] = self.total_label_T[label_name]
390 |                 step_metrics[f'total_{label_name}_C'] = self.total_label_C[label_name]
391 |             
392 |             self.metrics_tracker.update(step_metrics, step)
393 |             self.metrics_tracker.log_current(f"è®­ç»ƒ Step {step}")
394 |             
395 |             # 5. éªŒè¯
396 |             if step % validate_every == 0:
397 |                 self.validate_models(step)
398 |             
399 |             # 6. ä¿å­˜æ¨¡å‹
400 |             if step % save_every == 0:
401 |                 checkpoint_dir = os.path.join(self.exp_dir, "checkpoints")
402 |                 self.multi_label_model.save_models(checkpoint_dir, step)
403 |                 
404 |                 # ä¿å­˜ç‰¹å¾å¤„ç†å™¨
405 |                 self.feature_processor.save_processors(checkpoint_dir)
406 |         
407 |         logger.info("[Globalä»¿çœŸ] å…¨å±€ä»¿çœŸå®Œæˆ")
408 |     
409 |     def compute_gte(self) -> Dict[str, float]:
410 |         """è®¡ç®—GTEï¼ˆGlobal Treatment Effectï¼‰"""
411 |         logger.info("[GTEè®¡ç®—] å¼€å§‹è®¡ç®—å…¨å±€å¤„ç†æ•ˆåº”...")
412 |         
413 |         gte_results = {}
414 |         
415 |         for label_name in self.total_label_T:
416 |             gt_total = self.total_label_T[label_name]
417 |             gc_total = self.total_label_C[label_name]
418 |             
419 |             # è®¡ç®—GTE
420 |             gte = gt_total - gc_total
421 |             gte_relative = (gte / gc_total * 100) if gc_total != 0 else 0
422 |             
423 |             gte_results[f'GTE_{label_name}'] = gte
424 |             gte_results[f'GTE_{label_name}_relative'] = gte_relative
425 |             gte_results[f'GT_{label_name}'] = gt_total
426 |             gte_results[f'GC_{label_name}'] = gc_total
427 |             
428 |             logger.info(f"[GTEè®¡ç®—] {label_name}: GT={gt_total:.4f}, GC={gc_total:.4f}, GTE={gte:.4f} ({gte_relative:+.2f}%)")
429 |         
430 |         return gte_results
431 |     
432 |     def run(self):
433 |         """è¿è¡Œå®Œæ•´çš„Globalæ¨¡å¼å®éªŒ"""
434 |         logger.info("[Globalæ¨¡å¼] å¼€å§‹è¿è¡Œå®Œæ•´å®éªŒ...")
435 |         
436 |         try:
437 |             # 1. æ•°æ®åŠ è½½å’Œå‡†å¤‡
438 |             self.load_and_prepare_data()
439 |             
440 |             # 2. é¢„è®­ç»ƒ
441 |             self.pretrain_models()
442 |             
443 |             # 3. å…¨å±€ä»¿çœŸ
444 |             self.run_global_simulation()
445 |             
446 |             # 4. è®¡ç®—GTE
447 |             gte_results = self.compute_gte()
448 |             
449 |             # 5. ä¿å­˜æœ€ç»ˆç»“æœ
450 |             final_results = {
451 |                 'config': self.config,
452 |                 'gte_results': gte_results,
453 |                 'metrics_summary': self.metrics_tracker.get_summary(),
454 |                 'dataset_stats': self.data_loader.get_dataset_stats()
455 |             }
456 |             
457 |             results_path = os.path.join(self.exp_dir, 'result.json')
458 |             save_results(final_results, results_path)
459 |             
460 |             logger.info("[Globalæ¨¡å¼] å®éªŒå®Œæˆï¼")
461 |             
462 |             # æ‰“å°æœ€ç»ˆç»“æœ
463 |             logger.info("========== æœ€ç»ˆå®éªŒç»“æœ ==========")
464 |             for key, value in gte_results.items():
465 |                 logger.info(f"{key}: {value}")
466 |                 
467 |         except Exception as e:
468 |             logger.error(f"[Globalæ¨¡å¼] å®éªŒæ‰§è¡Œå¤±è´¥: {e}")
469 |             raise
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\modes\global_mode_optimized.py

- Extension: .py
- Language: python
- Size: 33813 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-18 01:38:44

### Code

```python
  1 | """
  2 | Globalæ¨¡å¼ä¼˜åŒ–å®ç° - è§£å†³GPUåˆ©ç”¨ç‡ä½ä¸‹é—®é¢˜
  3 | ä¸»è¦ä¼˜åŒ–ï¼š
  4 | 1. ä½¿ç”¨PyTorch DataLoaderè¿›è¡Œå¤šè¿›ç¨‹æ•°æ®åŠ è½½
  5 | 2. å¢åŠ GPUçŠ¶æ€ç›‘æ§å’Œè¯Šæ–­
  6 | 3. ä¼˜åŒ–æ‰¹å¤„ç†å’Œå†…å­˜ä½¿ç”¨
  7 | 4. æ·»åŠ è¯¦ç»†çš„æ€§èƒ½åˆ†æ
  8 | """
  9 | 
 10 | import os
 11 | import torch
 12 | from torch.utils.data import DataLoader, Dataset
 13 | import pandas as pd
 14 | import numpy as np
 15 | import logging
 16 | from typing import Dict, List, Tuple, Any
 17 | from tqdm import tqdm
 18 | import random
 19 | import time
 20 | # æ–°å¢çš„åº“
 21 | import matplotlib.pyplot as plt
 22 | from sklearn.model_selection import train_test_split
 23 | 
 24 | # ä½¿ç”¨æ–°çš„è®¾å¤‡ç®¡ç†å·¥å…·æ›¿ä»£æ—§çš„autocastå¯¼å…¥
 25 | 
 26 | from ..data import KuaiRandDataLoader, FeatureProcessor
 27 | from ..models import MultiLabelModel
 28 | from ..utils import MetricsTracker, save_results, get_device_and_amp_helpers
 29 | from ..utils.gpu_utils import log_gpu_info, log_gpu_memory_usage, test_gpu_training_speed, setup_gpu_monitoring
 30 | 
 31 | logger = logging.getLogger(__name__)
 32 | 
 33 | class TabularDataset(Dataset):
 34 |     """ä¼˜åŒ–çš„è¡¨æ ¼æ•°æ®Datasetï¼Œæ”¯æŒGPUåŠ é€Ÿ"""
 35 |     
 36 |     def __init__(self, features: np.ndarray, labels: Dict[str, np.ndarray], device='cpu'):
 37 |         """
 38 |         Args:
 39 |             features: ç‰¹å¾æ•°ç»„ (N, D)
 40 |             labels: æ ‡ç­¾å­—å…¸ {label_name: array(N,)}
 41 |             device: ç›®æ ‡è®¾å¤‡
 42 |         """
 43 |         self.device = device
 44 |         # é¢„è½¬æ¢ä¸ºtensorä»¥å‡å°‘è¿è¡Œæ—¶å¼€é”€
 45 |         self.features = torch.tensor(features, dtype=torch.float32)
 46 |         self.labels = {}
 47 |         for name, label_array in labels.items():
 48 |             self.labels[name] = torch.tensor(label_array, dtype=torch.float32).unsqueeze(1)
 49 |         
 50 |         logger.info(f"[æ•°æ®é›†] åˆ›å»ºTabularDatasetï¼Œæ ·æœ¬æ•°: {len(self.features)}, ç‰¹å¾ç»´åº¦: {self.features.shape[1]}")
 51 |             
 52 |     def __len__(self):
 53 |         return len(self.features)
 54 |         
 55 |     def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
 56 |         feature_vector = self.features[idx]
 57 |         target_dict = {name: label_tensor[idx] for name, label_tensor in self.labels.items()}
 58 |         return feature_vector, target_dict
 59 | 
 60 | class GlobalModeOptimized:
 61 |     """ä¼˜åŒ–çš„Globalæ¨¡å¼å®éªŒç®¡ç†å™¨"""
 62 |     
 63 |     def __init__(self, config: Dict, exp_dir: str, device_choice: str = 'auto'):
 64 |         self.config = config
 65 |         self.exp_dir = exp_dir
 66 |         
 67 |         # ä½¿ç”¨æ–°çš„è®¾å¤‡é€‰æ‹©è¾…åŠ©å‡½æ•°
 68 |         self.device, self.autocast, GradScalerClass = get_device_and_amp_helpers(device_choice)
 69 | 
 70 |         # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒ
 71 |         self.use_amp = self.device.type != 'cpu' and config.get('use_amp', True)
 72 |         self.scaler = GradScalerClass(enabled=self.use_amp)
 73 |         
 74 |         # --- å…³é”®ä¿®å¤ï¼šä¸ºautocastå‡†å¤‡å…¼å®¹æ€§å‚æ•° ---
 75 |         self.autocast_kwargs = {'enabled': self.use_amp}
 76 |         if self.device.type == 'cuda':
 77 |             self.autocast_kwargs['device_type'] = 'cuda'
 78 |         # For IPEX, we don't add 'device_type'
 79 |         # --- ä¿®å¤ç»“æŸ ---
 80 |         
 81 |         logger.info(f"[Globalæ¨¡å¼ä¼˜åŒ–] åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}, AMP: {self.use_amp}")
 82 |         
 83 |         self.data_loader_wrapper = KuaiRandDataLoader(config)
 84 |         self.feature_processor = FeatureProcessor(config)
 85 |         self.multi_label_model = None
 86 |         self.merged_data = None
 87 |         self.user_video_lists = None
 88 |         self.train_users = None
 89 |         self.val_users = None
 90 |         self.processed_data = None
 91 |         
 92 |         # ç‹¬ç«‹çš„usedè§†é¢‘é›†åˆ
 93 |         self.used_videos_T = set()  # Treatmentç»„
 94 |         self.used_videos_C = set()  # Controlç»„
 95 |         
 96 |         self.metrics_tracker = MetricsTracker()
 97 |         self.total_label_T = {label['name']: 0.0 for label in config['labels']}
 98 |         self.total_label_C = {label['name']: 0.0 for label in config['labels']}
 99 |         
100 |         # æ–°å¢ï¼šç”¨äºå­˜å‚¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡ï¼Œä»¥ä¾¿ç»˜å›¾
101 |         self.pretrain_metrics = []
102 |         
103 |         # GPUç›‘æ§å™¨
104 |         self.gpu_monitor = None
105 |         
106 |     def _perform_training_step(self, X_batch, targets_batch):
107 |         """æ‰§è¡Œä¸€ä¸ªä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤ï¼Œæ”¯æŒAMP"""
108 |         self.multi_label_model.set_train_mode()
109 |         
110 |         # æ¸…é›¶æ‰€æœ‰æ¢¯åº¦ä»¥å¤‡ä¸‹æ¬¡è¿­ä»£
111 |         for optimizer in self.multi_label_model.optimizers.values():
112 |             optimizer.zero_grad(set_to_none=True)
113 | 
114 |         # --- å…³é”®ä¿®å¤ï¼šä½¿ç”¨å‡†å¤‡å¥½çš„å…¼å®¹æ€§å‚æ•° ---
115 |         with self.autocast(**self.autocast_kwargs):
116 |             losses = self.multi_label_model.compute_losses(X_batch, targets_batch)
117 |             total_loss = sum(losses.values())
118 |         # --- ä¿®å¤ç»“æŸ ---
119 | 
120 |         if self.scaler.is_enabled():
121 |             self.scaler.scale(total_loss).backward()
122 |             for optimizer in self.multi_label_model.optimizers.values():
123 |                 self.scaler.step(optimizer)
124 |             self.scaler.update()
125 |         else:
126 |             total_loss.backward()
127 |             for optimizer in self.multi_label_model.optimizers.values():
128 |                 optimizer.step()
129 | 
130 |         return {name: loss.item() for name, loss in losses.items()}
131 |         
132 |     def start_gpu_monitoring(self):
133 |         """å¯åŠ¨GPUç›‘æ§"""
134 |         if torch.cuda.is_available():
135 |             self.gpu_monitor = setup_gpu_monitoring(log_interval=60)  # æ¯åˆ†é’Ÿè®°å½•ä¸€æ¬¡
136 |             
137 |     def stop_gpu_monitoring(self):
138 |         """åœæ­¢GPUç›‘æ§"""
139 |         if self.gpu_monitor:
140 |             self.gpu_monitor.stop_monitoring()
141 |             
142 |     def create_optimized_dataloader(self, data: pd.DataFrame, batch_size: int, shuffle: bool = True) -> DataLoader:
143 |         """åˆ›å»ºä¼˜åŒ–çš„DataLoader"""
144 |         feature_columns = self.feature_processor.get_feature_columns()
145 |         
146 |         # å‡†å¤‡ç‰¹å¾æ•°æ®
147 |         features = data[feature_columns].values.astype(np.float32)
148 |         
149 |         # å‡†å¤‡æ ‡ç­¾æ•°æ®
150 |         labels = {}
151 |         for label_config in self.config['labels']:
152 |             target_col = label_config['target']
153 |             labels[label_config['name']] = data[target_col].values.astype(np.float32)
154 |         
155 |         # åˆ›å»ºDataset
156 |         dataset = TabularDataset(features, labels, self.device)
157 |         
158 |         # DataLoaderå‚æ•°
159 |         num_workers = self.config['dataset'].get('num_workers', 4)
160 |         pin_memory = self.config['dataset'].get('pin_memory', True) and torch.cuda.is_available()
161 |         
162 |         dataloader = DataLoader(
163 |             dataset,
164 |             batch_size=batch_size,
165 |             shuffle=shuffle,
166 |             num_workers=num_workers,
167 |             pin_memory=pin_memory,
168 |             persistent_workers=num_workers > 0  # ä¿æŒworkerè¿›ç¨‹
169 |         )
170 |         
171 |         logger.info(f"[DataLoader] åˆ›å»ºå®Œæˆ - batch_size: {batch_size}, num_workers: {num_workers}, pin_memory: {pin_memory}")
172 |         return dataloader
173 |         
174 |     def load_and_prepare_data(self):
175 |         """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
176 |         logger.info("[Globalæ¨¡å¼ä¼˜åŒ–] å¼€å§‹æ•°æ®åŠ è½½å’Œå‡†å¤‡...")
177 |         
178 |         self.merged_data, self.user_video_lists, self.train_users, self.val_users = \
179 |             self.data_loader_wrapper.load_and_prepare_data()
180 |         
181 |         stats = self.data_loader_wrapper.get_dataset_stats()
182 |         for key, value in stats.items():
183 |             logger.info(f"[æ•°æ®ç»Ÿè®¡] {key}: {value}")
184 |         
185 |         logger.info("[ç‰¹å¾å¤„ç†] å¼€å§‹ç‰¹å¾é¢„å¤„ç†...")
186 |         self.processed_data = self.feature_processor.fit_transform(self.merged_data)
187 |         
188 |         feature_columns = self.feature_processor.get_feature_columns()
189 |         input_dim = len(feature_columns)
190 |         logger.info(f"[ç‰¹å¾å¤„ç†] ç‰¹å¾ç»´åº¦: {input_dim}")
191 |         logger.info(f"[ç‰¹å¾å¤„ç†] ç‰¹å¾åˆ—: {feature_columns}")
192 |         
193 |         self.multi_label_model = MultiLabelModel(
194 |             config=self.config, input_dim=input_dim, device=self.device
195 |         )
196 |         
197 |         # --- æ–°å¢ï¼šåŠ è½½é¢„è®­ç»ƒæƒé‡ ---
198 |         checkpoint_path = self.config['pretrain'].get('load_checkpoint_path')
199 |         if checkpoint_path and os.path.exists(checkpoint_path):
200 |             logger.info(f"[æ¨¡å‹åŠ è½½] å‘ç°é¢„è®­ç»ƒæƒé‡é…ç½®ï¼Œæ­£åœ¨ä» {checkpoint_path} åŠ è½½...")
201 |             try:
202 |                 self.multi_label_model.load_models(checkpoint_path)
203 |                 logger.info(f"[æ¨¡å‹åŠ è½½] æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡")
204 |             except Exception as e:
205 |                 logger.error(f"[æ¨¡å‹åŠ è½½] åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
206 |         elif checkpoint_path:
207 |             logger.warning(f"[æ¨¡å‹åŠ è½½] é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
208 |         # --- ç»“æŸæ–°å¢éƒ¨åˆ† ---
209 |         
210 |         # --- NEW: APPLY IPEX OPTIMIZE ---
211 |         # If we are in IPEX mode, apply the ipex.optimize() function here
212 |         if self.device.type == 'xpu':
213 |             logger.info("[IPEX] Applying ipex.optimize() to all models and optimizers...")
214 |             # Import ipex here, we know it's available because device_utils succeeded
215 |             import intel_extension_for_pytorch as ipex
216 |             
217 |             for label_name in self.multi_label_model.models:
218 |                 model = self.multi_label_model.models[label_name]
219 |                 optimizer = self.multi_label_model.optimizers[label_name]
220 |                 
221 |                 # The core of IPEX optimization. We use bfloat16 for mixed precision.
222 |                 optimized_model, optimized_optimizer = ipex.optimize(
223 |                     model, optimizer=optimizer, dtype=torch.bfloat16
224 |                 )
225 |                 
226 |                 # Replace original models/optimizers with the optimized ones
227 |                 self.multi_label_model.models[label_name] = optimized_model
228 |                 self.multi_label_model.optimizers[label_name] = optimized_optimizer
229 |             logger.info("[IPEX] ipex.optimize() applied successfully.")
230 |         # --- END OF NEW CODE ---
231 |         
232 |         logger.info("[Globalæ¨¡å¼ä¼˜åŒ–] æ•°æ®å‡†å¤‡å®Œæˆ")
233 | 
234 |     def _pretrain_validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
235 |         """åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œå¯¹ä¸€ä¸ªepochè¿›è¡ŒéªŒè¯"""
236 |         self.multi_label_model.set_eval_mode()
237 |         epoch_losses = {label['name']: [] for label in self.config['labels']}
238 |         
239 |         with torch.no_grad():
240 |             for X_batch, targets_batch in val_loader:
241 |                 X_batch = X_batch.to(self.device, non_blocking=True)
242 |                 targets_batch = {name: tensor.to(self.device, non_blocking=True) 
243 |                                  for name, tensor in targets_batch.items()}
244 |                 
245 |                 # --- ä½¿ç”¨å…¼å®¹çš„autocastä¸Šä¸‹æ–‡ ---
246 |                 with self.autocast(**self.autocast_kwargs):
247 |                     losses = self.multi_label_model.compute_losses(X_batch, targets_batch)
248 |                 
249 |                 for label_name, loss in losses.items():
250 |                     epoch_losses[label_name].append(loss.item())
251 | 
252 |         avg_losses = {f"val_{name}": np.mean(losses) for name, losses in epoch_losses.items() if losses}
253 |         return avg_losses
254 | 
255 |     def _plot_pretrain_losses(self):
256 |         """ç»˜åˆ¶å¹¶ä¿å­˜é¢„è®­ç»ƒæŸå¤±æ›²çº¿"""
257 |         if not self.pretrain_metrics:
258 |             return
259 |             
260 |         metrics_df = pd.DataFrame(self.pretrain_metrics)
261 |         epochs = metrics_df['epoch'].values
262 |         
263 |         plt.style.use('default')  # ä½¿ç”¨é»˜è®¤æ ·å¼ï¼Œé¿å…seabornä¾èµ–é—®é¢˜
264 |         fig, ax = plt.subplots(figsize=(12, 8))
265 |         
266 |         colors = plt.cm.get_cmap('tab10', len(self.config['labels']))
267 |         
268 |         for i, label_config in enumerate(self.config['labels']):
269 |             label_name = label_config['name']
270 |             train_loss_col = f'train_{label_name}'
271 |             val_loss_col = f'val_{label_name}'
272 |             
273 |             if train_loss_col in metrics_df.columns:
274 |                 ax.plot(epochs, metrics_df[train_loss_col], 'o-', color=colors(i), label=f'{label_name} Train Loss')
275 |             if val_loss_col in metrics_df.columns:
276 |                 ax.plot(epochs, metrics_df[val_loss_col], 'x--', color=colors(i), label=f'{label_name} Val Loss')
277 | 
278 |         ax.set_title('Pre-training Loss Curves')
279 |         ax.set_xlabel('Epoch')
280 |         ax.set_ylabel('Loss')
281 |         ax.legend()
282 |         ax.grid(True, alpha=0.3)
283 |         ax.set_xticks(epochs)
284 |         
285 |         plot_path = os.path.join(self.exp_dir, 'pretrain_loss_curves.png')
286 |         try:
287 |             plt.savefig(plot_path, dpi=150, bbox_inches='tight')
288 |             logger.info(f"[é¢„è®­ç»ƒç»˜å›¾] æŸå¤±æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {plot_path}")
289 |         except Exception as e:
290 |             logger.error(f"[é¢„è®­ç»ƒç»˜å›¾] ä¿å­˜æŸå¤±æ›²çº¿å›¾å¤±è´¥: {e}")
291 |         finally:
292 |             plt.close(fig)
293 | 
294 |     def pretrain_models_optimized(self):
295 |         """ä¼˜åŒ–çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…å«è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ã€æŒ‰epochéªŒè¯ã€ä¿å­˜å’Œç»˜å›¾"""
296 |         if not self.config['pretrain']['enabled']:
297 |             logger.info("[é¢„è®­ç»ƒ] è·³è¿‡é¢„è®­ç»ƒé˜¶æ®µ")
298 |             return
299 |         
300 |         logger.info("[é¢„è®­ç»ƒä¼˜åŒ–] å¼€å§‹é¢„è®­ç»ƒé˜¶æ®µ...")
301 |         log_gpu_memory_usage(" - é¢„è®­ç»ƒå¼€å§‹å‰")
302 |         
303 |         # --- 1. å‡†å¤‡å¹¶åˆ’åˆ†æ•°æ® ---
304 |         full_train_data = self.processed_data[self.processed_data['mask'] == 0].copy()
305 |         val_split_ratio = self.config['pretrain'].get('val_split_ratio', 0.5)
306 |         
307 |         pretrain_train_df, pretrain_val_df = train_test_split(
308 |             full_train_data, test_size=val_split_ratio, random_state=42
309 |         )
310 |         logger.info(f"[é¢„è®­ç»ƒä¼˜åŒ–] æ•°æ®åˆ’åˆ†å®Œæˆ - è®­ç»ƒé›†: {len(pretrain_train_df)}, éªŒè¯é›†: {len(pretrain_val_df)}")
311 |         
312 |         # --- 2. åˆ›å»ºDataLoaders ---
313 |         batch_size = self.config['pretrain']['batch_size']
314 |         train_loader = self.create_optimized_dataloader(pretrain_train_df, batch_size, shuffle=True)
315 |         val_loader = self.create_optimized_dataloader(pretrain_val_df, batch_size, shuffle=False)
316 |         
317 |         epochs = self.config['pretrain']['epochs']
318 |         
319 |         for epoch in range(1, epochs + 1):
320 |             logger.info(f"[é¢„è®­ç»ƒä¼˜åŒ–] Epoch {epoch}/{epochs}")
321 |             
322 |             # --- 3. è®­ç»ƒå¾ªç¯ ---
323 |             self.multi_label_model.set_train_mode()
324 |             epoch_train_losses = {label['name']: [] for label in self.config['labels']}
325 |             epoch_start_time = time.time()
326 |             pbar = tqdm(train_loader, desc=f"Epoch {epoch} Training")
327 |             batch_count = 0
328 |             
329 |             for X_batch, targets_batch in pbar:
330 |                 batch_start_time = time.time()
331 |                 X_batch = X_batch.to(self.device, non_blocking=True)
332 |                 targets_batch = {name: tensor.to(self.device, non_blocking=True) for name, tensor in targets_batch.items()}
333 |                 
334 |                 losses = self._perform_training_step(X_batch, targets_batch)
335 |                 
336 |                 for label_name, loss in losses.items():
337 |                     epoch_train_losses[label_name].append(loss)
338 |                 
339 |                 batch_time = time.time() - batch_start_time
340 |                 batch_count += 1
341 |                 
342 |                 # æ›´æ–°è¿›åº¦æ¡
343 |                 loss_info = {f"{k}": f"{v:.4f}" for k, v in losses.items()}
344 |                 loss_info['batch_time'] = f"{batch_time:.3f}s"
345 |                 pbar.set_postfix(loss_info)
346 |                 
347 |                 # æ¯100ä¸ªbatchè®°å½•ä¸€æ¬¡GPUçŠ¶æ€
348 |                 if batch_count % 100 == 0:
349 |                     log_gpu_memory_usage(f" - Epoch {epoch} Batch {batch_count}")
350 | 
351 |             epoch_time = time.time() - epoch_start_time
352 |             avg_train_losses = {f"train_{name}": np.mean(losses) for name, losses in epoch_train_losses.items() if losses}
353 |             loss_str_train = ", ".join([f"{k}: {v:.6f}" for k, v in avg_train_losses.items()])
354 |             logger.info(f"[é¢„è®­ç»ƒä¼˜åŒ–] Epoch {epoch} è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶: {epoch_time:.2f}ç§’ - å¹³å‡æŸå¤±: {loss_str_train}")
355 |             logger.info(f"[é¢„è®­ç»ƒä¼˜åŒ–] Epoch {epoch} ååé‡: {len(pretrain_train_df)/epoch_time:.0f} æ ·æœ¬/ç§’")
356 | 
357 |             # --- 4. éªŒè¯å¾ªç¯ ---
358 |             avg_val_losses = self._pretrain_validate_epoch(val_loader)
359 |             loss_str_val = ", ".join([f"{k}: {v:.6f}" for k, v in avg_val_losses.items()])
360 |             logger.info(f"[é¢„è®­ç»ƒä¼˜åŒ–] Epoch {epoch} éªŒè¯å®Œæˆ - å¹³å‡æŸå¤±: {loss_str_val}")
361 |             
362 |             # --- 5. ä¿å­˜æŒ‡æ ‡ç”¨äºç»˜å›¾ ---
363 |             current_epoch_metrics = {'epoch': epoch, **avg_train_losses, **avg_val_losses}
364 |             self.pretrain_metrics.append(current_epoch_metrics)
365 |             
366 |             # --- 6. ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ ---
367 |             checkpoint_dir = os.path.join(self.exp_dir, "checkpoints")
368 |             self.multi_label_model.save_models(checkpoint_dir, f"pretrain_epoch_{epoch}")
369 |             logger.info(f"[é¢„è®­ç»ƒä¿å­˜] Epoch {epoch} çš„æ¨¡å‹å·²ä¿å­˜åˆ°checkpointsç›®å½•")
370 |             
371 |             # --- 7. ç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿ ---
372 |             if self.config['pretrain'].get('plot_loss_curves', True):
373 |                 self._plot_pretrain_losses()
374 | 
375 |         log_gpu_memory_usage(" - é¢„è®­ç»ƒå®Œæˆå")
376 |         logger.info("[é¢„è®­ç»ƒä¼˜åŒ–] é¢„è®­ç»ƒé˜¶æ®µå®Œæˆ")
377 | 
378 |     def run_single_simulation_step_optimized(self, step: int, is_treatment: bool):
379 |         """ä¼˜åŒ–çš„å•æ­¥ä»¿çœŸ"""
380 |         prefix = "Treatment" if is_treatment else "Control"
381 |         used_videos = self.used_videos_T if is_treatment else self.used_videos_C
382 |         
383 |         batch_size = self.config['global']['batch_size']
384 |         n_candidate = self.config['global']['n_candidate']
385 |         
386 |         # æŠ½æ ·ç”¨æˆ·
387 |         batch_users = random.sample(self.train_users, min(batch_size, len(self.train_users)))
388 |         
389 |         step_rewards = {label['name']: [] for label in self.config['labels']}
390 |         processed_users = 0
391 |         
392 |         for user_id in batch_users:
393 |             user_videos = self.user_video_lists.get(user_id, [])
394 |             available_videos = [v for v in user_videos if v not in used_videos]
395 |             
396 |             if len(available_videos) < n_candidate:
397 |                 continue
398 |                 
399 |             # éšæœºé€‰æ‹©å€™é€‰è§†é¢‘
400 |             candidates = random.sample(available_videos, n_candidate)
401 |             
402 |             # è·å–å€™é€‰è§†é¢‘çš„ç‰¹å¾ - æ·»åŠ æ•°æ®ç±»å‹å®‰å…¨è½¬æ¢
403 |             candidate_mask = self.processed_data['video_id'].isin(candidates)
404 |             candidate_data = self.processed_data[candidate_mask & 
405 |                                               (self.processed_data['user_id'] == user_id)]
406 |             
407 |             if len(candidate_data) == 0:
408 |                 continue
409 |                 
410 |             feature_columns = self.feature_processor.get_feature_columns()
411 |             
412 |             # å®‰å…¨çš„æ•°æ®ç±»å‹è½¬æ¢
413 |             try:
414 |                 candidate_features = candidate_data[feature_columns].copy()
415 |                 # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
416 |                 for col in feature_columns:
417 |                     candidate_features[col] = pd.to_numeric(candidate_features[col], errors='coerce').fillna(0.0)
418 |                 
419 |                 X_candidates = torch.tensor(
420 |                     candidate_features.values.astype(np.float32), 
421 |                     dtype=torch.float32, 
422 |                     device=self.device
423 |                 )
424 |             except Exception as e:
425 |                 logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] ç‰¹å¾è½¬æ¢å¤±è´¥: {e}")
426 |                 continue
427 |             
428 |             # é¢„æµ‹æ¯ä¸ªå€™é€‰è§†é¢‘çš„åˆ†æ•°
429 |             with torch.no_grad():
430 |                 with self.autocast(**self.autocast_kwargs):
431 |                     predictions = self.multi_label_model.predict(X_candidates)
432 |             
433 |             # è®¡ç®—åŠ æƒåˆ†æ•°
434 |             combined_scores = torch.zeros(len(candidates), device=self.device)
435 |             for label_config in self.config['labels']:
436 |                 label_name = label_config['name']
437 |                 if label_name in predictions:
438 |                     alpha = label_config.get('alpha_T' if is_treatment else 'alpha_C', 1.0)
439 |                     pred_scores = predictions[label_name].squeeze()
440 |                     if pred_scores.dim() == 0:
441 |                         pred_scores = pred_scores.unsqueeze(0)
442 |                     combined_scores += alpha * pred_scores
443 |             
444 |             # ç¡®ä¿combined_scoresæ˜¯æ­£ç¡®çš„å½¢çŠ¶å¹¶è·å–æœ‰æ•ˆç´¢å¼•
445 |             scores_squeezed = combined_scores.squeeze()
446 |             if scores_squeezed.dim() == 0:
447 |                 scores_squeezed = scores_squeezed.unsqueeze(0)
448 |             elif scores_squeezed.dim() > 1:
449 |                 scores_squeezed = scores_squeezed.flatten()
450 |             
451 |             # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
452 |             if len(scores_squeezed) != len(candidates):
453 |                 logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] åˆ†æ•°å¼ é‡é•¿åº¦ {len(scores_squeezed)} ä¸å€™é€‰è§†é¢‘æ•° {len(candidates)} ä¸åŒ¹é…")
454 |                 safe_length = min(len(scores_squeezed), len(candidates))
455 |                 if safe_length == 0:
456 |                     continue  # è·³è¿‡è¿™ä¸ªç”¨æˆ·
457 |                 winner_idx = torch.argmax(scores_squeezed[:safe_length]).item()
458 |             else:
459 |                 winner_idx = torch.argmax(scores_squeezed).item()
460 |             
461 |             # å®‰å…¨ç´¢å¼•æ£€æŸ¥
462 |             if winner_idx >= len(candidates):
463 |                 logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] è·èƒœç´¢å¼• {winner_idx} è¶…å‡ºå€™é€‰èŒƒå›´ {len(candidates)}")
464 |                 continue
465 |                 
466 |             winner_video = candidates[winner_idx]
467 |             used_videos.add(winner_video)
468 |             
469 |             # è·å–çœŸå®åé¦ˆ
470 |             winner_mask = (self.processed_data['video_id'] == winner_video) & \
471 |                          (self.processed_data['user_id'] == user_id)
472 |             winner_data = self.processed_data[winner_mask]
473 |             
474 |             if len(winner_data) == 0:
475 |                 continue
476 |                 
477 |             # è®°å½•å¥–åŠ±å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®
478 |             for label_config in self.config['labels']:
479 |                 label_name = label_config['name']
480 |                 target_col = label_config['target']
481 |                 if target_col in winner_data.columns:
482 |                     label_tensor = torch.tensor(
483 |                         winner_data[target_col].values, 
484 |                         dtype=torch.float32, 
485 |                         device=self.device
486 |                     )
487 |                     
488 |                     # ç¡®ä¿å¼ é‡æ˜¯æ ‡é‡ï¼Œç„¶åæå–å€¼
489 |                     if label_tensor.numel() == 1:
490 |                         reward_value = label_tensor.item()
491 |                     else:
492 |                         # å¦‚æœå¼ é‡æœ‰å¤šä¸ªå…ƒç´ ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ æˆ–æ±‚å’Œ
493 |                         reward_value = label_tensor.sum().item()
494 |                     
495 |                     step_rewards[label_name].append(reward_value)
496 |             
497 |             processed_users += 1
498 |         
499 |         # æ‰¹é‡è®­ç»ƒï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
500 |         if processed_users > 0:
501 |             self.batch_training_optimized(batch_users, used_videos, prefix)
502 |         
503 |         # ç´¯åŠ æ€»å¥–åŠ±
504 |         if is_treatment:
505 |             total_rewards = self.total_label_T
506 |         else:
507 |             total_rewards = self.total_label_C
508 |             
509 |         for label_name, rewards in step_rewards.items():
510 |             if rewards:
511 |                 total_rewards[label_name] += sum(rewards)
512 |         
513 |         logger.info(f"[{prefix}ä»¿çœŸä¼˜åŒ–] Step {step}: å¤„ç†ç”¨æˆ·æ•° {processed_users}, "
514 |                    f"ä½¿ç”¨è§†é¢‘æ•° {len(used_videos)}")
515 |         
516 |         return processed_users
517 | 
518 |     def batch_training_optimized(self, batch_users: List[int], used_videos: set, prefix: str):
519 |         """ä¼˜åŒ–çš„æ‰¹é‡è®­ç»ƒ"""
520 |         try:
521 |             # è·å–è¿™äº›ç”¨æˆ·ä½¿ç”¨è¿‡çš„è§†é¢‘çš„æ•°æ®
522 |             user_mask = self.processed_data['user_id'].isin(batch_users)
523 |             video_mask = self.processed_data['video_id'].isin(used_videos)
524 |             training_data = self.processed_data[user_mask & video_mask]
525 |             
526 |             if len(training_data) == 0:
527 |                 logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œè·³è¿‡æ‰¹é‡è®­ç»ƒ")
528 |                 return
529 |             
530 |             feature_columns = self.feature_processor.get_feature_columns()
531 |             
532 |             # å®‰å…¨çš„ç‰¹å¾æ•°æ®è½¬æ¢
533 |             try:
534 |                 features_df = training_data[feature_columns].copy()
535 |                 # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
536 |                 for col in feature_columns:
537 |                     features_df[col] = pd.to_numeric(features_df[col], errors='coerce').fillna(0.0)
538 |                 
539 |                 all_features = torch.tensor(
540 |                     features_df.values.astype(np.float32), 
541 |                     dtype=torch.float32, 
542 |                     device=self.device
543 |                 )
544 |             except Exception as e:
545 |                 logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] ç‰¹å¾è½¬æ¢å¤±è´¥: {e}")
546 |                 return
547 |             
548 |             # å‡†å¤‡æ ‡ç­¾
549 |             combined_targets = {}
550 |             for label_config in self.config['labels']:
551 |                 target_col = label_config['target']
552 |                 if target_col in training_data.columns:
553 |                     combined_targets[label_config['name']] = torch.tensor(
554 |                         training_data[target_col].values, 
555 |                         dtype=torch.float32, 
556 |                         device=self.device
557 |                     ).unsqueeze(1)
558 |             
559 |             # éªŒè¯ç‰¹å¾å’Œæ ‡ç­¾çš„æ ·æœ¬æ•°é‡æ˜¯å¦ä¸€è‡´
560 |             n_features = all_features.size(0)
561 |             n_targets = combined_targets[list(combined_targets.keys())[0]].size(0)
562 |             
563 |             if n_features != n_targets:
564 |                 logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] ç‰¹å¾æ ·æœ¬æ•° {n_features} ä¸æ ‡ç­¾æ ·æœ¬æ•° {n_targets} ä¸åŒ¹é…ï¼Œè°ƒæ•´æ‰¹é‡è®­ç»ƒ")
565 |                 min_samples = min(n_features, n_targets)
566 |                 if min_samples == 0:
567 |                     logger.warning(f"[{prefix}ä»¿çœŸä¼˜åŒ–] æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ç”¨äºè®­ç»ƒï¼Œè·³è¿‡æ‰¹é‡è®­ç»ƒ")
568 |                     return
569 |                 
570 |                 # è°ƒæ•´å¼ é‡å¤§å°
571 |                 all_features = all_features[:min_samples]
572 |                 for label_name in combined_targets:
573 |                     combined_targets[label_name] = combined_targets[label_name][:min_samples]
574 |             
575 |             # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
576 |             losses = self._perform_training_step(all_features, combined_targets)
577 |             
578 |             # è®°å½•æŸå¤±ï¼ˆå¯é€‰ï¼‰
579 |             loss_str = ", ".join([f"{name}: {loss:.6f}" for name, loss in losses.items()])
580 |             logger.debug(f"[{prefix}ä»¿çœŸä¼˜åŒ–] æ‰¹é‡è®­ç»ƒæŸå¤± - {loss_str}")
581 |             
582 |         except Exception as e:
583 |             logger.error(f"[{prefix}ä»¿çœŸä¼˜åŒ–] æ‰¹é‡è®­ç»ƒå¤±è´¥: {e}")
584 | 
585 |     def run_simulation_for_group_optimized(self, is_treatment: bool):
586 |         """ä¸ºå•ä¸ªç»„è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–ä»¿çœŸ"""
587 |         prefix = "Treatment" if is_treatment else "Control"
588 |         logger.info(f"========== å¼€å§‹ {prefix} ç»„ä»¿çœŸï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ==========")
589 |         
590 |         n_steps = self.config['global']['n_steps']
591 |         validate_every = self.config['global']['validate_every']
592 |         
593 |         start_time = time.time()
594 |         
595 |         for step in range(1, n_steps + 1):
596 |             step_start_time = time.time()
597 |             
598 |             processed_users = self.run_single_simulation_step_optimized(step, is_treatment)
599 |             
600 |             step_time = time.time() - step_start_time
601 |             
602 |             if step % 10 == 0:  # æ¯10æ­¥æŠ¥å‘Šä¸€æ¬¡
603 |                 logger.info(f"[{prefix}ä»¿çœŸä¼˜åŒ–] Step {step}/{n_steps}, "
604 |                            f"å¤„ç†ç”¨æˆ·: {processed_users}, ç”¨æ—¶: {step_time:.2f}ç§’")
605 |             
606 |             # éªŒè¯æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
607 |             if step % validate_every == 0:
608 |                 self.validate_models_optimized(step, prefix)
609 |         
610 |         total_time = time.time() - start_time
611 |         logger.info(f"========== {prefix} ç»„ä»¿çœŸå®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.2f}ç§’ ==========")
612 | 
613 |     def validate_models_optimized(self, step: int, prefix: str):
614 |         """ä¼˜åŒ–çš„æ¨¡å‹éªŒè¯"""
615 |         logger.info(f"[{prefix}éªŒè¯ä¼˜åŒ–] Step {step} æ¨¡å‹éªŒè¯...")
616 |         
617 |         # ç®€åŒ–çš„éªŒè¯é€»è¾‘ï¼Œé¿å…è€—æ—¶çš„éªŒè¯è¿‡ç¨‹
618 |         val_data = self.processed_data[self.processed_data['mask'] == 1].sample(
619 |             min(1000, len(self.processed_data[self.processed_data['mask'] == 1]))
620 |         )
621 |         
622 |         if len(val_data) == 0:
623 |             return
624 |         
625 |         feature_columns = self.feature_processor.get_feature_columns()
626 |         
627 |         # å®‰å…¨çš„æ•°æ®è½¬æ¢
628 |         try:
629 |             val_features = val_data[feature_columns].copy()
630 |             # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
631 |             for col in feature_columns:
632 |                 val_features[col] = pd.to_numeric(val_features[col], errors='coerce').fillna(0.0)
633 |             
634 |             X_val = torch.tensor(
635 |                 val_features.values.astype(np.float32), 
636 |                 dtype=torch.float32, 
637 |                 device=self.device
638 |             )
639 |         except Exception as e:
640 |             logger.warning(f"[{prefix}éªŒè¯ä¼˜åŒ–] ç‰¹å¾è½¬æ¢å¤±è´¥: {e}")
641 |             return
642 |         
643 |         with torch.no_grad():
644 |             with self.autocast(**self.autocast_kwargs):
645 |                 predictions = self.multi_label_model.predict(X_val)
646 |         
647 |         # è®¡ç®—éªŒè¯æŒ‡æ ‡
648 |         for label_config in self.config['labels']:
649 |             label_name = label_config['name']
650 |             target_col = label_config['target']
651 |             
652 |             if label_name in predictions and target_col in val_data.columns:
653 |                 pred = predictions[label_name].cpu().numpy().flatten()
654 |                 true = val_data[target_col].values
655 |                 
656 |                 # è®¡ç®—ç›¸å¯¹è¯¯å·®
657 |                 non_zero_mask = true != 0
658 |                 if np.any(non_zero_mask):
659 |                     relative_errors = np.abs((pred[non_zero_mask] - true[non_zero_mask]) / true[non_zero_mask])
660 |                     mean_relative_error = np.mean(relative_errors) * 100
661 |                     logger.info(f"[{prefix}éªŒè¯ä¼˜åŒ–] Step {step} {label_name} å¹³å‡ç›¸å¯¹è¯¯å·®: {mean_relative_error:.2f}%")
662 | 
663 |     def run_global_simulation_optimized(self):
664 |         """è¿è¡Œä¼˜åŒ–çš„Globalä»¿çœŸ"""
665 |         logger.info("[Globalä»¿çœŸä¼˜åŒ–] å¼€å§‹å®Œæ•´å®éªŒ...")
666 |         
667 |         # å¯åŠ¨GPUç›‘æ§
668 |         self.start_gpu_monitoring()
669 |         
670 |         try:
671 |             # Treatmentç»„ä»¿çœŸ
672 |             self.run_simulation_for_group_optimized(is_treatment=True)
673 |             
674 |             logger.info("[Globalä»¿çœŸä¼˜åŒ–] Treatmentç»„å®Œæˆï¼Œå¼€å§‹Controlç»„...")
675 |             
676 |             # Controlç»„ä»¿çœŸ
677 |             self.run_simulation_for_group_optimized(is_treatment=False)
678 |             
679 |         finally:
680 |             # åœæ­¢GPUç›‘æ§
681 |             self.stop_gpu_monitoring()
682 | 
683 |     def compute_gte_optimized(self) -> Dict[str, float]:
684 |         """è®¡ç®—ä¼˜åŒ–çš„GTE"""
685 |         logger.info("[GTEè®¡ç®—ä¼˜åŒ–] å¼€å§‹è®¡ç®—å…¨å±€å¤„ç†æ•ˆåº”...")
686 |         gte_results = {}
687 |         
688 |         for label_name in self.total_label_T:
689 |             gt_total = self.total_label_T[label_name]
690 |             gc_total = self.total_label_C[label_name]
691 |             gte = gt_total - gc_total
692 |             gte_relative = (gte / gc_total * 100) if gc_total != 0 else 0
693 |             
694 |             gte_results[f'GTE_{label_name}'] = gte
695 |             gte_results[f'GTE_{label_name}_relative'] = gte_relative
696 |             
697 |             logger.info(f"[GTEè®¡ç®—ä¼˜åŒ–] {label_name}: GT={gt_total:.4f}, GC={gc_total:.4f}, "
698 |                        f"GTE={gte:.4f} ({gte_relative:+.2f}%)")
699 |         
700 |         return gte_results
701 | 
702 |     def run(self):
703 |         """è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–Globalæ¨¡å¼å®éªŒ"""
704 |         logger.info("[Globalæ¨¡å¼ä¼˜åŒ–] å¼€å§‹è¿è¡Œå®Œæ•´å®éªŒ...")
705 |         
706 |         try:
707 |             # GPUè¯Šæ–­
708 |             log_gpu_info()
709 |             test_gpu_training_speed()
710 |             
711 |             # æ•°æ®å‡†å¤‡
712 |             self.load_and_prepare_data()
713 |             
714 |             # ä¼˜åŒ–é¢„è®­ç»ƒ
715 |             self.pretrain_models_optimized()
716 |             
717 |             # ä¼˜åŒ–ä»¿çœŸ
718 |             self.run_global_simulation_optimized()
719 |             
720 |             # è®¡ç®—GTE
721 |             gte_results = self.compute_gte_optimized()
722 |             
723 |             # ä¿å­˜ç»“æœ
724 |             final_results = {
725 |                 'config': self.config,
726 |                 'gte_results': gte_results,
727 |                 'metrics_summary': self.metrics_tracker.get_summary(),
728 |                 'dataset_stats': self.data_loader_wrapper.get_dataset_stats()
729 |             }
730 |             
731 |             results_path = os.path.join(self.exp_dir, 'result.json')
732 |             save_results(final_results, results_path)
733 |             
734 |             logger.info("[Globalæ¨¡å¼ä¼˜åŒ–] å®éªŒå®Œæˆï¼")
735 |             logger.info("========== æœ€ç»ˆGTEç»“æœ ==========")
736 |             for key, value in gte_results.items():
737 |                 logger.info(f"{key}: {value}")
738 |                 
739 |         except Exception as e:
740 |             logger.error(f"[Globalæ¨¡å¼ä¼˜åŒ–] å®éªŒæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
741 |             raise
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\modes\__init__.py

- Extension: .py
- Language: python
- Size: 95 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
1 | """
2 | å®éªŒæ¨¡å¼æ¨¡å—
3 | """
4 | 
5 | from .global_mode import GlobalMode
6 | 
7 | __all__ = ['GlobalMode']
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\device_utils.py

- Extension: .py
- Language: python
- Size: 4342 bytes
- Created: 2025-08-17 11:34:47
- Modified: 2025-08-18 00:47:42

### Code

```python
 1 | """
 2 | è®¾å¤‡é€‰æ‹©å’Œç®¡ç†å·¥å…·
 3 | """
 4 | import torch
 5 | import logging
 6 | from contextlib import contextmanager
 7 | 
 8 | logger = logging.getLogger(__name__)
 9 | 
10 | # --- å…³é”®ä¿®å¤ï¼šæå‰åœ¨é¡¶å±‚å¯¼å…¥IPEX ---
11 | # æˆ‘ä»¬åœ¨ç¨‹åºæ—©æœŸå°±å°è¯•å¯¼å…¥IPEXï¼Œé¿å…ä¸å…¶ä»–åº“ï¼ˆå¦‚numpy, pandasï¼‰äº§ç”Ÿåº•å±‚DLLå†²çªã€‚
12 | _IPEX_AVAILABLE = False
13 | try:
14 |     import intel_extension_for_pytorch as ipex
15 |     _IPEX_AVAILABLE = True
16 |     # è¿™æ¡æ—¥å¿—ç°åœ¨åº”è¯¥ä¼šåœ¨ç¨‹åºå¯åŠ¨æ—¶å¾ˆæ—©å°±å‡ºç°
17 |     logger.info("Intel Extension for PyTorch (IPEX) discovered and imported successfully at top level.")
18 | except ImportError:
19 |     # å¦‚æœè¿™é‡Œå¤±è´¥ï¼Œè¯´æ˜ç¯å¢ƒç¡®å®æœ‰é—®é¢˜
20 |     logger.info("Intel Extension for PyTorch (IPEX) not found during initial import. IPEX backend will be unavailable.")
21 | # --- ä¿®å¤ç»“æŸ ---
22 | 
23 | 
24 | def get_device_and_amp_helpers(device_choice='auto'):
25 |     """
26 |     Dynamically determines the best available device and corresponding AMP tools.
27 |     """
28 |     class StubScaler:
29 |         def __init__(self, enabled=False): pass
30 |         def scale(self, loss): return loss
31 |         def step(self, optimizer): optimizer.step()
32 |         def update(self): pass
33 |         def get_scale(self): return 1.0
34 |         def is_enabled(self): return False
35 | 
36 |     # --- å…³é”®ä¿®å¤ï¼šä½¿ stub_autocast çš„å‚æ•°å˜ä¸ºå¯é€‰ ---
37 |     @contextmanager
38 |     def stub_autocast(device_type=None, *args, **kwargs):
39 |         yield
40 |     # --- ä¿®å¤ç»“æŸ ---
41 | 
42 |     # 'auto' detection order: cuda -> ipex -> xpu -> dml -> cpu
43 | 
44 |     # 1. Check for CUDA
45 |     if device_choice.lower() in ['auto', 'cuda']:
46 |         try:
47 |             if torch.cuda.is_available():
48 |                 from torch.amp import autocast, GradScaler
49 |                 device = torch.device("cuda")
50 |                 logger.info("[Device] CUDA is available. Using CUDA backend (Full AMP).")
51 |                 return device, autocast, GradScaler
52 |         except ImportError:
53 |             logger.warning("[Device] torch.cuda or torch.amp not found, skipping CUDA check.")
54 | 
55 |     # 2. Check for IPEX (Full Optimization)
56 |     if device_choice.lower() in ['auto', 'ipex']:
57 |         # å¯¼å…¥å·²åœ¨é¡¶å±‚å®Œæˆï¼Œè¿™é‡Œåªæ£€æŸ¥æ ‡å¿—å’Œè®¾å¤‡å¯ç”¨æ€§
58 |         if _IPEX_AVAILABLE and torch.xpu.is_available():
59 |             # IPEX has autocast but not GradScaler. We return our StubScaler.
60 |             from torch.xpu.amp import autocast
61 |             device = torch.device("xpu")
62 |             logger.info("[Device] Intel IPEX is available. Using XPU backend (Full IPEX Optimization & AMP).")
63 |             # Return the REAL autocast but a FAKE scaler class
64 |             return device, autocast, StubScaler
65 |         elif device_choice.lower() == 'ipex':
66 |             # å¤„ç†ç”¨æˆ·æ˜ç¡®è¦æ±‚'ipex'ä½†é¡¶å±‚å¯¼å…¥å¤±è´¥çš„æƒ…å†µ
67 |             logger.warning("[Device] 'ipex' was chosen, but the IPEX library could not be imported or is not functional.")
68 | 
69 |     # 3. Check for XPU (Basic Device Placement)
70 |     if device_choice.lower() in ['auto', 'xpu']:
71 |         try:
72 |             # å³ä½¿æ²¡æœ‰é¡¶å±‚å¯¼å…¥æˆåŠŸï¼ŒåŸºç¡€çš„xpuè®¾å¤‡ä¹Ÿå¯èƒ½è¢«torchè¯†åˆ«
73 |             if torch.xpu.is_available():
74 |                 device = torch.device("xpu")
75 |                 logger.info("[Device] Intel XPU device is available. Using XPU backend (Basic, NO IPEX Optimizations, NO AMP).")
76 |                 return device, stub_autocast, StubScaler
77 |         except (ImportError, AttributeError):
78 |             if device_choice.lower() == 'xpu':
79 |                  logger.warning("[Device] 'xpu' was chosen, but torch.xpu was not available.")
80 | 
81 |     # 4. Check for DirectML
82 |     if device_choice.lower() in ['auto', 'dml']:
83 |         try:
84 |             import torch_directml
85 |             if torch_directml.is_available():
86 |                 device = torch_directml.device()
87 |                 logger.info("[Device] DirectML is available. Using DML backend (NO AMP).")
88 |                 return device, stub_autocast, StubScaler
89 |         except ImportError:
90 |             if device_choice.lower() == 'dml':
91 |                 logger.warning("[Device] 'dml' was chosen, but torch_directml not found.")
92 |             
93 |     # 5. Fallback to CPU
94 |     logger.info("[Device] No specified or available GPU backend found. Falling back to CPU.")
95 |     device = torch.device("cpu")
96 |     return device, stub_autocast, StubScaler
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\experiment_utils.py

- Extension: .py
- Language: python
- Size: 1407 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
 1 | """
 2 | å®éªŒå·¥å…·å‡½æ•°
 3 | """
 4 | 
 5 | import os
 6 | import json
 7 | import logging
 8 | from datetime import datetime
 9 | from typing import Dict, Any
10 | 
11 | logger = logging.getLogger(__name__)
12 | 
13 | def create_experiment_dir(base_dir: str) -> str:
14 |     """åˆ›å»ºå®éªŒç›®å½•"""
15 |     timestamp = datetime.now().strftime("%Y%m%d_%H%M")
16 |     exp_dir = os.path.join(base_dir, "results", timestamp)
17 |     
18 |     # åˆ›å»ºç›®å½•ç»“æ„
19 |     os.makedirs(exp_dir, exist_ok=True)
20 |     os.makedirs(os.path.join(exp_dir, "checkpoints"), exist_ok=True)
21 |     
22 |     logger.info(f"[å®éªŒç›®å½•] åˆ›å»ºå®éªŒç›®å½•: {exp_dir}")
23 |     return exp_dir
24 | 
25 | def save_results(results: Dict[str, Any], save_path: str):
26 |     """ä¿å­˜å®éªŒç»“æœåˆ°JSONæ–‡ä»¶"""
27 |     try:
28 |         with open(save_path, 'w', encoding='utf-8') as f:
29 |             json.dump(results, f, indent=2, ensure_ascii=False)
30 |         logger.info(f"[ç»“æœä¿å­˜] ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
31 |     except Exception as e:
32 |         logger.error(f"[ç»“æœä¿å­˜] ä¿å­˜å¤±è´¥: {e}")
33 | 
34 | def load_results(file_path: str) -> Dict[str, Any]:
35 |     """ä»JSONæ–‡ä»¶åŠ è½½å®éªŒç»“æœ"""
36 |     try:
37 |         with open(file_path, 'r', encoding='utf-8') as f:
38 |             results = json.load(f)
39 |         logger.info(f"[ç»“æœåŠ è½½] ç»“æœå·²ä» {file_path} åŠ è½½")
40 |         return results
41 |     except Exception as e:
42 |         logger.error(f"[ç»“æœåŠ è½½] åŠ è½½å¤±è´¥: {e}")
43 |         return {}
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\gpu_utils.py

- Extension: .py
- Language: python
- Size: 6618 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
  1 | """
  2 | GPUè¯Šæ–­å’Œç›‘æ§å·¥å…·
  3 | ç”¨äºæ£€æµ‹GPUçŠ¶æ€ã€å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„GPUåˆ©ç”¨ç‡
  4 | """
  5 | 
  6 | import logging
  7 | import torch
  8 | import time
  9 | import threading
 10 | import subprocess
 11 | import os
 12 | 
 13 | logger = logging.getLogger(__name__)
 14 | 
 15 | def log_gpu_info():
 16 |     """è®°å½•è¯¦ç»†çš„GPUç¯å¢ƒä¿¡æ¯"""
 17 |     if not torch.cuda.is_available():
 18 |         logger.warning("[GPUæ£€æŸ¥] CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿è¡Œ")
 19 |         return
 20 | 
 21 |     logger.info("========== GPUè¯Šæ–­ä¿¡æ¯ ==========")
 22 |     try:
 23 |         device_id = torch.cuda.current_device()
 24 |         device_name = torch.cuda.get_device_name(device_id)
 25 |         logger.info(f"[GPUæ£€æŸ¥] CUDAå¯ç”¨ï¼Œä½¿ç”¨GPU: {device_name}")
 26 |         logger.info(f"[GPUæ£€æŸ¥]   - è®¾å¤‡ID: {device_id}")
 27 |         
 28 |         # è·å–GPUå±æ€§
 29 |         props = torch.cuda.get_device_properties(device_id)
 30 |         total_mem = props.total_memory / (1024**3)
 31 |         logger.info(f"[GPUæ£€æŸ¥]   - æ€»å†…å­˜: {total_mem:.2f} GB")
 32 |         logger.info(f"[GPUæ£€æŸ¥]   - è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
 33 |         logger.info(f"[GPUæ£€æŸ¥]   - å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}")
 34 |         
 35 |         # åˆå§‹å†…å­˜ä½¿ç”¨æƒ…å†µ
 36 |         allocated_mem = torch.cuda.memory_allocated(device_id) / (1024**2)
 37 |         reserved_mem = torch.cuda.memory_reserved(device_id) / (1024**2)
 38 |         logger.info(f"[GPUæ£€æŸ¥]   - åˆå§‹å·²åˆ†é…å†…å­˜: {allocated_mem:.2f} MB")
 39 |         logger.info(f"[GPUæ£€æŸ¥]   - åˆå§‹ä¿ç•™å†…å­˜: {reserved_mem:.2f} MB")
 40 |         
 41 |         # æµ‹è¯•GPUæ“ä½œ
 42 |         test_tensor = torch.randn(1000, 1000).cuda()
 43 |         result = torch.mm(test_tensor, test_tensor)
 44 |         logger.info(f"[GPUæ£€æŸ¥]   - GPUè¿ç®—æµ‹è¯•: é€šè¿‡ (1000x1000çŸ©é˜µä¹˜æ³•)")
 45 |         
 46 |         # æ¸…ç†æµ‹è¯•å¼ é‡
 47 |         del test_tensor, result
 48 |         torch.cuda.empty_cache()
 49 |         
 50 |     except Exception as e:
 51 |         logger.error(f"[GPUæ£€æŸ¥] è·å–GPUè¯¦æƒ…å¤±è´¥: {e}")
 52 |     logger.info("=====================================")
 53 | 
 54 | def log_gpu_memory_usage(prefix=""):
 55 |     """è®°å½•å½“å‰GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
 56 |     if not torch.cuda.is_available():
 57 |         return
 58 |     
 59 |     try:
 60 |         device_id = torch.cuda.current_device()
 61 |         allocated = torch.cuda.memory_allocated(device_id) / (1024**2)
 62 |         reserved = torch.cuda.memory_reserved(device_id) / (1024**2)
 63 |         logger.info(f"[GPUå†…å­˜{prefix}] å·²åˆ†é…: {allocated:.2f} MB, ä¿ç•™: {reserved:.2f} MB")
 64 |     except Exception as e:
 65 |         logger.error(f"[GPUå†…å­˜{prefix}] è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
 66 | 
 67 | def test_gpu_training_speed():
 68 |     """æµ‹è¯•GPUè®­ç»ƒé€Ÿåº¦"""
 69 |     if not torch.cuda.is_available():
 70 |         logger.warning("[GPUé€Ÿåº¦æµ‹è¯•] CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
 71 |         return
 72 |     
 73 |     logger.info("[GPUé€Ÿåº¦æµ‹è¯•] å¼€å§‹GPUè®­ç»ƒé€Ÿåº¦æµ‹è¯•...")
 74 |     try:
 75 |         # åˆ›å»ºæµ‹è¯•æ•°æ®
 76 |         batch_size = 1024
 77 |         input_dim = 100
 78 |         hidden_dim = 256
 79 |         
 80 |         # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®
 81 |         model = torch.nn.Sequential(
 82 |             torch.nn.Linear(input_dim, hidden_dim),
 83 |             torch.nn.ReLU(),
 84 |             torch.nn.Linear(hidden_dim, 1)
 85 |         ).cuda()
 86 |         
 87 |         data = torch.randn(batch_size, input_dim).cuda()
 88 |         target = torch.randn(batch_size, 1).cuda()
 89 |         criterion = torch.nn.MSELoss()
 90 |         optimizer = torch.optim.Adam(model.parameters())
 91 |         
 92 |         # é¢„çƒ­
 93 |         for _ in range(10):
 94 |             output = model(data)
 95 |             loss = criterion(output, target)
 96 |             optimizer.zero_grad()
 97 |             loss.backward()
 98 |             optimizer.step()
 99 |         
100 |         # æ­£å¼æµ‹è¯•
101 |         torch.cuda.synchronize()
102 |         start_time = time.time()
103 |         
104 |         for i in range(100):
105 |             output = model(data)
106 |             loss = criterion(output, target)
107 |             optimizer.zero_grad()
108 |             loss.backward()
109 |             optimizer.step()
110 |         
111 |         torch.cuda.synchronize()
112 |         end_time = time.time()
113 |         
114 |         elapsed = end_time - start_time
115 |         samples_per_sec = (100 * batch_size) / elapsed
116 |         
117 |         logger.info(f"[GPUé€Ÿåº¦æµ‹è¯•] å®Œæˆ100æ¬¡è¿­ä»£ï¼Œç”¨æ—¶: {elapsed:.2f}ç§’")
118 |         logger.info(f"[GPUé€Ÿåº¦æµ‹è¯•] å¤„ç†é€Ÿåº¦: {samples_per_sec:.0f} æ ·æœ¬/ç§’")
119 |         
120 |         # æ¸…ç†
121 |         del model, data, target
122 |         torch.cuda.empty_cache()
123 |         
124 |     except Exception as e:
125 |         logger.error(f"[GPUé€Ÿåº¦æµ‹è¯•] æµ‹è¯•å¤±è´¥: {e}")
126 | 
127 | class GPUMonitor:
128 |     """GPUå®æ—¶ç›‘æ§å™¨"""
129 |     
130 |     def __init__(self, log_interval=30):
131 |         self.log_interval = log_interval
132 |         self.monitoring = False
133 |         self.monitor_thread = None
134 |         
135 |     def start_monitoring(self):
136 |         """å¼€å§‹ç›‘æ§GPUçŠ¶æ€"""
137 |         if not torch.cuda.is_available():
138 |             logger.warning("[GPUç›‘æ§] CUDAä¸å¯ç”¨ï¼Œè·³è¿‡ç›‘æ§")
139 |             return
140 |             
141 |         self.monitoring = True
142 |         self.monitor_thread = threading.Thread(target=self._monitor_loop)
143 |         self.monitor_thread.daemon = True
144 |         self.monitor_thread.start()
145 |         logger.info(f"[GPUç›‘æ§] å¼€å§‹ç›‘æ§ï¼Œæ¯{self.log_interval}ç§’è®°å½•ä¸€æ¬¡")
146 |         
147 |     def stop_monitoring(self):
148 |         """åœæ­¢ç›‘æ§"""
149 |         self.monitoring = False
150 |         if self.monitor_thread:
151 |             self.monitor_thread.join()
152 |         logger.info("[GPUç›‘æ§] åœæ­¢ç›‘æ§")
153 |         
154 |     def _monitor_loop(self):
155 |         """ç›‘æ§å¾ªç¯"""
156 |         while self.monitoring:
157 |             try:
158 |                 log_gpu_memory_usage(" - ç›‘æ§")
159 |                 
160 |                 # å°è¯•è·å–GPUåˆ©ç”¨ç‡
161 |                 try:
162 |                     result = subprocess.run(
163 |                         ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
164 |                         capture_output=True, text=True, timeout=5
165 |                     )
166 |                     if result.returncode == 0:
167 |                         gpu_util = float(result.stdout.strip())
168 |                         logger.info(f"[GPUç›‘æ§] GPUåˆ©ç”¨ç‡: {gpu_util}%")
169 |                 except Exception:
170 |                     pass  # å¦‚æœnvidia-smiä¸å¯ç”¨ï¼Œè·³è¿‡åˆ©ç”¨ç‡æ£€æŸ¥
171 |                     
172 |                 time.sleep(self.log_interval)
173 |             except Exception as e:
174 |                 logger.error(f"[GPUç›‘æ§] ç›‘æ§è¿‡ç¨‹å‡ºé”™: {e}")
175 |                 break
176 | 
177 | def setup_gpu_monitoring(log_interval=30):
178 |     """è®¾ç½®GPUç›‘æ§"""
179 |     monitor = GPUMonitor(log_interval)
180 |     monitor.start_monitoring()
181 |     return monitor
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\logger.py

- Extension: .py
- Language: python
- Size: 949 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
 1 | """
 2 | æ—¥å¿—é…ç½®å·¥å…·
 3 | """
 4 | 
 5 | import logging
 6 | import os
 7 | from datetime import datetime
 8 | 
 9 | def setup_logger(log_file: str, level: str = 'INFO') -> logging.Logger:
10 |     """è®¾ç½®æ—¥å¿—å™¨"""
11 |     # åˆ›å»ºæ—¥å¿—ç›®å½•
12 |     os.makedirs(os.path.dirname(log_file), exist_ok=True)
13 |     
14 |     # é…ç½®æ—¥å¿—æ ¼å¼
15 |     formatter = logging.Formatter(
16 |         '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
17 |         datefmt='%Y-%m-%d %H:%M:%S'
18 |     )
19 |     
20 |     # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
21 |     file_handler = logging.FileHandler(log_file, encoding='utf-8')
22 |     file_handler.setFormatter(formatter)
23 |     
24 |     # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
25 |     console_handler = logging.StreamHandler()
26 |     console_handler.setFormatter(formatter)
27 |     
28 |     # é…ç½®æ ¹æ—¥å¿—å™¨
29 |     logger = logging.getLogger()
30 |     logger.setLevel(getattr(logging, level.upper()))
31 |     logger.addHandler(file_handler)
32 |     logger.addHandler(console_handler)
33 |     
34 |     return logger
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\metrics.py

- Extension: .py
- Language: python
- Size: 1854 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-14 10:20:58

### Code

```python
 1 | """
 2 | æŒ‡æ ‡è·Ÿè¸ªå™¨
 3 | """
 4 | 
 5 | import logging
 6 | from typing import Dict, List, Any
 7 | from collections import defaultdict
 8 | import numpy as np
 9 | 
10 | logger = logging.getLogger(__name__)
11 | 
12 | class MetricsTracker:
13 |     """æŒ‡æ ‡è·Ÿè¸ªå™¨"""
14 |     
15 |     def __init__(self):
16 |         self.metrics = defaultdict(list)
17 |         self.current_metrics = {}
18 |         
19 |     def update(self, metrics: Dict[str, float], step: int = None):
20 |         """æ›´æ–°æŒ‡æ ‡"""
21 |         for key, value in metrics.items():
22 |             self.metrics[key].append(value)
23 |         
24 |         self.current_metrics = metrics.copy()
25 |         if step is not None:
26 |             self.current_metrics['step'] = step
27 |     
28 |     def get_latest(self) -> Dict[str, float]:
29 |         """è·å–æœ€æ–°æŒ‡æ ‡"""
30 |         return self.current_metrics.copy()
31 |     
32 |     def get_history(self, key: str) -> List[float]:
33 |         """è·å–æŒ‡æ ‡å†å²"""
34 |         return self.metrics[key].copy()
35 |     
36 |     def get_summary(self) -> Dict[str, Dict[str, float]]:
37 |         """è·å–æŒ‡æ ‡æ‘˜è¦"""
38 |         summary = {}
39 |         for key, values in self.metrics.items():
40 |             if values:
41 |                 summary[key] = {
42 |                     'mean': float(np.mean(values)),
43 |                     'std': float(np.std(values)),
44 |                     'min': float(np.min(values)),
45 |                     'max': float(np.max(values)),
46 |                     'latest': float(values[-1])
47 |                 }
48 |         return summary
49 |     
50 |     def log_current(self, prefix: str = ""):
51 |         """è®°å½•å½“å‰æŒ‡æ ‡"""
52 |         if self.current_metrics:
53 |             metrics_str = ", ".join([f"{k}: {v:.6f}" for k, v in self.current_metrics.items()])
54 |             logger.info(f"[{prefix}] {metrics_str}")
55 |     
56 |     def reset(self):
57 |         """é‡ç½®æŒ‡æ ‡"""
58 |         self.metrics.clear()
59 |         self.current_metrics.clear()
```

## File: E:\MyDocument\Codes_notnut\_notpad\IEDA\RealdataEXP\libs\utils\__init__.py

- Extension: .py
- Language: python
- Size: 551 bytes
- Created: 2025-08-14 10:20:58
- Modified: 2025-08-17 11:40:57

### Code

```python
 1 | """
 2 | å·¥å…·æ¨¡å—
 3 | """
 4 | 
 5 | from .logger import setup_logger
 6 | from .metrics import MetricsTracker
 7 | from .experiment_utils import create_experiment_dir, save_results
 8 | from .gpu_utils import log_gpu_info, log_gpu_memory_usage, test_gpu_training_speed, setup_gpu_monitoring
 9 | from .device_utils import get_device_and_amp_helpers
10 | 
11 | __all__ = [
12 |     'setup_logger', 'MetricsTracker', 'create_experiment_dir', 'save_results', 
13 |     'log_gpu_info', 'log_gpu_memory_usage', 'test_gpu_training_speed', 'setup_gpu_monitoring',
14 |     'get_device_and_amp_helpers'
15 | ]
```

